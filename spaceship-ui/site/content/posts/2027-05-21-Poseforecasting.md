---
title: Pose Forecasting
description: Alessio Sampieri1, Guido Maria D’Amely di Melendugno1, Andrea Avogaro2,
pubDate: 2027-05-21
draft: true
tags:
- artificial-intelligence
- machine-learning
- deep-learning
- pose-estimation
- robotics
---

## Pose Forecasting in Industrial Human-Robot Collaboration
===
Alessio Sampieri1, Guido Maria D’Amely di Melendugno1, Andrea Avogaro2,
Federico Cunico2, Francesco Setti2, Geri Skenderi2, Marco Cristani2, 그리고 Fabio Galasso1

1 로마 사피엔자 대학교
{sampieri, damely, galasso}@di.uniroma1.it

2 베로나 대학교
{andrea.avogaro, federico.cunico, francesco.setti, geri.skenderi, marco.cristani}@univr.it

초록
===

산업 환경에서 **협업 로봇(cobot)**의 한계를 극복하기 위해, 우리는 자세 예측을 위한 새로운 **Separable-Sparse Graph Convolutional Network(SeS-GCN)**를 제안한다. SeS-GCN은 공간, 시간, 채널 차원 간의 상호작용을 병목 처리하는 최초의 GCN 구조이며, **teacher-student 프레임워크**를 통해 희소한 인접 행렬을 학습한다. 기존 최첨단 기법들과 비교해, 이 모델은 단 1.72%의 파라미터만을 사용하며, 약 4배 빠른 속도를 제공하면서도 Human3.6M 데이터셋에서 1초 미래의 자세 예측 정확도에서 유사한 성능을 유지한다. 이는 협업 로봇이 인간 작업자의 동작을 인식하게 해 준다. 두 번째 기여로, 우리는 산업 협업 시의 인간과 로봇 간 상호작용을 벤치마킹하기 위한 새로운 데이터셋 **CHICO(Cobots and Humans in Industrial COllaboration)**를 소개한다. CHICO는 다각도 영상, 3D 자세, 그리고 20명의 인간 작업자와 협업 로봇이 수행한 7가지 현실적인 산업 작업에 대한 궤적을 포함하고 있다. 또한 인간과 협업 로봇 간에 발생한 실제 충돌 사례 226건을 포함한다. 우리는 CHICO에서 두 가지 주요 로봇 지각(perception) 과제를 평가하였다: 1초 후의 인간 자세 예측에서 **평균 오차(MPJPE)**가 85.3mm이고, 실행 시간은 2.3밀리초였으며, 예측된 인간 동작과 알려진 로봇 동작을 비교한 충돌 감지에서는 **F1-score 0.64**를 달성하였다.

---

1. 서론
===

**협업 로봇(cobot)**과 현대적 **인간-로봇 협업(HRC)**은 기존 산업용 로봇의 기능적 분리 개념에서 벗어나, 인간과 로봇이 작업 공간을 공유한다는 점에서 차별된다 [33] [38]. 더불어, cobot과 인간은 동시에 작업을 수행하게 되며, 이로 인해 물리적인 접촉이 발생할 수밖에 없다. 대기 시간 감소로 인해 생산성이 최대 85%까지 향상된다는 이점이 있지만 [65] [66], 작업장 안전 측면에서는 새로운 도전 과제가 존재한다 [24]. 문제는 접촉 여부가 아니라, 그 접촉이 어떤 결과를 초래하는지를 이해하는 데 있다 [56].

Shah 등 [66]의 선도적 연구는, cobot이 인간 작업자와 원활하고 효율적으로 협업하기 위해서는 두 가지 협업 원칙을 따라야 한다고 제시했다: (1) 실시간으로 결정을 내리는 능력, (2) 자신의 결정이 팀원에게 미치는 영향을 고려하는 능력. 첫 번째 원칙은 작업 공간 내에서 인간의 동작을 빠르고 정확하게 감지하는 것을 의미한다. 두 번째는 인간 작업자의 자세 궤적을 예측하고, 미래에 발생할 수 있는 충돌을 미리 파악해야 함을 뜻한다.

이러한 문제의식에서 출발하여, 본 논문의 첫 번째 기여는 인간 자세 예측을 위한 새로운 **Separable-Sparse Graph Convolutional Neural Network(SeS-GCN)**를 제안하는 것이다. 자세 예측은 인간의 복잡한 시공간 관절 동역학을 이해해야 하며, 최근에는 이를 단일 GCN 프레임워크로 모델링하는 접근이 유망하다는 것이 밝혀졌다 [15,17,45,47,53,72,77]. 우리는 성능과 효율성을 동시에 고려하여, 처음으로 다음의 세 가지 주요 모델링 원칙을 통합한 SeS-GCN을 설계하였다: **depthwise-separable 그래프 합성곱** [40], **시공간 분리형 인접 행렬** [68], **희소화된 인접 행렬** [67].

SeS-GCN에서 ‘separable’은 관절 간 상호작용(공간), 시점 간 상호작용(시간), 채널별 상호작용(깊이 방향)을 제한하는 것을 의미한다. GCN 내부에서는 여전히 다양한 채널, 프레임, 관절 간의 멀티홉 메시지를 통해 상호작용이 이뤄진다. 희소성은 최초로 **teacher-student 프레임워크**를 통해 달성되었다. 상호작용 제한과 희소성 덕분에, SeS-GCN은 기존 GCN 기반 모델들 [40,68,67]보다 파라미터 수가 적거나 비슷하면서도, 최소 2.7%의 성능 향상을 이룬다.

SeS-GCN은 **최신 기술(State-of-the-Art, SoA)**인 [52]와 비교해 매우 경량화되어 있으며, 파라미터 수는 단 1.72%에 불과하고 약 4배 빠른 속도를 자랑한다. 그럼에도 불구하고 Human3.6M [32] 데이터셋에서 1초 후의 자세를 예측할 때 오차는 단 1.5% 증가하는 수준으로 경쟁력을 유지한다. 모델의 자세한 설명은 3장에서, 실험과 **소거 연구(ablation study)**는 5장에서 다룬다.

우리는 또한 산업 환경에서의 인간-로봇 협업을 위한 첫 번째 벤치마크인 **CHICO(Cobots and Humans in Industrial COllaboration)**를 소개한다 (도표 1 참조). CHICO는 하나의 공유 작업 공간 내에서 KUKA LBR iiwa 로봇 팔과 협업하는 20명의 인간 작업자의 다각도 영상, 3D 자세 및 관절 궤적 데이터를 포함한다. 이 데이터셋은 실제 산업 조립 라인에서 마커 없이 촬영된 7가지 현실적인 작업 동작을 담고 있다. CHICO의 목표는 cobot이 접촉 기반 인간 협업을 수행할 수 있도록 인지 능력을 부여하는 것이다.

이러한 차세대 협업을 위해 CHICO는 두 가지 핵심 과제를 위한 벤치마크를 제시한다: **인간 자세 예측**과 **충돌 감지**. 현재의 cobot은 접촉 렌치, 제어 토크, **감응성 피부(sensitive skin)** 등 기계적인 수단에 의존해 충돌을 감지한다. 이는 안전을 보장하지만, 인간-로봇 간 상호작용을 방해하는 단점이 있다. 충돌은 양쪽의 동작을 중단시키고 생산성을 저하시킬 뿐만 아니라, 인간 작업자에게 불편을 줄 수 있기 때문이다.

CHICO는 1분 분량의 비디오 240개를 포함하고 있으며, 이 중 두 가지 테스트 세트가 구성된다. 하나는 자세 예측 정확도를 평가하기 위한 것이고, 이는 cobot이 가까운 미래(1.0초 후)를 예측할 수 있도록 돕는다. 다른 하나는 226건의 실제 충돌 사례가 포함되어 있으며, 이를 통해 cobot이 충돌을 사전에 예측하고 경로를 재계획할 수 있도록 한다. 데이터셋에 대한 상세 설명은 4장에서, 관련 실험은 6장에서 제시된다.

---

CHICO에서 테스트했을 때, 제안된 SeS-GCN은 모든 기존 모델을 능가하며, 1.00초 후의 예측에서 **평균 관절 위치 오차(MPJPE)**가 85.3mm에 도달했으며, 실행 시간은 단 2.3 밀리초에 불과하였다(표 5 참고). 더불어, 예측된 인간 동작은 cobot과의 충돌 여부를 판단하는 데 활용되며, 이는 예측된 인간 궤적이 cobot의 궤적과 교차하는지를 확인함으로써 이루어진다. SeS-GCN은 이 과정에서 **F1-score 0.64**를 달성하였고, 이는 매우 고무적인 결과다. 이러한 두 가지 성과는 산업용 HRC에서 cobot이 미래를 인지할 수 있도록 하는 중요한 기반이 된다.

⸻

### **2 Related Work (관련 연구)**


인간 자세 예측(Human pose forecasting)
---

**인간 자세 예측**은 비교적 최근에 등장한 분야로, 컴퓨터 비전에서의 **인간 행동 예측** [45] 및 **인간-로봇 협업(HRC)** [18]과 일부 영역이 겹친다. 기존 연구에서는 시간적 정보를 모델링하기에 적합한 구조인 **TCN(Temporal Convolutional Networks)** [2,22,44,62]과 **RNN(Recurrent Neural Networks)** [20,23,34]이 활용되어 왔다. 최근에는 다양한 방법이 추가되어, 공간 구조를 암묵적으로 모델링하는 특정 또는 모델 비종속적인 레이어 [1], **변분 오토인코더(VAE)** [8], **트랜스포머 네트워크** [9] 등이 적용되고 있다.

그래프 합성곱 신경망(GCN)을 이용한 자세 예측
---

최근의 연구들은 대부분 **GCN(그래프 합성곱 신경망)** [17,47,52,68,77]을 사용하고 있다. [52]에서는 관절 간 상호작용을 모델링하기 위해 GCN을 활용하고, 시간적 패턴을 위해 트랜스포머 네트워크를 결합하였다. 다른 연구들 [47,68,77]은 시공간 기반의 신체 운동학을 모델링하기 위해 GCN을 채택하였고, 특히 [17]에서는 거칠게부터 세밀하게 이르는 신체 움직임을 표현하기 위한 계층적 구조를 고안하였다.

우리는 GCN의 효율성을 향상시키기 위한 세 가지 주요 연구 방향을 다음과 같이 정리하였다:

1.  **시공간 분리형 GCN** [68] — 인접 행렬에서 공간적(관절 간) 및 시간적 패턴을 분리하여 표현
2.  **깊이 방향 분리형 그래프 합성곱** [30] — 주파수 영역에서 [3]이 탐구한 방법
3.  **희소화 GCN** [67] — GCN의 인접 행렬 항들을 반복적으로 **가지치기(prune)**함

이 세 가지 기법은 모두 기본 GCN보다 뛰어난 성능을 제공한다는 점에서 주목할 만하다. 본 논문에서는 이 세 가지 요소를 통합하여, 완전한 end-to-end 구조의 **시공간-깊이 방향 분리형 및 희소 GCN**을 제안한다. 이 세 가지 기법은 효율성과 성능을 동시에 향상시키는 데 상호보완적이지만, 이를 통합하려면 구조적인 변경이 필요하며(예: 희소화를 위한 **teacher-student 아키텍처** 채택), 이에 대한 설명은 3장에서 다룬다.

인간-로봇 협업(HRC)
---

**HRC(인간-로봇 협업)**는 인간과 로봇 에이전트가 공동의 목표를 달성하기 위해 함께 작업하는 협업 과정을 연구하는 분야이다 [4,11]. HRC에 관한 컴퓨터 비전 연구는 주로 장면 내에서 관절 기반 인간 신체를 정확히 파악하기 위한 **자세 추정(pose estimation)** [10,21,43]에 집중되어 있다.

[12,35,60]에서는 로봇의 경로 계획 및 충돌 회피를 위한 기법들을 제안하지만, 이들은 본 논문의 관점과 반대되는 입장으로, 인간 작업자가 아닌 로봇에 초점을 맞춘다. 반면에 [5,14,36,48]의 연구는 단순한 바운딩 박스를 이용해 인간의 형상을 근사화함으로써 작업자의 위치를 감지하는 알고리즘을 제시한다. 협업 작업 중 인간 동작을 예측하는 접근으로는, RNN을 활용한 [69,76]과 가우시안 프로세스를 활용한 [71]이 있다. [41]은 인간-로봇 간 물체 전달(hand-over) 상황을 고려하여, 인간의 상반신 및 오른손(‘Human End Effector’로 지칭됨)을 모델링하였다. 본 논문에서는 동작 예측 엔진으로 **DCT-RNN-GCN** [52]을 기준 모델로 삼아 실험에서 비교한다.

자세 예측을 위한 데이터셋
---

인간 자세 예측을 위한 데이터셋은 매우 다양한 시나리오를 포함하고 있으며, 이에 대한 비교 분석은 표 1에 제시되어 있다. 예를 들어, **Human3.6M** [32]은 대화, 식사, 인사, 흡연 등 일상적인 동작을 포함하고 있다.

이 데이터들은 10대의 고속 적외선 카메라로 구성된 3D 마커 기반 모션 캡처 시스템을 통해 수집되었다. **AMASS** [51]는 일상 동작을 광학 마커 기반 모션 캡처로 기록한 15개의 데이터셋 모음이다. Human3.6M과 AMASS는 자세 예측에서 표준 벤치마크로 간주되며, 포함된 동작 유형에서도 일부 겹치는 부분이 있다.

**3DPW 데이터셋** [54]은 실외에서 수행된 동작에 초점을 맞추며, 이동식 카메라와 특수 모션 캡처 수트에 내장된 17개의 **관성 측정 장치(IMU)**를 통해 촬영되었다 [63]. 최근의 **ExPI 데이터셋** [25]은 전문 무용수들이 수행한 16가지의 다양한 춤 동작(총 115개 시퀀스)을 포함하며, 동작 예측을 목표로 한다. ExPI는 68대의 동기화 및 보정된 컬러 카메라와 20대의 모션 캡처 카메라로 수집되었다.

마지막으로 **CHI3D 데이터셋** [19]은 인간 간 상호작용을 연구하기 위해 MOCAP 시스템으로 수집된 3D 데이터를 제공한다.

하지만 위의 어떤 데이터셋도 본 연구의 요구를 충족시키지 못한다. 즉, 에너지 효율적이고 **마커가 필요 없는(markerless)** 방식으로 수집되며, **인간-로봇 협업(HRC)**의 산업 현장 시나리오에 초점을 맞춘 벤치마크가 필요하다. 특히 이러한 시나리오에서는 충돌을 예측하는 데 자세 예측이 실질적으로 유용하다.

실제로 산업 응용 분야와 관련된 데이터셋은 **InHARD** [16] 하나뿐이다. 이 데이터셋에서는 피험자들이 각각의 팔다리에 관성 센서를 착용한 상태에서 조립 작업을 수행하게 된다. InHARD는 **인간 동작 인식(HAR)**을 목적으로 설계되었으며, 총 16명이 각기 다른 13가지 동작을 수행하며 생성된 4800개의 동작 샘플과 200만 개 이상의 프레임을 포함한다. 협업 로봇이 등장하긴 하지만, 해당 데이터셋의 로봇은 대부분 정지 상태에 있어, 충돌 예측 용도로는 적합하지 않다.

3. 방법론
===

본 연구에서는 다음 세 가지 다양한 연구 방향을 결합함으로써 정확하면서도 메모리 효율적이며 빠른 GCN을 구축하였다:

* i. **시공간 분리형 인접 행렬**,
* ii. **깊이 방향 분리형 그래프 합성곱**,
* iii. **희소 인접 행렬**.

이로 인해 우리는 인간 신체 운동학을 인코딩할 수 있는 전면 분리형 및 희소 GCN 인코더를 완성하였으며, 이를 **SeS-GCN**이라 명명한다. 이후 미래 프레임은 **Temporal Convolutional Network (TCN)**를 통해 예측된다.

3.1. 배경
---

**문제 정의**. 자세 예측은 T개의 프레임 동안 V개의 관절에 대한 3차원 좌표 x_{v,t}를 관측한 후, 미래의 K개 프레임에서 해당 관절들의 위치를 예측하는 문제로 정식화된다. 표기 편의를 위해, 각 프레임 t에서 모든 관절의 좌표를 행렬 X_t = [x_{v,t}]{v=1}^{V} \\in \\mathbb{R}^{3 \\times V}로 정의한다. 이어서 입력 프레임들을 포함하는 텐서 X{in} = [X_1, X_2, …, X_T], 예측 대상인 출력 프레임들을 포함하는 텐서 X_{out} = [X_{T+1}, …, X_{T+K}]를 정의한다.

우리는 신체 운동학을 인코딩하기 위해 그래프 G = (V, E)를 고려한다. 여기서 노드 집합 V는 관측된 모든 프레임에서의 관절들로 구성되며 V = \\{v_{i,t}\\}_{i=1,t=1}^{V,T}로 정의된다. 엣지 (v{i,t}, v_{j,s}) \\in E는 프레임 t와 s에서 각각 관절 i와 j를 연결한다.

**그래프 합성곱 신경망 (GCN)**. GCN은 다음과 같은 층 기반 구조로 구성된다:

X^{(l+1)} = \\sigma(A^{(l)} X^{(l)} W^{(l)})

GCN의 각 층 l의 입력은 텐서 X^{(l)} \\in \\mathbb{R}^{C^{(l)} \\times V \\times T}이며, 이는 V개의 관절과 T개의 관측 프레임과의 대응 관계를 유지하되, 특성의 깊이를 C^{(l)}개의 채널로 확장한다. 첫 번째 층의 입력은 X^{(1)} = X_{in}이며, 이 경우 C^{(1)} = 3, 즉 3차원 좌표 값으로 구성된다. A^{(l)} \\in \\mathbb{R}^{VT \\times VT}는 모든 프레임에서의 관절 쌍들 간의 관계를 나타내는 인접 행렬이며, 최신 연구들에 따라 학습되는 구조이다. W^{(l)} \\in \\mathbb{R}^{C^{(l)}}는 그래프 합성곱의 학습 가능한 가중치이며, \\sigma는 비선형 PReLU 활성화 함수이다.

---

3.2. 분리형 및 희소 그래프 합성곱 신경망 (SeS-GCN)
---

우리는 앞서 언급한 세 가지 모델링 차원을 통합하여 **SeS-GCN**을 설계하였다:

* i. GCN의 인접 행렬 내에서 공간 및 시간 상호작용 항을 분리하고,
* ii. 그래프 합성곱을 깊이 방향으로 분리하며,
* iii. GCN의 인접 행렬을 희소화하는 것이다.

**시공간 분리**. STS-GCN [68]에서는 각 GCN 층 l에서 인접 행렬 A^{(l)}을 두 개의 항, 즉 시간-시간 관계를 담당하는 A^{(l)}_s \\in \\mathbb{R}^{V \\times V \\times T}와 관절-관절 관계를 담당하는 A^{(l)}_t \\in \\mathbb{R}^{T \\times T \\times V}로 분해하였다. 이로 인해 GCN은 다음과 같이 표현된다:

X^{(l+1)} = \\sigma\\left(A^{(l)}_s A^{(l)}_t X^{(l)} W^{(l)}\\right)

식 (2)는 서로 다른 프레임 간의 관절 상호작용을 병목 처리하며, 동일 프레임 내 관절 간의 상호작용(A_s^{(l)})과 각 관절의 시간적 패턴(A_t^{(l)})에 더 많은 비중을 두게 된다. 이 접근은 GCN의 메모리 사용량을 약 4배 줄이면서 성능은 오히려 향상시키는 효과를 보였다(자세한 내용은 5.1절 참조). 이 방법은 [74], [7]에서 경로 예측과 동작 인식을 위해 공간과 시간 모듈을 번갈아 사용하는 방식과는 다르다는 점에 주의해야 한다.

**깊이 방향 분리**. **깊이 방향 합성곱(depth-wise convolution)** [13,30]에서 영감을 받은 [40]은 이미지 분류를 위해 깊이 방향 그래프 합성곱을 도입하였고, 이후 [3]에서는 이를 그래프 분류에 적합하도록 주파수 기반으로 확장하였다. 본 논문에서는 자세 예측에 이 구조를 적용하였다. 이 깊이 방향 구조는 인접 행렬 A^{(l)}에 의해 처리되는 공간과 시간 간 상호작용을, 그래프 합성곱 가중치 W^{(l)}의 채널 간 상호작용과 분리시킨다. 그 결과 탄생한 완전 분리형 모델은 **STS-DW-GCN**이라 불리며, 다음과 같이 정의된다:

$$H^{(l)} = \\gamma\\left((A^{(l)}_s A^{(l)}_t) X^{(l)} W^{(l)}_{DW}\\right) \\tag{3a}$$

$$X^{(l+1)} = \\sigma\\left(H^{(l)} W^{(l)}_{MLP}\\right) \\tag{3b}$$

깊이 방향 그래프 합성곱을 도입함으로써, 각 GCN 층 l은 두 개의 항으로 분리된다. 첫 번째 항(식 3a)은 시공간 간 상호작용에 초점을 맞추고, W^{(l)}_{DW} \\in \\mathbb{R}^{C^{(l)}\\alpha \\times 1 \\times 1}를 사용하여 채널 간 간섭을 제한한다. 여기서 1 \\leq \\alpha \\leq C^{(l)}는 합성곱 그룹의 수를 조절하는 하이퍼파라미터이다. 두 번째 항(식 3b)은 채널 내부 간 통신만을 모델링하며, 이는 단순한 1D MLP 합성곱 W^{(l)}_{MLP} \\in \\mathbb{R}^{C^{(l)} \\times 1 \\times 1}으로, 특성을 C^{(l)}에서 C^{(l+1)}로 재매핑한다. 여기서 \\gamma는 ReLU6 비선형 활성화 함수이다. 이 구조는 파라미터 수를 크게 줄이진 않지만, GCN의 깊이를 증가시켜 성능을 향상시키며, **과도한 평활화(over-smoothing)**를 방지한다 (자세한 내용은 5.1절 참조).

GCN 희소화(Sparsifying the GCN)
---

**희소화(sparsification)**는 신경망의 효율성(메모리, 경우에 따라 실행 시간)을 향상시키기 위해 오랫동안 사용되어 왔으며, 그 출발점은 [42]의 **가지치기(pruning)** 연구였다. [67]에서는 경로 예측을 위해 GCN을 희소화한 바 있으며, 이는 GCN의 인접 행렬에서 특정 파라미터를 선택적으로 제거하기 위한 마스크 M을 학습하는 방식이다. 본 연구에서는 이러한 희소화를 완전 분리형 GCN 설계와 통합하여, 인간 자세 예측을 위한 최종 모델인 **SeS-GCN**을 제안한다:

$$H^{(l)} = \\gamma \\left( (M_s^{(l)} \\odot A_s^{(l)})(M_t^{(l)} \\odot A_t^{(l)}) X^{(l)} W_{DW}^{(l)} \\right) \\tag{4a}$$

$$X^{(l+1)} = \\sigma \\left( H^{(l)} W_{MLP}^{(l)} \\right) \\tag{4b}$$

여기서 \\odot는 **요소별 곱(element-wise product)**을 의미하며, M^{(l)}_{\{s,t\}}는 이진 마스크이다. [67]에서는 학습 및 추론 과정 모두에서 마스크를 생성하여, 인접 행렬 A의 일부 계수를 0으로 만들고, 이렇게 희소화된 GCN을 경로 예측에 활용하였다. 이에 반해, 본 연구에서는 학습 중 **teacher-student 프레임워크**를 채택하였다. teacher 모델이 마스크를 학습하며, student 모델은 해당 마스크에 의해 선택된 인접 항만을 사용한다. 추론 시에는 오직 student 모델만을 사용하며, 학습된 희소 인접 행렬 A_s와 A_t를 그대로 활용한다. [67]과 비교했을 때, SeS-GCN은 학습 과정에서 더 안정적이며, 추론 시 모델 파라미터 수가 약 30% 더 적고, 성능도 더 우수하다(자세한 내용은 5.1절 참조).

---

3.3. 디코더를 통한 예측
---

SeS-GCN에 의해 인코딩된 시공간 표현을 바탕으로, 미래 프레임들은 **Temporal Convolutional Network(TCN)** [2,22,44,68]을 통해 디코딩된다. TCN은 시간 축을 재매핑하여 원하는 개수의 예측 프레임에 맞도록 조정한다. 이 디코더 부분은 이미 효율적이며 만족스러운 성능을 보이기 때문에, 본 논문에서는 별도의 개선 대상으로 삼지 않았다.

---

4. CHICO 데이터셋
===

이 섹션에서는 CHICO 데이터셋의 수집 환경과 장비, 사용된 **협업 로봇(cobot)**, 수행된 동작들에 대해 자세히 설명한다. 우리는 RGB 비디오, 스켈레톤(관절 좌표), 그리고 보정 파라미터를 함께 공개한다‡.

‡코드와 데이터셋은 다음 링크에서 확인 가능하다: [<https://github.com/AlessioSam/CHICO-PoseForecasting>](https://github.com/AlessioSam/CHICO-PoseForecasting)

**작업 환경**. 본 데이터셋은 스마트 팩토리 환경에서 수집되었으며, 작업자는 0.9m × 0.6m 크기의 작업대 앞에 서 있고, 그 끝에는 **협업 로봇(cobot)**이 배치되어 있다(그림 1 참조). 작업자는 장비 쪽으로 몸을 돌려 조립, 적재, 하역 작업 등을 수행할 수 있는 여유 공간을 가진다 [57]. 사용된 도구로는 가벼운 플라스틱 부품, 무거운 타일, 망치, 연마 스펀지 등이 있다. 각 작업의 세부 구성은 보조 자료에 도식화되어 있다. 본 연구에는 총 20명의 인간 작업자가 참여하였으며, cobot 사용법에 대한 교육을 받고, 녹화 전에 동의서에 서명하였다.

**협업 로봇**. 데이터 수집 과정에서 인간 작업자와 협업한 로봇은 **7자유도(DoF)**를 가진 Kuka LBR iiwa 14 R820 모델이다. 무게는 29.5kg이며 최대 14kg까지의 하중을 처리할 수 있어, 현대 산업 현장에서 널리 사용되고 있다. 로봇에 대한 추가 정보는 보조 자료에서 확인할 수 있다.

**데이터 수집 구성**. 본 수집 시스템은 RGB HD 카메라 3대로 구성되며, 동일한 작업 공간을 두 개의 전측면 시점과 하나의 후면 시점에서 촬영한다. 프레임 레이트는 25Hz이다. 영상은 오류나 이상 프레임이 있는지 먼저 점검된 후, **Voxelpose** [70]를 이용해 각 프레임에서 3D 인간 자세를 추출하였다. 각 카메라의 외부 보정 파라미터는 로봇 기준 좌표계에 대해 1×1m 크기의 체스보드로 보정되었으며, 모든 구성 요소는 인터넷 타임 서버를 통해 시간 동기화되었다.

우리 환경에서 Voxelpose는 세 개의 카메라를 활용하여 **평균 관절 위치 오차(MPJPE)** 기준 약 24.99mm의 정확도를 달성하였으며, 이는 시스템의 휴대성과 정확도 간 균형을 고려할 때 목적에 충분한 성능이다. 우리는 이 수치를 두 가지 방식으로 검증하였다. 첫째, 인간과 cobot 간 충돌이 **F1-score 100%**로 정확히 감지되었는지 확인하였다(충돌은 인간 사지와 로봇 링크 간 최소 거리가 미리 정의된 임계값보다 작을 때 발생함). 둘째, 새로운 CHICO 데이터셋은 단순한 **’정지 모델(zero velocity model)’**로는 최신 기법만큼 성능이 나오지 않음을 보여주며, 이는 대규모 벤치마크 Human3.6M과 동일한 경향이다 [55].

**작업 (Actions)**. CHICO의 7가지 작업 유형은 [29]의 리뷰 논문에서 설명된 **HRC(인간-로봇 협업)** 해체 작업 라인에서의 일반적인 작업 세션에서 영감을 받아 설계되었다. 각 작업은 평균 약 1분간 반복 수행되며, 각각의 작업에는 인간 작업자가 정해진 시간 안에 달성해야 할 목표가 설정되어 있다. 이로 인해 작업자는 일정 속도로 움직여야 한다.

각 작업은 로봇과의 반복적인 상호작용(예: 로봇이 배치하고 인간이 집어 들기)으로 구성되어 있으며, 제한된 공간 내에서 수행되기 때문에 **비제약 충돌(unconstrained collisions)**이 발생한다. 우리는 이러한 충돌을 라벨링하였다. 전체적으로 7가지 작업 × 20명의 작업자 구성에서 총 226건의 서로 다른 충돌 데이터를 수집하였다. 충돌 장면을 포함한 비디오 일부는 보조 자료에 포함되어 있다.

아래에는 각 작업을 간략히 설명한다.

※ ***비제약 충돌(unconstrained collisions)***은 [26]에서 정의된 용어로, 충돌에 오직 인간과 로봇만이 직접적으로 관여하는 상황을 의미한다.

CHICO의 7가지 작업 설명
---

* **경량 물체 운반 (Lightweight Pick and Place, Light P&P)**
  인간 작업자는 약 50g 무게의 소형 물체를 적재 구역에서 반대편의 전달 지점까지 일정 시간 내에 옮겨야 한다. 이때 로봇은 적재 구역에서 작업을 수행하고 있어, 작업자는 로봇 팔 근처를 지나야 하며, 많은 경우 인간의 사지와 로봇 팔 사이 거리는 수 센티미터 이내로 좁다.
* **중량 물체 운반 (Heavyweight Pick and Place, Heavy P&P)**
  작업 환경은 Light P&P와 동일하지만, 옮겨야 하는 물체는 무게가 0.75kg에 달하는 바닥 타일이다. 따라서 이 작업은 양손을 사용하여 수행해야 한다.
* **표면 연마 (Surface Polishing)**
  이 작업은 [50]의 사례에서 영감을 받았으며, 인간 작업자가 40×60cm 크기의 타일 가장자리를 연마 스펀지를 이용해 닦는 동안, 로봇은 시각적 품질 검사를 흉내 낸다.
* **정밀 물체 이동 (Precision Pick and Place, Prec. P&P)**
  로봇이 30×30cm 크기의 작업대 중앙 테이블 네 모서리에 플라스틱 부품 4개를 배치하면, 인간은 이를 제거하여 적재 구역에 놓아야 한다. 이후 로봇이 동일한 방식으로 다시 적재를 반복한다.
* **무작위 물체 이동 (Random Pick and Place, Rnd. P&P)**
  Prec. P&P와 유사하지만, 플라스틱 부품이 로봇에 의해 무작위 위치에 연속적으로 배치된다. 인간 작업자는 이를 실시간으로 제거해야 한다.
* **높은 선반 적재 (High Shelf Lifting, High Lift)**
  로봇이 측면 적재 구역에 가벼운 플라스틱 부품(각 50g)을 놓으면, 인간은 이를 집어 작업대 반대편의 1.70m 높이 선반에 올려야 한다. 공간 구조상 인간의 팔은 움직이는 로봇 팔 위나 아래를 통과해야 하므로, 로봇 링크와 인간의 팔 또는 팔뚝 사이에 매우 근접한 거리가 형성된다.
* **망치질 (Hammering)**
  작업자는 로봇이 고정하고 있는 금속 타일을 망치로 두드린다. 이 작업의 목적은, 인간의 팔이 실제로 로봇에 충돌하지 않더라도, 로봇 팔 근처(금속 타일 부분)에서 충돌에 가까운 동작이 발생했을 때 충돌 감지 시스템이 얼마나 견고하게 작동하는지를 검증하는 것이다.

---

평가 지표 (Metric)
---

예측 오차는 **MPJPE (Mean Per Joint Position Error)** 지표를 통해 정량화된다 [32,53].
이 지표는 특정 미래 프레임 t에서 예측된 3D 좌표와 실제 정답(ground truth) 사이의 변위를 밀리미터 단위로 측정한 것이다.

$$\\begin{equation}\\text{L}_{MPJPE} = \\frac{1}{V} \\sum_{v=1}^{V} \\| \\hat{x}_{vt} - x_{vt} \\|_2\\end{equation}$$

여기서:

* V: 관절의 개수
* \\hat{x}_{vt}: 예측된 관절 v의 좌표 (시간 t에서)
* x_{vt}: 실제 관절 v의 좌표 (시간 t에서)

5. SeS-GCN의 모델링 선택
===

---

효율적인 GCN 기반 모델 (Efficient GCN baselines)
---

표 2에서는 효율적인 GCN 설계를 위한 세 가지 서로 다른 접근법을 검증하였다. 구체적으로는 공간-시간 분리형 **STS-GCN** [68], 깊이 방향 분리 그래프 합성곱인 **DW-GCN** [40], 그리고 희소 그래프 기반의 **Sparse-GCN** [67]이 그것이다.

STS-GCN은 가장 적은 수의 파라미터(57.6K, 약 4배 더 적음)로, 1초 후 예측에서 MPJPE 오차가 117.0mm로 가장 낮았으며, DW-GCN 대비 2.4%, Sparse-GCN 대비 4.8% 더 우수한 성능을 보였다. 따라서 우리는 이 접근법을 기반으로 삼았다.

---

더 깊은 GCN (Deeper GCNs)
---

**심층 신경망(Deep Neural Networks, DNN)**의 성능은 종종 네트워크의 깊이에 기인한다고 여겨진다 [27,49,73,75]. 그러나 모델이 깊어질수록 파라미터 수가 증가하고 처리 시간이 길어지며, 특히 GCN에서는 **over-smoothing 문제**가 발생할 수 있다 [61].

정확도와 효율성 모두를 향상시키기 위해 다음 세 가지 전략을 고려하였다:

1.  GCN 레이어 수를 증가
2.  GCN 레이어 사이에 MLP 레이어 추가
3.  GCN 내부에 깊이 방향 분리 합성곱을 적용 (이 경우 MLP도 포함됨, 3.2절 참조)

표 2에 따르면, STS-GCN 레이어를 5개로 늘렸을 때 성능이 다소 개선되어 MPJPE는 115.9mm로 낮아졌다. 하지만 6개 이상으로 늘리면 오히려 성능이 저하된다.

GCN 사이에 MLP 레이어를 추가한 경우(5+5 레이어 구성)에는 성능이 크게 감소하여 MPJPE가 125.2mm로 높아졌다. 반면, **깊이 방향 분리 그래프 합성곱(5+5 구성)**을 적용한 STS-DW-GCN에서는 MPJPE가 114.8mm로 감소했다.

이는 기존 문헌 [13,40,68]에서도 언급된 바와 같이, 채널 간 간섭을 줄이면서 깊이를 증가시켰기 때문으로 해석할 수 있다.

📊 **표 2. Human3.6M (25프레임 예측 기준)에서 MPJPE (mm) 및 파라미터 수 비교**

| 모델                  | 깊이 | MPJPE (mm) | 파라미터 수 (K) | DW-Separable | ST-Separable | Sparse | MLP 포함 | Teacher-Student |
| :-------------------- | :--- | :--------- | :-------------- | :----------- | :----------- | :----- | :------- | :-------------- |
| GCN                   | 4    | 123.2      | 222.7           |              |              |        |          |                 |
| DW-GCN [40]           | 4+4  | 119.8      | 223.2           | ✅           | ✅           |        |          |                 |
| STS-GCN† [68]         | 4    | 117.0      | 57.6            |              | ✅           |        |          |                 |
| Sparse-GCN [67]       | 4    | 122.7      | 257.9           |              | ✅           | ✅     |          |                 |
| STS-GCN               | 5    | 115.9      | 68.6            |              | ✅           |        |          |                 |
| STS-GCN               | 6    | 116.1      | 79.9            |              | ✅           |        |          |                 |
| STS-GCN + MLP         | 5+5  | 125.2      | 101.4           |              | ✅           |        | ✅       |                 |
| STS-DW-GCN            | 5+5  | 114.8      | 70.0            | ✅           | ✅           |        | ✅       |                 |
| STS-DW-Sparse-GCN     | 5+5  | 115.7      | 122.4           | ✅           | ✅           | ✅     | ✅       |                 |
| SeS-GCN (proposed)    | 5+5  | 113.9      | 58.6            | ✅           | ✅           | ✅     | ✅       | ✅              |

---

GCN 희소화 및 제안된 SeS-GCN (Sparsifying GCNs and the proposed SeS-GCN)
---

마지막으로, 모델 압축을 통해 효율성을 향상시키고자 하였다. 최근 추세는 파라미터 정밀도 감소 [64], 일부 파라미터 **가지치기** 및 희소화 [59], 또는 **teacher-student 프레임워크** 활용 [28,46] 등을 통해 모델 크기를 줄이고 있다. 이 중 마지막 방법은 거대 모델(예: Transformers)을 실제 배포할 때도 널리 사용되고 있다 [6].

먼저, Sparse-GCN [67] 방식에 따라 희소 인접 행렬을 사용해 모델을 압축하였다. 해당 방식은 파라미터와 마스크를 반복적으로 최적화하며, 마스크 선택은 네트워크 브랜치를 통해 수행된다(추론 시에도 적용됨, 3.2절 참조). 그러나 표 2에 나타난 것처럼, 이 접근법(STS-DW-Sparse-GCN)은 오차가 115.7mm로 증가하고 파라미터 수도 122.4K로 늘어나면서 효과적인 방향은 아닌 것으로 판단된다.

이에 따라 우리는 **teacher-student 모델** 전략을 활용한 **SeS-GCN**을 제안하였다. 먼저 teacher 모델로 STS-DW-GCN을 학습시킨 후, 그 파라미터를 활용해 student 모델의 인접 행렬을 희소화한다. 이후 student는 처음부터 다시 학습된다.

이렇게 설계된 **SeS-GCN은 가장 낮은 MPJPE 오차(113.9mm)**를 기록하며, 최신 SoA 모델 [52]과 유사한 성능을 달성하면서도 **파라미터 수는 단 1.72% (58.6K vs. 3.4M)**에 불과하다.

---

5.2. 최신 기법(State-of-the-Art, SoA)과의 비교
---

표 2에서는 제안한 SeS-GCN을 최근의 세 가지 최신 기법들과 비교하여, **짧은 예측 구간(10프레임, 400밀리초)**과 **긴 예측 구간(25프레임, 1000밀리초)** 모두에서 성능을 평가하였다.

첫 번째는 현재의 SoA로 간주되는 **DCT-RNN-GCN** [52]이며, 이 모델은 DCT 인코딩, **모션 어텐션(motion attention)**, RNN을 활용한다. 이 모델은 다른 모델들과 달리 입력 프레임 수가 많다(50프레임 vs. 일반적으로 10프레임).

다른 두 모델은 **MSR-GCN** [17]과 **STS-GCN** [68]이며, 모두 GCN만을 사용한 프레임워크다. MSR-GCN은 다중 스케일 접근법을 채택하였고, STS-GCN은 공간 및 시간 인코딩을 분리하여 처리한다.

제안된 SeS-GCN은 짧은 예측 구간(400ms)과 긴 예측 구간(1000ms) 모두에서 다른 기술들 [68,52]보다 뛰어난 성능을 보이며, 현재 SoA인 [52] 대비 오차는 단 1.5% 이내이고, 파라미터 수는 단 1.72%, 속도는 약 4배 빠르다.

6. CHICO에서의 실험
===

우리는 CHICO 데이터셋에서 기존 SoA 모델들과 제안된 SeS-GCN 모델을 벤치마킹하였다. 두 가지 HRC 과제인 **인간 자세 예측**과 **충돌 감지**는 각각 6.1절과 6.2절에서 다룬다.

†참고: STS-GCN의 성능은 원 논문 [68]과 차이가 있으며, 저자에 의해 결과가 수정되었음. ([<https://github.com/FraLuca/STSGCN>](https://github.com/FraLuca/STSGCN) 참조)

---

📊 **표 3. Human3.6M에서 3D 관절 위치 예측 성능 (MPJPE, mm 단위)**

**설명**:
Human3.6M 데이터셋에서 짧은 예측 구간(400ms, 10프레임)과 긴 예측 구간(1000ms, 25프레임)에 대한 MPJPE 성능을 나타낸다. 제안된 SeS-GCN 모델은 기존 최신 기법 [52]과 유사한 성능을 보이면서도 파라미터 수는 1.72%에 불과하고, 실행 속도는 약 4배 더 빠르다 (표 5 참조). 결과 분석은 5.2절에 포함되어 있다.

| 동작             | 400ms DCT-RNN-GCN | 400ms MSR-GCN | 400ms STS-GCN | 400ms SeS-GCN | 1000ms DCT-RNN-GCN | 1000ms MSR-GCN | 1000ms STS-GCN | 1000ms SeS-GCN |
| :--------------- | :---------------- | :------------ | :------------ | :------------ | :----------------- | :------------- | :------------- | :------------- |
| Walking          | 39.8              | 45.2          | 51.0          | 48.8          | 58.1               | 63.0           | 70.2           | 67.3           |
| Eating           | 36.2              | 40.4          | 43.3          | 41.7          | 75.5               | 77.1           | 82.6           | 78.1           |
| Smoking          | 36.4              | 38.1          | 42.3          | 40.8          | 69.5               | 71.6           | 76.1           | 73.7           |
| Discussion       | 65.4              | 69.7          | 71.9          | 70.6          | 119.8              | 117.5          | 118.9          | 116.7          |
| Directions       | 56.5              | 53.8          | 63.2          | 60.3          | 106.5              | 100.5          | 109.6          | 106.9          |
| Greeting         | 78.1              | 93.3          | 86.4          | 83.8          | 138.8              | 147.2          | 136.1          | 137.2          |
| Phoning          | 49.2              | 51.2          | 53.8          | 52.6          | 105.0              | 104.3          | 108.3          | 106.7          |
| Posing           | 75.8              | 85.0          | 84.7          | 82.6          | 178.2              | 174.3          | 178.4          | 173.5          |
| Purchases        | 73.9              | 79.6          | 83.1          | 82.2          | 134.2              | 139.1          | 141.0          | 139.1          |
| Sitting          | 56.0              | 57.8          | 60.8          | 59.9          | 115.9              | 120.0          | 121.4          | 117.5          |
| Sitting Down     | 72.0              | 76.8          | 79.4          | 78.1          | 143.6              | 155.4          | 148.4          | 146.0          |
| Taking Photo     | 51.5              | 56.3          | 59.4          | 57.7          | 115.9              | 121.8          | 126.3          | 121.2          |
| Waiting          | 54.9              | 59.2          | 62.0          | 58.5          | 108.2              | 106.2          | 113.6          | 107.5          |
| Walking Dog      | 86.3              | 93.3          | 97.3          | 94.0          | 146.9              | 148.2          | 151.5          | 147.7          |
| Walking Together | 41.9              | 43.8          | 49.1          | 48.3          | 64.9               | 65.9           | 72.5           | 70.8           |
| 평균             | 58.3              | 62.9          | 65.8          | 64.0          | 112.1              | 114.1          | 117.0          | 113.9          |

---

📊 **표 4. CHICO에서 3D 관절 위치 예측 성능 (MPJPE, mm 단위)**

**설명**:
CHICO 데이터셋에서 400ms와 1000ms 예측 구간에 대한 MPJPE 성능. 제안된 SeS-GCN은 단기 예측에서 평균 오차가 다른 모델보다 7.9% 낮으며, 장기 예측에서는 2.4% 더 낮다. 분석은 6.1절 참조.

| 작업        | 400ms DCT-RNN-GCN | 400ms MSR-GCN | 400ms STS-GCN | 400ms SeS-GCN | 1000ms DCT-RNN-GCN | 1000ms MSR-GCN | 1000ms STS-GCN | 1000ms SeS-GCN |
| :---------- | :---------------- | :------------ | :------------ | :------------ | :----------------- | :------------- | :------------- | :------------- |
| Hammer      | 41.1              | 39.0          | 41.6          | 40.9          | 69.4               | 67.8           | 64.2           | 49.3           |
| High Lift   | 50.6              | 50.2          | 48.3          | 46.0          | 83.3               | 81.3           | 79.5           | 77.4           |
| Prec. P&P   | 52.7              | 53.4          | 52.0          | 48.4          | 88.2               | 90.3           | 87.9           | 84.8           |
| Rnd. P&P    | 42.1              | 41.1          | 42.1          | 38.8          | 76.0               | 73.2           | 73.9           | 72.4           |
| Polishing   | 64.1              | 62.7          | 60.6          | 56.1          | 121.5              | 118.2          | 106.5          | 104.4          |
| Heavy P&P   | 62.1              | 61.5          | 57.2          | 56.2          | 104.2              | 101.9          | 95.2           | 92.2           |
| Light P&P   | 54.6              | 54.1          | 53.0          | 48.8          | 91.6               | 90.7           | 87.4           | 85.3           |

---

6.1. 자세 예측 벤치마크
---

---

평가 프로토콜
---

CHICO 데이터셋에 대해, 다음과 같은 방식으로 학습/검증/테스트 세트를 나누었다:

* **검증 세트**: 피험자 0번과 4번
* **테스트 세트**: 피험자 2, 3, 18, 19번
* **학습 세트**: 나머지 14명

단기 예측 실험은 Human3.6M [32]의 설정을 따라, 관측 시간은 10프레임, 예측 구간은 10 또는 25프레임으로 설정하였다. 단, DCT-RNN-GCN은 다른 모델들과 달리 50프레임의 입력이 필요하다.

MPJPE는 Human3.6M과 동일하게 Eq. (5)를 사용하며, 모든 평가된 기법들의 학습 손실 함수로도 사용된다.

참고로, 자세 예측에 사용된 모션 시퀀스에는 충돌이 포함되지 않는다. 이 실험의 목적은 충돌로 인한 인간의 회피 동작이나 정지를 학습하는 것이 아니라, 정상적인 협업 동작을 모델링하는 데에 있다.

---

평가 프로토콜 (Evaluation protocol)
---

CHICO 데이터셋을 위한 평가 프로토콜은 다음과 같이 구성된다.
검증/학습/테스트 세트는 다음 기준으로 나뉜다:

* **검증용 피험자**: 0번, 4번
* **테스트용 피험자**: 2번, 3번, 18번, 19번
* **학습용 피험자**: 나머지 14명

단기 예측 실험에서는 Human3.6M [32] 설정을 준수하여, **10프레임을 관측 시간(observation time)**으로 사용하고, **10프레임 또는 25프레임을 예측 구간(forecasting horizon)**으로 설정한다.

다른 모든 기법들과 달리, DCT-RNN-GCN은 50프레임의 입력을 필요로 한다.

우리는 Human3.6M과 동일한 **MPJPE (Mean Per Joint Position Error)** 지표 [32]를 사용하며, 이는 본 논문에서 평가한 모든 기법의 **학습 손실 함수(loss function)**로도 활용된다.

참고로, 자세 예측에 사용된 모든 모션 시퀀스에는 충돌이 포함되어 있지 않다.
이 실험의 목적은 충돌 회피 동작이나 정지 동작을 학습하는 것이 아니라, 정상적인 협업 상황에서의 바람직한 인간 행동을 학습하고 평가하는 데 있다.

∗∗ 이 문장은 충돌로 인한 회피(retractions) 또는 일시 정지(pauses)가 포함된 행동은 본 실험의 훈련 및 테스트 대상이 아님을 강조합니다.

비교 평가 (Comparative Evaluation)
---

표 4에서는 기존 최신 기술들과 제안된 SeS-GCN 모델 간의 자세 예측 성능을 비교한다.

* **단기 예측(400ms)**에서는 SeS-GCN이 가장 뛰어난 성능을 보였으며, MPJPE 오차는 48.8mm로, 두 번째로 성능이 좋은 STS-GCN [68]보다 7.9% 낮은 오차를 기록하였다.
* **장기 예측(1000ms)**에서도 SeS-GCN이 최고 성능을 기록하였으며, MPJPE 오차는 85.3mm로, 역시 STS-GCN [68]보다 2.4% 낮다.

제안된 SeS-GCN 모델은 Hammer 작업을 제외한 모든 작업에서 기존 기법들을 능가한다. Hammer는 단시간에 반복되는 작업이며, 망치질 한 번 한 번의 차이가 크기 때문에 개인별 차이가 클 수 있다.

한편, DCT-RNN-GCN [52]는 입력 프레임 수가 50개로 많기 때문에 성능상 이점을 가질 수 있다, 다른 모든 기법들은 10프레임을 입력으로 사용한다.

---

시각적 분석(Fig. 10)
---

400ms (왼쪽) 및 1000ms (오른쪽) 예측 구간에서의 관절별 오차 분포가 제시된다.
양쪽 모두에서, 관절의 오차는 **신체 운동 사슬(kinematic skeleton)**의 끝점에 가까울수록 더 커지는 경향을 보이는데, 이는 해당 관절들이 가장 많이 움직이기 때문이다.

예를 들어, **오른손의 예측 오차는 각각 70.03mm (400ms), 125.76mm (1000ms)**로 가장 크며, 이는 대부분의 피험자들이 오른손잡이인 사실과 일치한다. 다만 일부 작업에서는 양손을 사용하는 경우도 있었다.

---

기준선 모델(Sanity check)
---

**정지 속도(zero velocity) 모델**의 성능도 평가하였다.
[55]에 따르면, 마지막 관측된 자세를 그대로 유지하는 단순 모델이 의외로 강력한 기준선 역할을 할 수 있다고 보고되었다.

CHICO 데이터셋에서 **zero velocity 모델**의 성능은 25프레임 예측에서 MPJPE 110.6mm로, **SeS-GCN(85.3mm)**보다 훨씬 열악했다.
이는 대규모 데이터셋 Human3.6M [32]에서도 유사한 경향을 보이며, 해당 데이터셋에서 정지 속도 모델의 성능은 153.3mm이다.

---

∗∗ 참고: 충돌이 발생한 후, 로봇은 1초간 작동을 멈추며, 이 동안 인간 작업자는 로봇이 작업을 재개할 때까지 대기 상태를 유지하는 경우가 많다.

6.2. 충돌 감지 실험
---

---

평가 프로토콜 (Evaluation protocol)
---

우리는 다음 조건을 기준으로 **충돌(collision)**을 정의한다:
인간 피험자의 신체 부위(limb)가 로봇(cobot)의 어떤 부위와도 일정 거리 이하로 가까워질 경우,
즉, 단 하나의 프레임이라도 두 객체 간 거리가 임계값 이내로 좁혀지면 충돌로 간주한다.

보다 구체적으로, 충돌은 **예측 대상 구간(forecast portion)**에서 인간과 로봇 간의 근접을 의미한다.
유클리드 거리 기준 임계값은 13cm로 설정하였다.

* 로봇의 움직임은 사전에 스크립트화되어 있어 완전히 알려진 상태이다.
* 반면, 인간 피험자의 향후 1000ms 동안의 동작은 예측해야 한다.
* 관측 구간은 400ms이고, 평가에 사용되는 시퀀스는 10+25 프레임 구성이며, stride는 10으로 설정된다.

---

충돌 감지 평가 (Evaluation of collision detection)
---

충돌 여부를 평가하기 위해 [58]을 참고하여 다음과 같은 간략화 모델을 사용하였다:

* 로봇의 팔 파트와 인간의 사지(limb)는 **원기둥(cylinder)** 형태로 근사화되며,
* 로봇의 각 부위 직경은 고정값 8cm,
* 인간 사지의 직경은 **인체 아틀라스(human atlas)**에서 추출한 평균값을 사용하였다.

표 5에는 테스트 피험자 2명의 모션에 대해 충돌 감지 결과로 **정밀도(Precision)**, **재현율(Recall)**, **F1 점수**가 보고되어 있으며, 이 테스트 세트에는 총 21건의 충돌 사례가 포함되어 있다.

자세 예측 성능에서 최고를 기록한 SeS-GCN은 충돌 감지에서도 **F1 점수 0.64**로 가장 우수한 성능을 보였다.
반면, 성능이 낮았던 **MSR-GCN [17]**은 충돌 감지에서도 부진한 결과를 보였으며, F1 점수는 0.31에 불과했다.

---

📊 **그림 2 설명**

그림 2는 CHICO 데이터셋 내 모든 작업에 대해, 각 관절(joint)별 MPJPE 오차의 평균 분포를 시각화한 것이다.

* **(a)**는 **단기 예측(0.40초)**에 대한 분포
* **(b)**는 **장기 예측(1.00초)**에 대한 분포

이 시각화에서:

* 각 **구(점의 반지름(radius)**은 해당 관절의 예측 오차의 크기를 나타낸다.
* 스켈레톤의 공간적 스케일과 동일한 비율로 표시되어 있어, 오차가 큰 관절이 직관적으로 드러난다.

---

📊 **표 5. 충돌 감지 성능 비교 및 추론 시간 (1000ms 기준)**

각 자세 예측 기법에 대해 Precision(정밀도), Recall(재현율), F1 점수, 추론 시간(초)을 비교함.
자세한 분석은 본문 6.2절 참조.

| 모델                | 정밀도 (Precision) | 재현율 (Recall) | F1 점수 | 추론 시간 (초)   |
| :------------------ | :----------------- | :-------------- | :------ | :--------------- |
| DCT-RNN-GCN [52]    | 0.63               | 0.58            | 0.56    | 9.1 × 10⁻³       |
| MSR-GCN [17]        | 0.63               | 0.30            | 0.31    | 25.2 × 10⁻³      |
| STS-GCN [68]        | 0.68               | 0.61            | 0.63    | 2.3 × 10⁻³       |
| SeS-GCN (제안 모델) | 0.84               | 0.54            | 0.64    | 2.3 × 10⁻³       |

---

📌 **주요 해설 요약**:

* **SeS-GCN**은 **가장 높은 F1 점수(0.64)**를 기록하며 충돌 감지 성능에서도 선두를 차지.
* **STS-GCN**은 F1 점수 0.63으로 근소한 차이로 뒤를 이음.
* **DCT-RNN-GCN**은 정밀도와 재현율은 중간 수준이나, 추론 시간이 4배 이상 느림.
* **MSR-GCN**은 재현율과 F1 점수 모두 낮아, 충돌 감지 성능이 미흡함.

7. 결론
===

산업 환경에서의 **인간-로봇 협업(HRC)** 중 인간의 움직임을 예측하는 것을 목표로, 우리는 정확성과 효율성을 모두 고려한 새로운 모델인 **SeS-GCN**을 제안하였다. 이 모델은 다음 세 가지 최신 모델링 기법을 통합하고 있다:

1.  **시공간 분리형 GCN (Space-time separable GCNs)**
2.  **깊이 방향 분리 그래프 합성곱 (Depth-wise separable graph convolutions)**
3.  **희소 GCN (Sparse GCNs)**

또한 우리는 실제 조립 라인에서 수집된 **CHICO**라는 새로운 데이터셋을 공개하였으며, 이는 **인간 자세 예측**과 **충돌 감지**라는 두 가지 핵심 HRC 과제에 대해 벤치마크를 제공하는 최초의 데이터셋이다.

제안된 SeS-GCN 모델은 1초 후 미래 예측에서 MPJPE 오차 85.3mm, 추론 시간 2.3밀리초라는 탁월한 성능을 보이며, 로보틱스 분야에서의 **지각(perception)** 알고리즘 및 그 응용 가능성에 큰 잠재력을 제공한다.

**감사의 말**:
본 연구는 이탈리아 교육연구부(MIUR)의 “2018-2022 우수학과 지원사업(Dipartimenti di Eccellenza)“을 통해 지원을 받았으며, 일부는 **DsTech S.r.l.**의 자금 지원을 받았다.

---

8. 부록
===

본 섹션은 메인 논문 제출물에 다음과 같은 추가 정보를 보완하여 제공한다:

* 8.1절에서는 CHICO 데이터셋에 포함된 각 작업(action)에 대해 보다 자세한 설명과 시각 자료를 제공한다. 이는 메인 논문에 포함된 내용을 확장한 것이다.
* 8.2절에서는 SeS-GCN의 구현 및 학습 관련 추가 세부사항에 대해 논의한다.

8.1. CHICO 데이터셋
---

---

개요 및 추가 설명
---

이 섹션에서는 CHICO 데이터셋의 수집 환경과 과정에 대한 추가 정보를 설명하며, 8가지 작업(action)에 대한 시각적 설명도 함께 제공한다.
자세한 내용과 영상은 다음 링크에서 확인할 수 있다:
🔗 [<https://github.com/AlessioSam/CHICO-PoseForecasting>](https://github.com/AlessioSam/CHICO-PoseForecasting)

데이터셋 및 수집 과정에 대한 세부 사항
---

* **수집 기간**: 2021년 10월 ~ 2022년 3월
* **수집 장소**: 총 면적 500m²의 Industry 4.0 실험실에서 수행되었으며, 이 공간에는
  * 구성 가능한 11m 생산 라인,
  * **협업 로봇(cobot)** 4대,
  * 품질 검사 셀,
  * 조립/해체 스테이션,
  * 기타 장비들이 포함되어 있다.

CHICO 데이터셋은 이 실험실 내 조립 스테이션의 0.9m × 0.6m 크기의 작업대에서, Kuka LBR iiwa 14 R820 cobot 앞에서 수집되었다.

* **해당 로봇의 정확도**: 위치 정확도는 ±0.1mm, 축별 토크 정확도는 ±2% [39]
* **안전 기능**: 로봇 관절에는 토크 센서가 내장되어 있어 접촉을 감지하면 힘과 속도를 줄이며,
  이는 ISO/TS 15066:2016 표준 [33]을 준수한다.

CHICO 프로젝트는 로봇과 작업자 간의 충돌이 예상된 실험이기 때문에,
로봇 링크의 최대 직선 속도는 200mm/s로 제한되었으며, 이는 ISO 표준보다 다소 낮은 수치다.
기계적 제동이 작동되기 전 허용되는 안전 토크 한계는

* 각 관절: 30 N·m
* **말단 장치(end-effector)**: 50 N·m

또한, 직교력 기준으로 10N의 프로그래밍 가능한 안전 확인 값도 설정되었다.

---

참가자 정보 및 충돌 수집 현황
---

* 총 20명의 피험자가 참여하였으며,
* 남성 14명, 여성 6명
* 평균 연령은 23세

이들은 모두 사전 동의서를 작성하고, Kuka 협업 로봇과 협력하는 방법에 대한 단기 집중 교육을 받은 뒤 실험에 참여하였다.

데이터 수집 기간 동안, 작업 중에 발생한 작업자와 로봇 간의 충돌 장면 일부를 선별하였으며,
최종적으로 총 226건의 충돌 사례가 기록되었다.

* 작업당 평균 약 45건의 충돌이 포함되었으며,
* 유일한 예외는 Hammering 작업이다.

해당 작업에서는 로봇이 물체를 고정한 채로 정지 상태를 유지하고,
인간 작업자가 로봇 팔 근처를 반복적으로 움직이는 구조이므로
실제 충돌 발생 빈도가 상대적으로 낮다.

그러나 이 동작 역시 충돌 감지 데이터셋에 포함되어 있으며,
**오탐(false positive)** 여부를 검증하는 데 유용하다.

---

🔹 **CHICO 데이터셋 작업 세부 설명 (추가 설명 및 그림 포함)**

---

✅ **경량 물체 운반 (Lightweight Pick and Place, Light P&P)**

작업자는 약 50그램의 소형 물체를 **적재 구역(B)**에서 **전달 위치(A)**로 일정 시간 안에 옮겨야 한다.
작업대의 양쪽 끝에 **B(적재 구역)**와 **A(전달 위치)**가 각각 위치하며,
이 작업이 수행되는 동안 로봇은 적재 구역에서 작업 중이기 때문에,
작업자는 로봇 팔 근처를 **매우 가까운 거리(수 센티미터)**로 지나가야 한다.

---

📘 **그림 3. 경량 물체 운반 작업의 예시**

1   2                3
Home → H가 B에서 픽업 → H가 A에 배치
 B   B   B
     ↓   ↓
 A   A   A

4   5                6
Home → R이 A에서 픽업 → R이 B에 배치
 B   B   B
     ↑   ↑
 A   A   A

* H: Human (작업자)
* R: Robot (로봇)
* 붉은 블록 하나만 예시로 표시되었지만, 실제로는 수십 개의 아이템이 작업 중 배치되었다.

---

✅ **중량 물체 운반 (Heavyweight Pick and Place, Heavy P&P)**

이 작업의 기본 구조는 Light P&P와 동일하나, 옮겨야 하는 물체는 무게 0.75kg의 바닥 타일이다.
이로 인해 해당 작업은 반드시 양손으로 수행되어야 한다.

---

📘 **그림 4. 중량 물체 운반 작업의 예시**

1   2                 3
Home → H가 B에서 픽업 → H가 A에 배치
 B2  B   B
      ↓   ↓
 A   A   10

4   5                 6
Home → R이 A에서 픽업 → R이 B에 배치
 B   B   B2
     ↑   ↑
10  10   A

* 두 손으로 물체를 옮기는 과정에서 상체를 크게 회전해야 하며, 이로 인해 로봇이 작업자의 시야에서 가려지게 된다.
* 이러한 시야 제한이 충돌 발생의 주요 원인이었다.

---

✅ **표면 연마 (Surface Polishing)**

이 작업은 [50]의 연구에서 영감을 받은 것으로, 작업자가 40×60cm 크기의 타일 가장자리를 연마 스펀지로 문지르는 동안,
로봇은 시각적 품질 검사를 수행하는 것처럼 동작한다.

---

📘 **그림 5. 표면 연마 작업의 예시**

1              2                3
H가 연마       → R이 품질 검사  → H가 다시 연마

* 작업자는 금속 타일에서 재료를 제거하기 위해 연마 스펀지를 사용한다.
* 이 작업은 사용자가 상체를 기울여 표면 가까이에서 작업하기 때문에, 로봇 시야를 차단하게 되어 충돌이 가장 많이 발생한 작업 중 하나였다.

---

🔹 **CHICO 데이터셋 작업 설명 (계속)**

---

✅ **정밀 물체 이동 (Precision Pick and Place, Prec. P&P)**

로봇이 작업대 중앙의 30×30cm 테이블 네 모서리에 플라스틱 부품 4개를 배치하면,
인간 작업자는 이 부품들을 하나씩 집어 ‘U’ 구역에 내려놓는다.
작업이 끝나면 로봇이 같은 과정을 반복하며 부품을 다시 배치한다.

---

📘 **그림 6. 정밀 물체 이동 작업 예시**

1   2                        3
Home → R이 B에서 픽업      → R이 정확한 지점에 배치
  B     B   B
             ↓ ↓
  U     U   U

4          5             6
H가 정확 지점에서 픽업 → H가 U에 하역
  B     B
            ↓
  U     U

* 이 작업은 예측 모델이 정확한 목표 지점을 얼마나 잘 식별할 수 있는지를 평가하기에 중요한 과제이다.

---

✅ **무작위 물체 이동 (Random Pick and Place, Rnd. P&P)**

구조는 Prec. P&P와 유사하지만, 로봇이 플라스틱 부품들을 테이블 위의 무작위 위치에 배치한다.
작업자는 해당 물체들을 집어 ‘U’ 구역에 내려놓는다.
이 과정에서 로봇과 인간 간의 예상치 못한 경로 중첩이 발생하여 충돌이 종종 일어났다.

---

📘 **그림 7. 무작위 물체 이동 작업 예시**

1   2                        3

Home → R이 B에서 픽업      → R이 무작위 위치에 배치
  B     B   B
             ↓ ↓
  ?     ?   ?
  U     U   U

4             5             6
H가 무작위 위치에서 픽업 → H가 U에 하역
  B     B   B
               ↓
  U     U   U

* 예시에서는 붉은 블록 하나만 표현되었으나, 실제 작업에서는 수십 개의 아이템이 사용되었다.

---

✅ **높은 선반 적재 (High Shelf Lifting, High Lift)**

이 작업의 목표는 **로봇이 측면에 위치한 적재 구역(B)에 배치한 가벼운 플라스틱 부품(50g)**을
1.70m 높이에 위치한 선반(A)에 올려놓는 것이다.

작업대의 공간적 구조로 인해, 작업자의 팔은 로봇 팔의 위나 아래를 통과해야 하며,
이로 인해 작업자 팔과 로봇 링크 간 근접 상황이 자주 발생한다.

---

📘 **그림 8. 높은 선반 적재 작업 예시**

1   2                     3
Home → R이 B에서 픽업  → R이 A에 배치
  B     B   B
            ↓ ↓
  A     A   A   (모두 1.70m 높이)

4           5
H가 A에서 픽업 → H가 높이 선반에 적재
  B     B
             ↓
  A     A   (1.70m 높이)

* 이 작업은 로봇과 신체 간 거리가 매우 가까운 고위험 작업에 해당된다.

---

✅ **망치질 (Hammering)**

작업자는 로봇이 고정하고 있는 **금속 부품(타일)**을 망치로 두드린다.
이 실험의 목적은 충돌 감지 시스템이 로봇에 실제로 충돌하지 않았더라도,
매우 가까운 거리에서의 작업을 오탐 없이 처리할 수 있는지를 확인하는 것이다.

---

📘 **그림 9. 망치질 작업 예시**

1
R이 금속 바를 고정,
H가 망치질 수행

* 이 작업은 충돌 감지 모델의 **‘오탐 방지 능력’**을 평가하기에 가장 중요한 작업 중 하나이다.
* 사용자는 한 손으로 물체를 잡고, 다른 손으로 망치를 사용하는 구조이기 때문에,
  로봇과 작업자 간의 거리는 지속적으로 매우 가까운 상태가 된다.
* 실제로 이 작업은 전체 충돌 감지 실험 중 **가장 많은 오탐(false positives)**을 유발했으며,
  그 비율은 약 80%에 달했다.

---

8.2. 구현 및 학습
---

---

구현 세부사항 (Implementation details)
---

3장과 5장에서 설명한 내용에 더해, 이 섹션에서는 SeS-GCN의 구현 및 학습 과정에 관한 추가 정보를 제공한다.

* 제안된 SeS-GCN은 PyTorch 프레임워크로 구현되었으며,
  소스코드는 추후 공개될 예정이다.
* 모델은 **각 GCN 레이어에 잔차 연결(residual connection)**을 적용하며,
  각 GCN 레이어 말미에 **배치 정규화(Batch Normalization) [31]**를 사용해 정규화를 수행한다.
* 최적화는 **ADAM 옵티마이저 [37]**를 사용하여 수행된다.

---

학습 시간 (Learning time)
---

* Human3.6M [32] 데이터셋에서, SeS-GCN의 학습은 teacher 모델과 student 모델 각각에 대해 60 에폭(epoch) 동안 진행된다.
* 배치 크기는 256,
  **초기 학습률(learning rate)**은 0.1로 설정되며,
  에폭 5, 20, 30, 37에서 **학습률 감소(learning rate decay)**가 0.1 배율로 적용된다.
* NVIDIA RTX 2060 GPU 기준으로 전체 학습 소요 시간은 약 30분이다.

---

손실 함수 (Loss function)
---

문헌 [53, 52, 17]에 따르면, 손실 함수는 테스트 시 사용하는 평가 지표(Eq. 5)와는 다르다.
손실 함수는 전체 예측 시퀀스에 대한 MPJPE의 평균값을 기반으로 한다:

\\mathcal{L}_{\text{MPJPE}} = \\frac{1}{VT} \\sum_{t=0}^{T} \\sum_{v=1}^{V} \\left\\| \\hat{x}_{vt} - x_{vt} \\right\\|_2

여기서:

* \\hat{x}_{vt}, x_{vt}:\\n  각각 예측값과 실제값에 해당하는 관절 v의 3차원 좌표 벡터 (프레임 t에서)
* V: 관절의 수
* T: 예측 프레임 수

이 수식은 메인 논문의 식 (5)와 일치하며, 예측 시퀀스 전체의 오차 평균을 기반으로 모델을 학습시킨다.
