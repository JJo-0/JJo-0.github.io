---
layout: single
title: "이상 탐지 평가 지표: 현황, 과제 및 데이터 유형별 분석"
categories: [AI, Project]
tag: [AI, Anomaly Detection, Evaluation Metrics]
toc: true
author_profile: false
sidebar:
    nav: "docs"
---

# 이상 탐지 평가 지표: 현황, 과제 및 데이터 유형별 분석

## 1. 서론: 이상 탐지에서 평가의 중요성

이상 탐지(Anomaly Detection, AD)는 정상적인 데이터 분포에서 현저하게 벗어나는 희귀한 관측치나 패턴을 식별하는 과정입니다. 이는 **사이버 보안**(네트워크 침입, 악성 행위 탐지)[^1][^9], **금융**(사기 거래 탐지)[^2], **산업 제조 및 검사**(제품 결함, 시스템 오류 감지)[^4][^6], **의료 및 의료 영상 분석**(질병 징후, 비정상 조직 식별)[^3][^13], **시스템 모니터링**(성능 저하, 장애 예측)[^5] 등 다양한 분야에서 핵심적인 역할을 수행합니다.

이상 탐지 알고리즘의 성능을 비교하고, 하이퍼파라미터를 최적화하며, 모델의 신뢰성을 보장하기 위해서는 견고한 평가 지표가 필수적입니다. 특히 **안전이 중요한 응용 분야**(safety-critical applications)에서는 모델 결정에 대한 설명 가능성 요구[^15][^26]와 더불어 평가의 중요성이 더욱 부각됩니다.

그러나 이상 탐지 평가에는 다음과 같은 핵심적인 어려움이 존재합니다:

*   **클래스 불균형(Class Imbalance)**: 정의상 이상 데이터는 정상 데이터에 비해 극히 드물게 발생합니다[^7][^8]. 이로 인해 정확도(Accuracy)와 같은 표준적인 분류 지표는 모델 성능을 잘못 나타낼 수 있습니다. 예를 들어, 모든 데이터를 정상으로 예측하는 모델도 매우 높은 정확도를 보일 수 있습니다[^14].
*   **다양한 이상 유형(Diverse Anomaly Types)**: 이상은 단일 데이터 포인트에서 나타나는 **점 이상**(point anomaly), 특정 문맥에서만 이상으로 간주되는 **문맥적 이상**(contextual anomaly), 데이터 포인트 집합이 함께 이상 패턴을 보이는 **집합적 이상**(collective anomaly), 또는 특정 패턴 형태를 띠는 **패턴 이상**(pattern-wise anomaly) 등 다양한 형태로 나타날 수 있습니다[^2][^18][^20]. 각 유형은 서로 다른 평가 관점을 요구할 수 있습니다.
*   **도메인 특수성(Domain Specificity)**: 이상의 정의와 중요성은 분야마다 크게 다릅니다. 예를 들어, 의료 분야에서의 작은 체온 변화는 이상일 수 있지만, 주식 시장에서의 유사한 변동은 정상으로 간주될 수 있습니다. 이는 평가 지표의 선택과 해석에 영향을 미칩니다.
*   **데이터 양식(Data Modality)**: 이미지[^15], 시계열[^18], 그래프[^20] 등 데이터 유형에 따라 특화된 평가 지표가 필요합니다.
*   **합의 부족 및 결함 있는 벤치마크(Lack of Consensus & Flawed Benchmarks)**: 어떤 지표가 최선인지에 대한 보편적인 합의가 부족하며[^68][^101], 기존 벤치마크와 평가 방식에 대한 비판도 제기되고 있습니다[^28][^70][^81].

이러한 배경은 이상 탐지가 다양한 중요 도메인에 걸쳐 사용됨에 따라 평가의 중요성이 매우 크다는 점을 시사합니다. 부적절한 평가는 비효율적이거나 심지어 해로운 시스템의 배포로 이어질 수 있습니다. 의료 진단[^3]이나 산업 안전[^4]과 같은 안전 필수 영역에서 평가 지표가 실제 성능을 정확히 반영하지 못하면(예: 불균형 문제나 특정 이상 유형 무시), 모델이 치명적인 이벤트를 탐지하지 못해 심각한 결과를 초래할 수 있습니다. 모델의 설명 가능성 요구[^15][^26]는 모델이 왜 특정 대상을 이상으로 판단하는지 이해하는 것의 중요성을 강조하며, 이는 모델 성능 측정 방식과 밀접하게 연관됩니다.

또한, 이상 탐지 평가의 본질적인 어려움(불균형, 다양성, 도메인 특수성)은 단일 '최고' 지표가 존재하기 어렵다는 것을 의미합니다. 다양한 상황에서 서로 다른 지표가 우수성을 보인다(예: 희귀 긍정 클래스에 대한 AUPRC[^84][^96], 전반적인 균형을 위한 MCC[^10][^19], 시계열 데이터에 대한 범위 기반 지표[^68][^70]). 어떤 단일 지표도 모든 바람직한 속성(예: 불균형에 대한 강건성, 특정 오류 유형에 대한 민감도, 시간적 인식)을 포착하지 못합니다. 따라서 여러 지표를 함께 사용하는 **다중 지표 접근 방식**(multi-metric approach)이 종종 필요하며, 이는 성능에 대한 보다 전체적인 그림을 제공합니다[^7][^27].

마지막으로, 기존 벤치마크와 지표에 대한 비판[^28][^70][^81]은 이 분야가 평가 관행에 대해 비판적인 자기 성찰의 시기를 겪고 있음을 시사합니다. 여러 최근 연구[^28][^70]는 시계열 데이터에 대한 점별 F1 점수와 같은 일반적인 지표의 결함을 명시적으로 지적하고, 이러한 지표들이 "**진보의 환상(illusion of progress)**"을 만들 수 있다고 주장합니다. 이는 기존의 확립된 관행이 불충분하거나 오해의 소지가 있을 수 있다는 인식이 커지고 있음을 나타내며, 보다 견고한 평가 방법론(예: 범위 기반 지표[^68][^70], 소속 지표[^74], PRO-AUC[^57])에 대한 연구를 촉진하고 있습니다.

## 2. 기본적인 평가 지표

많은 분류 기반 평가 지표는 **혼동 행렬(Confusion Matrix)**을 기반으로 계산됩니다. 혼동 행렬은 실제 클래스와 모델 예측 클래스를 비교하여 네 가지 결과를 요약합니다[^12]:

*   **True Positive (TP)**: 실제 이상(Positive)을 이상으로 올바르게 예측한 경우.
*   **False Positive (FP)**: 실제 정상(Negative)을 이상으로 잘못 예측한 경우 (Type I Error).
*   **False Negative (FN)**: 실제 이상을 정상으로 잘못 예측한 경우 (Type II Error).
*   **True Negative (TN)**: 실제 정상을 정상으로 올바르게 예측한 경우.

이 네 가지 값은 다음과 같은 기본적인 평가 지표를 계산하는 데 사용됩니다.

### 2.1. 정밀도(Precision), 재현율(Recall), F1-점수(F1-Score)

*   **정밀도 (Precision)**: 모델이 이상으로 예측한 것 중에서 실제 이상인 비율을 측정합니다. 즉, "탐지된 이상 중 얼마나 많은 것이 진짜 이상인가?"에 답합니다. 수학적으로는 `Precision = TP / (TP + FP)` 로 정의됩니다. FP 비용이 높을 때(예: 잘못된 경보로 인한 피로도 증가[^16]) 중요합니다. 그러나 FN을 무시한다는 한계가 있습니다.

*   **재현율 (Recall, Sensitivity, True Positive Rate)**: 실제 이상 중에서 모델이 이상으로 올바르게 예측한 비율을 측정합니다. 즉, "실제 이상 중 얼마나 많이 탐지되었는가?"에 답합니다. 수학적으로는 `Recall = TP / (TP + FN)` 로 정의됩니다. FN 비용이 높을 때(예: 중요한 사기나 질병 놓침[^3]) 중요합니다. 그러나 FP를 무시한다는 한계가 있습니다.

*   **F1-점수 (F1-Score)**: 정밀도와 재현율의 조화 평균(harmonic mean)으로, 두 지표 간의 균형을 제공합니다. 수학적으로는 `F1 = 2 * (Precision * Recall) / (Precision + Recall)` 로 정의됩니다. F1 점수는 널리 사용되지만, 분류 임계값(threshold) 선택에 민감할 수 있으며[^17], 심각한 불균형 상황에서는 성능을 제대로 반영하지 못할 수 있다는 비판도 있습니다[^19][^21]. 정밀도와 재현율에 다른 가중치를 부여하는 일반화된 **F-beta 점수**(`F_beta = (1 + beta^2) * (Precision * Recall) / ((beta^2 * Precision) + Recall)`)도 사용될 수 있습니다.

정밀도와 재현율(또는 F1 점수로 균형) 중 어느 것을 우선시할지는 근본적으로 특정 응용 분야가 FP와 FN 중 어떤 오류를 더 심각하게 여기는지에 따라 결정됩니다. 정의 자체에서 정밀도는 FP 오류(분모 TP+FP)에, 재현율은 FN 오류(분모 TP+FN)에 초점을 맞춘다는 것을 알 수 있습니다. 실제 사용 사례는 이를 뒷받침합니다: 잘못된 경보 최소화(높은 정밀도 필요)[^16] 대 모든 중요한 이벤트 포착(높은 재현율 필요)[^3]. 이는 지표 선택이 일반적일 수 없으며, 다양한 오류 유형의 후속 영향을 이해해야 함을 시사합니다.

### 2.2. ROC 곡선 하 면적 (Area Under the ROC Curve, AUROC 또는 AUC)

*   **ROC 곡선 (Receiver Operating Characteristic Curve)**: 분류 임계값을 변경함에 따라 TPR(재현율) 대 FPR(False Positive Rate, `FPR = FP / (FP + TN)`)의 변화를 그래프로 나타낸 것입니다.
*   **AUROC (Area Under the ROC Curve)**: ROC 곡선 아래의 면적을 의미하며, 0과 1 사이의 값을 가집니다. 1에 가까울수록 모델 성능이 우수함을 나타냅니다.
*   **해석**: 무작위로 선택된 이상 샘플이 무작위로 선택된 정상 샘플보다 모델에 의해 더 높은 점수(순위)를 받을 확률로 해석될 수 있습니다[^84].
*   **장점**: 임계값에 의존하지 않고 모델의 전반적인 판별 능력을 평가할 수 있습니다.
*   **단점**: 클래스 불균형이 심한 경우, 다수 클래스인 정상 샘플(TN)의 영향을 크게 받아 이상 샘플(소수 클래스)에 대한 성능이 낮음에도 불구하고 높은 점수를 나타낼 수 있습니다[^84][^96].

AUROC의 임계값 독립성은 모델의 내재적인 순위 결정 능력을 비교하는 데 유용하지만, FPR에 의존하기 때문에 희귀 이상에 대한 낮은 성능을 가릴 수 있습니다[^84]. AUROC는 모든 임계값에서의 성능을 통합합니다. 그러나 FPR=FP/(FP+TN)입니다. 매우 불균형한 데이터셋에서는 TN이 매우 크다. 모델은 FPR을 크게 증가시키지 않으면서 많은 FP를 허용할 수 있으며, 이로 인해 이상에 대한 정밀도(Precision=TP/(TP+FP))가 낮더라도 높은 AUROC 점수를 얻을 수 있습니다. 이는 높은 AUROC 점수와 희귀 이벤트를 찾는 실제 유용성 사이에 잠재적인 불일치를 야기합니다[^84][^96].

## 3. 클래스 불균형 해결: 특화된 지표

이상 탐지 데이터는 본질적으로 불균형하므로, 정확도와 같은 표준 지표는 신뢰하기 어렵습니다[^7][^14]. 이러한 문제를 해결하기 위해 다음과 같은 특화된 지표들이 제안되었습니다.

### 3.1. 정밀도-재현율 곡선 하 면적 (Area Under the Precision-Recall Curve, AUPRC 또는 PR-AUC)

*   **정밀도-재현율 곡선 (Precision-Recall Curve)**: 분류 임계값을 변경함에 따라 정밀도(Y축) 대 재현율(X축)의 변화를 그래프로 나타낸 것입니다.
*   **AUPRC (Area Under the PR Curve)**: PR 곡선 아래의 면적을 의미하며, 0과 1 사이의 값을 가집니다. 1에 가까울수록 성능이 우수함을 나타냅니다. **평균 정밀도(Average Precision, AP)**는 특히 객체 탐지[^29][^30]나 정보 검색 분야에서 AUPRC의 근사치 또는 동의어로 자주 사용되는 관련 개념입니다.
*   **이론적 근거**: AUPRC는 양성 클래스(소수 클래스/이상)의 성능에 초점을 맞추므로, TN이 데이터셋을 지배하는 경우 AUROC보다 더 유익한 정보를 제공합니다[^84][^96]. AUPRC의 기준선(baseline)은 양성 클래스의 비율(prevalence)과 같습니다.

*   **AUPRC 대 AUROC 논쟁**:
    *   **일반적인 주장**: 불균형 데이터셋에서는 AUPRC가 AUROC보다 우수하거나 선호된다는 주장이 널리 퍼져 있습니다[^96][^100].
    *   **반론 및 미묘한 차이 (최근 연구 기반)**[^84][^87]:
        *   AUPRC는 이상치 비율(outlier fraction)에 민감하지만, AUROC는 더 안정적입니다[^8].
        *   AUROC는 모든 FP를 동일하게 가중치를 부여하지만, AUPRC는 특정 임계값(τ)에서의 FP에 대해 모델이 τ보다 높은 점수를 출력할 가능성의 역수로 가중치를 부여합니다 (즉, 높은 점수에서 발생한 오류를 더 중요하게 여긴다).
        *   AUROC는 모든 양성 샘플에 대해 균일하게 모델 개선을 선호하지만, AUPRC는 낮은 점수를 받은 샘플보다 높은 점수를 받은 샘플의 개선을 더 선호한다.
        *   "불균형 시 AUPRC가 더 좋다"는 주장은 종종 근거가 부족하거나 잘못된 인용에 기반한다.
        *   AUPRC는 낮은 점수를 받는 하위 집단에 대한 알고리즘 편향을 증폭시킬 수 있다.

*   **논쟁의 결론**: 어떤 지표를 선택할지는 단순히 불균형 비율이 아니라, 사용 사례(예: 높은 신뢰도의 오류 우선 처리 vs. 균일한 개선)와 잠재적인 공정성 영향에 따라 달라집니다. AUPRC는 양성 클래스 성능이 가장 중요할 때(예: 정보 검색, 희귀 이벤트 탐지) 유용하며[^84], AUROC는 오류 비용이 균형을 이루거나 균일한 개선을 보장해야 할 때 더 적합할 수 있습니다.

### 3.2. 매튜 상관 계수 (Matthews Correlation Coefficient, MCC)

*   **정의**: 관찰된 실제 클래스와 예측된 이진 클래스 간의 상관 관계를 측정하는 계수입니다. TP, TN, FP, FN을 모두 사용하여 계산되며, 공식은 다음과 같습니다: `MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))`[^10][^19].
*   **범위 및 해석**: 값의 범위는 -1에서 +1 사이입니다. +1은 완벽한 예측, 0은 무작위 예측과 동일한 수준, -1은 예측과 실제가 완전히 반대임을 의미합니다.
*   **주요 장점**: MCC는 분류기가 혼동 행렬의 네 가지 범주(TP, TN, FP, FN) 모두에서 좋은 성능을 보여야만 높은 점수를 얻습니다. 이는 불균형 데이터셋에서 F1 점수나 정확도보다 더 신뢰할 수 있고 유익한 지표가 되게 합니다[^10][^19][^22]. 양성 클래스와 음성 클래스 모두의 크기에 비례하여 균형을 고려합니다.
*   **이상 탐지 적합성**: 이상 탐지의 본질적인 불균형 특성 때문에 MCC가 적합하며[^10][^22], 문헌에서 점점 더 많이 추천되고 있습니다[^19][^25].
*   **잠재적 한계**: 혼동 행렬의 행이나 열 중 하나가 모두 0이면 정의되지 않을 수 있으며[^19], TN 수를 알아야 하므로 객체 탐지와 같은 개방형 환경(open-world setting)에서는 사용하기 어려울 수 있습니다.

### 3.3. 기타 지표

*   **균형 정확도 (Balanced Accuracy)**: TPR과 TNR(True Negative Rate, `TNR = TN / (TN + FP)`)의 평균으로, 불균형을 다루는 또 다른 방법입니다[^14].
*   **평균 정밀도 (Average Precision, AP)**: PR 곡선과의 연관성을 다시 강조하며, 특히 탐지 작업에서 널리 사용됩니다[^29][^30].

AUPRC와 AUROC를 둘러싼 논쟁은 평가 지표가 실제로 측정하는 성능의 어떤 측면인지, 그리고 그것이 특정 AD 작업의 목표(예: 어떤 이상이든 찾는 것 vs. 높은 신뢰도의 이상을 정확하게 찾는 것)와 일치하는지에 대한 명확성이 더 깊이 필요함을 드러냅니다. 핵심 차이는 오류 가중 방식에 있습니다. AUPRC가 높은 점수의 인스턴스에 집중하는 것은 상위 순위가 가장 중요한 정보 검색과 같은 작업에 적합하게 만듭니다[^84]. AUROC의 균일한 가중치는 양성 및 음성 예제 간의 모든 오분류가 동일하게 중요할 때 더 좋다[^84]. "불균형"이 종종 "희귀 양성 클래스가 중요하다"는 의미로 사용되기 때문에 혼란이 발생하지만, 이것이 자동으로 어떤 오류 가중 방식(AUPRC 또는 AUROC)이 가장 적절한지를 결정하지는 않습니다. AUPRC가 낮은 점수의 양성 인스턴스에 대한 낮은 성능을 가리거나 편향을 증폭시킬 가능성[^84]은 불균형만으로 단순한 선호도를 결정하는 것을 더욱 복잡하게 만듭니다.

반면, MCC는 혼동 행렬을 전체적으로 고려하여 불균형 AD에서 단일 값 요약 지표로서 강력한 후보로 부상하며, F1이나 정확도보다 더 균형 잡힌 시각을 제공합니다[^10][^19]. 여러 연구에서 불균형 환경에서 F1/정확도보다 MCC를 명시적으로 지지합니다[^19][^22]. MCC 공식은 네 가지 TP, TN, FP, FN 값을 대칭적으로 통합합니다. 이는 일부 범주에서만 뛰어난 성능(예: 높은 재현율이지만 낮은 정밀도, 또는 다수 클래스 예측으로 인한 높은 정확도)으로 높은 점수를 얻는 시나리오를 방지합니다. 상관 관계 기반 특성은 정확도나 F1 점수와 같은 지표보다 클래스의 기본 비율에 덜 민감한 예측과 실제 간의 일치 정도를 측정합니다[^19].

여러 불균형 인식 지표(AUPRC, MCC, 균형 정확도)의 존재와 홍보는 표준 지표가 불충분하며 신뢰할 수 있는 AD 평가를 위해서는 지표 속성에 대한 신중한 고려가 필수적이라는 생각을 강화한다. 이러한 특화된 지표들의 개발과 논의[^7][^10][^14][^19][^84][^96] 자체가 불균형 상황에서 정확도와 같은 전통적인 지표의 인식된 실패[^14]에서 비롯되었다. 각 특화된 지표는 특정 단점(예: AUPRC가 TN 무시, MCC가 네 셀 모두 균형)을 해결하려고 시도하며, 이는 평가 문제 자체가 다면적임을 나타낸다.

## 4. 이상 탐지 평가의 동향 및 비판

### 4.1. 일반적으로 사용되는 지표 및 동향

조사된 논문과 벤치마크를 바탕으로 볼 때, 알려진 한계에도 불구하고 **정밀도, 재현율, F1 점수, AUROC**는 여전히 널리 사용되고 있습니다[^1][^2][^18]. **AUPRC** 또한 특히 최근 논문과 불균형 환경에서 자주 보고됩니다[^3][^84][^96]. **MCC**는 덜 자주 나타나지만 점차 주목받거나 추천되고 있습니다[^10][^19][^22]. 이미지 AD 벤치마크에서는 **Pixel-AUROC, Image-AUROC, PRO-AUC**와 같은 특정 지표가 표준으로 사용되며[^38][^40][^57], 시계열 AD에서는 다양한 **범위 기반 지표**[^68][^70][^74]와 **NAB 점수**[^76][^81]가 사용됩니다.

AUPRC 대 AUROC 사용 경향과 관련하여, 섹션 3에서 언급했듯이 최근의 비판적 분석[^84][^87]에도 불구하고 불균형 상황에서 AUPRC를 선호하는 주장은 의료와 같은 고위험 도메인을 포함한 다양한 분야의 문헌에서 여전히 널리 퍼져 있습니다[^98][^100].

### 4.2. 평가 방식에 대한 비판과 과제

*   **결함 있는 지표 (Flawed Metrics)**: 점별 지표(P/R/F1)는 시계열의 범위 기반 이상(range-based anomalies)을 평가하기에 부적절합니다[^28][^70]. 일부 범위 기반 지표조차도 조작(gaming)이 가능하거나 성능을 과대평가할 수 있습니다 (예: PA-F1의 단일 탐지점 인정 방식[^70], 부정확하거나 불충분한 탐지에 대한 문제[^74]). 이미지의 픽셀 수준 지표는 큰 영역에 의해 지배될 수 있습니다[^57].
*   **일관성 없는 벤치마킹 (Inconsistent Benchmarking)**: 통일된 벤치마크와 평가 프로토콜의 부재는 연구 간의 공정한 비교를 방해합니다[^92][^101]. ADGym[^93], IM-IAD[^64], TimeSeriesBench[^71], ADer[^53]와 같은 노력은 이 문제를 해결하고자 합니다.
*   **벤치마크 데이터셋 문제 (Benchmark Dataset Issues)**: 일부 벤치마크 데이터셋 자체에 결함(사소한 이상, 잘못된 레이블링, 비현실적인 밀도 등)이 있습니다[^40][^81]. 실제 데이터의 부족 및 개인 정보 보호 문제도 어려움입니다[^81][^104].
*   **복잡성에 대한 과도한 강조 (Overemphasis on Complexity)**: 새로운 딥러닝 모델이 엄격하게 평가될 때 항상 더 간단한 기준 모델(baseline)보다 성능이 우수하지는 않다[^70][^81]. 이는 인지된 발전이 오해의 소지가 있을 수 있음을 시사합니다.
*   **임계값 의존성 (Threshold Dependency)**: F1 점수와 같은 지표는 임계값 선택을 필요로 하며, 이는 모델의 순위 결정 능력과는 별개로 점수에 영향을 미칠 수 있습니다[^17].

연구 커뮤니티에서 지표 결함이 확인된 후 실제 관행이 변경되기까지 상당한 지연이 있다는 점은 주목할 만하다. 표준적이지만 잠재적으로 결함이 있는 지표들이 여전히 널리 사용되고 있다. 시계열에 대한 점별 지표[^28][^70]나 불균형에 대한 AUROC[^84][^96]의 문제점을 지적하는 수많은 논문에도 불구하고, 이러한 지표들은 계속해서 자주 보고되고 있다. 이는 연구 관행의 관성, 새로운 지표 채택의 어려움, 또는 대안에 대한 인식 부족이나 합의 부재를 시사한다. 반박에도 불구하고 "불균형에는 AUPRC"라는 주장이 지속되는 것[^96][^100]도 또 다른 예이다.

표준화된 벤치마크를 향한 움직임[^53][^64][^71][^93]은 일관성 없는 평가로 인한 혼란에 대한 직접적인 대응이며, 보다 엄격하고 비교 가능한 연구로 나아가려는 움직임을 나타낸다. IM-IAD[^64], TimeSeriesBench[^71], ADer[^53]와 같은 벤치마크의 명시적인 목표는 통일된 설정, 포괄적인 지표, 공정한 비교를 제공하는 것이다. 이는 비판에서 강조된 일관성 없는 프로토콜과 비교 불가능한 결과의 문제를 직접적으로 해결한다. 이러한 추세는 진정한 발전을 가능하게 하기 위해 분야가 표준화의 필요성을 인식하고 있음을 시사한다.

복잡한 모델이 항상 간단한 모델보다 성능이 우수하지 않다는 발견[^70][^81]은 평가의 엄격함이 모델 비교뿐만 아니라 연구 방향을 안내하는 데에도 중요하다는 것을 의미하며, 잠재적으로 불필요한 복잡성에서 벗어나도록 유도할 수 있다. 엄격한 벤치마킹을 사용하는 논문[^70]은 AR이나 선형 매핑과 같은 더 간단한 방법이 특정 이상 유형(예: 점별 이상)에 대해 놀라울 정도로 효과적일 수 있음을 발견했다. 이는 더 복잡한 딥러닝 모델이 본질적으로 더 낫다는 가정을 반박하며, 결함 있는 평가가 이전에 복잡한 방법의 인지된 성능을 부풀렸을 수 있음을 시사한다. 따라서 엄격한 평가는 연구 노력을 더 간단하고, 해석 가능하며, 진정으로 더 효과적인 접근 방식으로 재정향할 수 있다.

## 5. 이미지 이상 탐지 평가

이미지 이상 탐지는 주로 **산업 검사**(제품 표면 결함 등)[^4][^6] 및 **의료 영상 분석**(병변 탐지 등)[^3][^13]과 같은 응용 분야에 중점을 둡니다. **MVTec AD**[^40][^46], **VisA**[^66], **MVTec-3D**[^66], **BTAD**[^66] 등이 널리 사용되는 벤치마크 데이터셋입니다.

### 5.1. 평가 수준

이미지 AD 평가는 크게 세 가지 수준에서 이루어진다:

*   **이미지 수준 (Image-Level)**: 전체 이미지를 정상 또는 이상으로 분류합니다. 이미지당 단일 점수를 출력합니다. 일반적인 지표로는 **이미지 수준 AUROC (I-AUC)**[^41][^43]와 이미지 레이블에 적용된 표준 분류 지표(P/R/F1/Accuracy/MCC)가 있습니다.
*   **픽셀 수준 (Pixel-Level, Anomaly Localization/Segmentation)**: 이미지 내에서 이상이 발생한 특정 영역 또는 픽셀을 식별합니다. 이상 맵(anomaly map) 또는 히트맵(heatmap) 형태의 출력을 생성합니다. 픽셀 단위의 정밀한 정답 레이블(ground truth annotation)이 필요합니다[^38][^47].
*   **객체 수준 (Object-Level)**: 이미지 내의 비정상적인 구조나 객체를 기반으로 탐지를 평가합니다. 픽셀 수준 평가보다 덜 일반적이지만, 이상이 명확한 객체 형태일 때 유용합니다[^42].

### 5.2. 주요 픽셀 수준 평가 지표

*   **픽셀 수준 AUROC (Pixel-AUROC, P-AUC)**: 픽셀 단위 예측에 대해 계산된 표준 AUROC입니다[^38][^41][^43]. 널리 보고되는 지표입니다. 그러나 다수의 정상 픽셀이나 큰 이상 영역에 의해 점수가 지배되어 작은 이상에 대한 낮은 성능을 가릴 수 있다는 한계가 있습니다[^47][^57].
*   **영역별 중첩률 AUC (Per-Region-Overlap AUC, PRO-AUC 또는 AUPRO)**: 각 실제 이상 영역에 대해 독립적으로 중첩 점수를 계산하고 평균을 내어, 영역 크기에 관계없이 모든 영역에 동일한 가중치를 부여합니다[^57][^62]. 이는 픽셀-AUROC의 크기 편향 문제를 해결하기 위해 설계되었습니다. 종종 P-AUC와 함께 보고됩니다[^63].
*   **Dice 유사도 계수 (Dice Similarity Coefficient, DSC) / F1 점수**: 예측된 마스크와 실제 마스크 간의 중첩을 측정합니다[^38][^47]. 종종 최적의 임계값에서 계산됩니다. 임계값 선택에 민감할 수 있습니다. Soft DSC와 같은 변형도 존재합니다. 픽셀 수준 F1 점수도 사용됩니다[^41].
*   **IoU (Intersection over Union)**: 세분화(segmentation)에서 흔히 사용되는 또 다른 중첩 지표입니다[^38].
*   **평균 정밀도 (Average Precision, AP) / 평균 정밀도 평균 (mean Average Precision, mAP)**: 주로 객체 탐지[^29][^30][^31][^32][^36]와 관련되지만, 픽셀 수준 정밀도-재현율 곡선에서 계산된 AP는 이상 위치 파악에도 사용되며[^38], 특히 불균형 상황에서 픽셀-AUROC보다 선호되기도 합니다[^33][^34]. mAP는 클래스 또는 IoU 임계값에 걸쳐 AP를 평균한 값입니다[^30].

이미지 AD 평가는 이상 이미지를 탐지하는 것(이미지 수준)과 그 안에서 이상을 위치 파악하는 것(픽셀 수준)을 구별해야 한다. 모델이 한쪽에서는 뛰어날 수 있지만 다른 쪽에서는 그렇지 않을 수 있기 때문이다. 이미지 수준 지표인 I-AUROC는 모델이 전반적으로 이상 이미지에 더 높은 점수를 부여했는지 여부만 알려준다[^41]. 모델이 이상의 위치를 아는 것을 보장하지는 않는다. 픽셀 수준 지표(P-AUC, PRO-AUC, DSC, IoU)는 위치 파악 정확도를 구체적으로 평가한다[^38][^47][^57]. 논문들은 종종 이 차이를 인식하고 두 수준의 지표를 모두 보고한다[^41][^43]. 일부 방법은 탐지는 잘하지만 위치 파악은 못하거나 그 반대일 수 있다.

PRO-AUC의 개발[^57]은 이상 위치 파악에서 표준 픽셀-AUROC의 특정 부적절성, 즉 영역 크기에 대한 민감성을 강조한다. 이는 작업의 특정 과제에 맞는 지표의 필요성을 보여준다. 픽셀-AUROC는 모든 픽셀을 동일하게 취급한다[^47]. 크고 탐지하기 쉬운 이상 영역은 많은 올바르게 분류된 픽셀에 기여하여, 모델이 여러 작은 이상을 완전히 놓치더라도 P-AUC 점수를 부풀릴 수 있다. PRO-AUC는 이 문제를 해결하기 위해 명시적으로 설계되었으며[^57], 영역별 중첩 점수를 평균하여 작은 이상이 무시되지 않도록 보장한다. 이러한 발전은 픽셀 수준 AD에 특화된 평가 사고의 개선을 보여준다.

객체 탐지 및 정보 검색에서 흔히 사용되는 지표인 AP/mAP[^29][^30]를 픽셀 수준 이상 위치 파악에 사용하는 것은 개념적 연결을 시사한다: 이상 위치 파악을 관련성 있는(비정상적인) 픽셀 검색으로 취급하는 것이다. AP/mAP는 기본적으로 관련 항목(객체, 문서 또는 이 경우 픽셀)의 순위 평가를 수행한다. 픽셀 수준 AD에서의 사용[^38]은 많은 정상 픽셀 속에서 "비정상 픽셀 찾기"라는 렌즈를 통해 작업을 보는 것을 의미하며, 이는 대규모 코퍼스에서 관련 문서를 찾는 것과 유사하다. 이러한 관점은 이상 픽셀이 적은 일반적인 불균형 상황에서 특히 관련성이 있으며, 이 경우 PR 곡선(AP의 기초)이 ROC 곡선보다 더 유익한 경우가 많다[^33][^34].

다음 표는 이미지 이상 탐지에 사용되는 주요 평가 지표를 요약합니다.

**표 1: 이미지 이상 탐지를 위한 주요 평가 지표**

| 지표 이름             | 평가 수준 | 설명                                                                 | 주요 특징/초점                               |
| :-------------------- | :-------- | :------------------------------------------------------------------- | :------------------------------------------- |
| **Image-AUROC (I-AUC)** | 이미지    | 전체 이미지에 대한 AUROC 점수.                                       | 전반적인 이미지 분류 성능 (정상 vs. 이상).   |
| **Pixel-AUROC (P-AUC)** | 픽셀      | 픽셀 단위 예측에 대한 AUROC 점수.                                    | 픽셀 수준 이상 위치 파악 정확도.             |
| **PRO-AUC (AUPRO)**     | 픽셀      | 각 실제 이상 영역의 중첩 점수를 평균.                                | 크기에 무관한 이상 영역 위치 파악 성능.      |
| **Pixel-F1 / DSC**    | 픽셀      | 예측 마스크와 실제 마스크 간의 중첩 측정<br>(조화 평균 또는 Dice 계수). | 픽셀 수준 분할 중첩 성능.                    |
| **Pixel-AP / mAP**    | 픽셀      | 픽셀 수준 PR 곡선 아래 면적<br>(또는 클래스/IoU 임계값 평균).         | 이상 픽셀의 순위 결정 및 검색 성능.          |

## 6. 시계열 이상 탐지 평가

시계열 이상 탐지는 센서 판독값, 금융 데이터, 시스템 로그, 네트워크 트래픽과 같은 순차적 데이터에 초점을 맞춥니다[^18][^67].

### 6.1. 시계열 데이터 평가의 특수 과제

*   **시간적 의존성 (Temporal Dependency)**: 이상은 문맥적일 수 있으며, 개별적으로는 정상이지만 이전 값들을 고려할 때 비정상적일 수 있습니다. 표준 지표는 종종 독립 동일 분포(i.i.d.) 데이터를 가정합니다[^18][^70].
*   **범위 기반 이상 (Range-Based Anomalies)**: 이상은 종종 단일 시점이 아닌 연속적인 구간(이벤트)에 걸쳐 발생합니다[^68][^70].
*   **적시성 (Timeliness)**: 조기 탐지가 중요한 경우가 많습니다[^76][^81].

### 6.2. 점별 평가 대 범위 기반 평가

*   **점별 지표 (Point-wise Metrics)**: 표준 P/R/F1, 점별 AUROC 등 각 시점을 독립적으로 평가합니다.
*   **한계**: 시간적 상관관계를 고려하지 못하고, 사소한 시간적 이동에도 불이익을 주며, 범위 기반 이상에 대해 오해의 소지가 있는 점수를 제공할 수 있습니다(예: 긴 탐지 구간의 모든 점을 TP로 계산하여 점수 부풀리기)[^28][^70].

### 6.3. 주요 시계열 특화 지표

점별 지표의 한계를 극복하기 위해 다양한 시계열 특화 지표가 제안되었습니다.

*   **점 조정 F1 (Point-Adjusted F1, PA-F1)**: 실제 이상 구간 내의 어떤 한 점이라도 탐지되면, 해당 구간 전체가 재현율 계산 시 TP로 간주됩니다[^70]. 부분적인 탐지라도 인정하려는 목적을 가집니다. **비판**: 쉽게 조작될 수 있고, 부정확하거나 불충분한 탐지를 보상하며, 단일 탐지점으로 전체 구간에 대한 완전한 점수를 부여하여 성능을 과대평가할 수 있습니다[^70][^74].
*   **범위 기반 정밀도/재현율/F1 (Range-based P/R/F1 - General Concept)**: 예측된 이상 구간과 실제 이상 구간 간의 중첩 또는 관계를 기반으로 평가하도록 설계된 지표들입니다. 여러 구체적인 구현이 존재합니다:
    *   **Tatbul et al. (R-based P/R)**: NeurIPS 2018[^69][^72]. 존재(existence), 크기(size), 위치 편향(positional bias)을 사용자 정의 가능한 가중치 함수(α,β,δ)를 통해 고려합니다. 유연성을 목표로 합니다.
    *   **Hwang et al. (TS-Aware P/R)**[^73]: 예측이 이상 구간의 최소 비율(θ) 이상을 포함해야 탐지로 간주한다. R-based P/R보다 간단하다.
    *   **TaPR (Time-series Aware Precision and Recall)**[^73]: 모든 공격(이상)을 동일하게 평가한다고 언급되어, 가중치 접근 방식에 비해 잠재적 한계가 있음을 시사한다. [^73]에서 제안된 개선 버전은 "실제 영향 구간(real affected section)"을 고려한다.
    *   **PATE (Proximity-Aware Time series anomaly Evaluation)**[^75][^76]: 이상 구간 주변의 버퍼 영역(buffer zone)을 고려한 근접성 기반 가중치(proximity-based weighting)를 사용하며, 가중치가 적용된 AUPRC를 계산한다. 시간적 근접성을 고려한 미묘한 평가를 목표로 한다.

*   **소속 지표 (Affiliation Metrics)**: KDD 2022[^74]. 실제 이벤트 주변의 "소속 영역(zone of affiliation)"을 기반으로 P/R을 확장한다. 파라미터가 없고, 해석 가능하며, 점 및 범위 이상 모두에 적용 가능하고, 조작 전략에 강건하다.
*   **NAB 점수 (NAB Score)**: Numenta Anomaly Benchmark[^76][^81]. 실시간 스트리밍 이상 탐지를 위한 전용 점수 체계이다. 이상 윈도우(anomaly window)와 애플리케이션 프로파일(FP/FN 가중치 차등 적용)을 사용하여 조기 탐지를 보상하고 잘못된 경보에 불이익을 준다. 점수는 0-100으로 정규화된다.
*   **평균 탐지 시간 (Mean Time To Detection, MTTD)**: 탐지 속도를 측정하며, 실시간 시스템에서 중요하다[^76].

점별 지표에서 범위 기반 지표로의 진화는 시간적 맥락과 이상 지속 시간이 표준 지표가 무시하는 시계열 AD의 중요한 측면이라는 근본적인 인식을 반영한다. 점별 지표에 대한 명시적인 비판[^28][^70]과 다양한 범위 기반 대안의 후속 개발[^68][^69][^73][^74][^75]은 이해의 명확한 변화를 보여준다. 연구자들은 시계열 포인트를 독립적인 이벤트로 취급하는 것이 이상 구조와 시간적 관계에 대한 중요한 정보를 놓치고 잠재적으로 오해의 소지가 있는 평가로 이어진다는 것을 깨달았다.

범위 기반 지표 내에서도 부분 탐지를 어떻게 보상하고 시간적 불일치를 처리할지에 대한 지속적인 개선과 논쟁이 있다(예: PA-F1[^70] 대 R-based P/R[^69] 대 Affiliation[^74] 대 PATE[^75]). PA-F1 접근 방식은 간단한 조정이지만 너무 관대하다는 비판을 받는다[^70][^74]. R-based P/R은 사용자 정의 기능을 제공하지만 파라미터를 도입한다[^69]. 소속 지표는 파라미터 없는 강건성을 목표로 한다[^74]. PATE는 근접성 가중치를 도입한다[^75]. 이러한 다양성은 "좋은 범위 탐지"라는 복잡한 개념을 스칼라 값으로 매핑하는 단일 합의된 방법이 없음을 보여주며, 이는 새로운 지표의 지속적인 혁신과 제안으로 이어진다[^73][^74][^75].

NAB 벤치마크와 점수[^76][^81]는 배치 평가를 위해 설계된 지표(AUROC, AUPRC, 표준 범위 기반 F1 등)가 종종 간과하는 스트리밍 AD의 고유한 요구 사항(적시성, 지속적인 학습)을 구체적으로 다룬다. NAB의 설계는 이상 윈도우 내에서 조기 탐지를 명시적으로 보상하고 스트리밍 맥락에서 잘못된 경보에 불이익을 준다[^81]. 이는 전체 데이터셋을 회고적으로 평가하고 본질적으로 실시간 측면을 포착하지 못하는 일반적인 배치 지표와 대조된다. 이는 오프라인 분석 시스템과 온라인 모니터링 시스템 간에 평가 요구 사항이 크게 다르다는 것을 강조한다.

다음 표는 시계열 이상 탐지에 사용되는 주요 평가 지표를 요약합니다.

**표 2: 시계열 이상 탐지를 위한 주요 평가 지표**

| 지표 이름                           | 평가 유형     | 주요 특징/초점                                                              |
| :---------------------------------- | :------------ | :-------------------------------------------------------------------------- |
| **점별 F1 (Point-wise F1)**         | 점            | 각 시점을 독립적으로 평가. 범위 무시.                                       |
| **점 조정 F1 (PA-F1)**              | 이벤트/범위   | 이상 구간 내 단일 탐지 시 전체 구간 인정.                                   |
| **범위 기반 P/R/F1 (R-based P/R/F1)** | 이벤트/범위   | 존재, 크기, 위치 편향 등 사용자 정의 가능한 범위 중첩 평가.                 |
| **TS-Aware P/R**                    | 이벤트/범위   | 예측이 이상 구간의 특정 비율(θ) 이상 포함해야 탐지로 간주.                  |
| **TaPR**                            | 이벤트/범위   | 시계열 인식 P/R. 개선 버전은 '실제 영향 구간' 고려.                         |
| **소속 지표 (Affiliation P/R)**     | 이벤트/범위   | 실제 이벤트 주변의 '소속 영역' 기반 P/R 확장. 파라미터 없음.                |
| **PATE**                            | 이벤트/범위   | 버퍼 영역을 고려한 근접성 기반 가중치 적용. 가중 AUPRC 계산.                |
| **NAB 점수**                        | 스트리밍      | 실시간 조기 탐지 보상, FP/FN 차등 불이익.                                   |

## 7. 종합 및 권장 사항

이상 탐지 평가 지표의 환경은 기본적이고, 불균형에 초점을 맞추며, 이미지 및 시계열 데이터에 특화된 접근 방식으로 나뉩니다. 효과적인 평가는 기본 지표의 이해와 함께 데이터 및 작업의 특정 문제를 해결하는 고급 지표의 적용을 필요로 합니다.

다음 표는 본 보고서에서 논의된 가장 중요한 평가 지표들을 종합적으로 요약합니다.

**표 3: 주요 이상 탐지 평가 지표 종합 요약**

| 지표 이름                                       | 간략 설명                                                     | 주요 사용 사례/초점                               | 주요 강점                                       | 주요 한계                                         |
| :---------------------------------------------- | :------------------------------------------------------------ | :------------------------------------------------ | :---------------------------------------------- | :------------------------------------------------ |
| **정밀도 (Precision)**                          | 예측된 이상 중 실제 이상의 비율                               | FP 비용이 높을 때 (예: 경보 피로)                 | 예측의 정확성 측정                              | FN 무시                                           |
| **재현율 (Recall)**                             | 실제 이상 중 올바르게 예측된 비율                             | FN 비용이 높을 때 (예: 중요 이벤트 누락)          | 모든 실제 이상 탐지 능력 측정                   | FP 무시                                           |
| **F1-점수 (F1-Score)**                          | 정밀도와 재현율의 조화 평균                                   | 정밀도와 재현율 균형                              | 단일 값으로 균형 제공                           | 임계값 의존성, 불균형에 약함                      |
| **AUROC**                                       | ROC 곡선 아래 면적 (TPR vs FPR)                               | 일반적인 분류 성능 비교                           | 임계값 독립성, 순위 결정 능력 평가              | 심각한 불균형 시 성능 왜곡 가능                   |
| **AUPRC (PR-AUC)**                              | PR 곡선 아래 면적 (Precision vs Recall)                       | 불균형 데이터, 희귀 이벤트 탐지                   | 소수 클래스 성능에 집중                         | AUROC보다 불안정, 편향 증폭 가능성                |
| **MCC**                                         | 관찰/예측 간 상관 계수<br>(TP,TN,FP,FN 모두 사용)              | 불균형 데이터에서의 균형 잡힌 평가                | 모든 혼동 행렬 범주 고려,<br>불균형에 강건      | TN 필요, 특정 조건에서 정의되지 않음              |
| **Image-AUROC**                                 | 이미지 수준 AUROC                                             | 전체 이미지 분류 (정상 vs. 이상)                  | 이미지 단위 탐지 성능 요약                      | 위치 정보 없음                                    |
| **Pixel-AUROC**                                 | 픽셀 수준 AUROC                                               | 픽셀 단위 이상 위치 파악                          | 픽셀별 분류 성능 측정                           | 큰 영역/픽셀 수에 지배될 수 있음                  |
| **PRO-AUC**                                     | 영역별 중첩률 기반 AUC                                        | 크기 불변 픽셀 수준 위치 파악                     | 작은 이상 영역 평가에 유리                      | 계산 복잡성                                       |
| **PA-F1**                                       | 점 조정 F1 (시계열)                                           | 범위 기반 이상 탐지 (단순화)                      | 부분 탐지 인정                                  | 성능 과대평가, 조작 용이성                        |
| **범위 기반 P/R/F1<br>(R-based, Affiliation 등)** | 예측/실제 이상 구간 관계 평가                                 | 시계열 범위 이상 평가                             | 시간적 특성 반영                                | 구현/해석 복잡성, 합의 부족                       |
| **NAB 점수**                                    | 실시간 스트리밍 평가 프레임워크                               | 스트리밍 이상 탐지                                | 조기 탐지 보상, 실시간 특성 반영                | 특정 벤치마크에 한정됨                            |

**평가 지표 선택 지침:**

*   **데이터 유형 고려**: 이미지 데이터에는 이미지 특화 지표(Pixel-AUROC, PRO-AUC 등)를, 시계열 데이터에는 시계열 특화 지표(범위 기반 P/R/F1, Affiliation, NAB 등)를 사용합니다.
*   **작업 목표 고려**: 단순히 이상 발생 여부를 탐지하는 것(Image-AUROC, 표준 P/R/F1/MCC/AUPRC)인지, 이상의 위치를 파악하는 것(Pixel-AUROC, PRO-AUC, Pixel-F1/AP)인지, 아니면 실시간 스트리밍 성능(NAB Score, MTTD)이 중요한지에 따라 지표를 선택합니다.
*   **클래스 불균형 고려**: 불균형이 심하고 소수 클래스(이상) 성능이 중요하다면, 표준 정확도/F1/AUROC 대신 AUPRC, MCC, 균형 정확도 또는 특화된 지표를 우선적으로 고려합니다. AUPRC와 AUROC 간의 미묘한 차이(오류 우선순위)를 이해하고 작업 목표에 맞춰 선택합니다[^84][^96].
*   **이상 특성 고려**: 시계열 데이터의 경우 이상이 점 형태인지 범위 형태인지, 이미지의 경우 작은 이상과 큰 이상의 중요도가 같은지 등을 고려합니다.
*   **오류 비용 고려**: 정밀도와 재현율 중 더 비용이 큰 오류 유형에 민감한 지표를 선택하거나 F-beta 점수를 사용하여 가중치를 조절합니다.
*   **다중 지표 평가의 중요성**: 어떤 단일 지표도 완벽하지 않으므로, 여러 지표(예: AUROC와 AUPRC[^84], 또는 I-AUC와 PRO-AUC[^57])를 함께 보고하는 것이 모델 성능에 대한 더 완전하고 균형 잡힌 시각을 제공합니다[^7][^27].

**한계 및 향후 방향**: 더 나은 평가 지표에 대한 연구는 계속 진행 중이며[^74][^75], 더 신뢰할 수 있는 벤치마크 구축의 필요성도 강조되고 있습니다[^71][^93][^101]. 또한, 정량적인 지표 외에도 모델의 **설명 가능성**[^15][^26]과 **계산 효율성**과 같은 질적인 측면도 실제 적용에서 중요한 고려 사항입니다.

**결론적으로**, 효과적인 이상 탐지 평가는 기본 지표에 대한 이해를 바탕으로 데이터 유형, 작업 목표, 불균형 정도, 이상 특성, 오류 비용 등 다양한 맥락을 고려한 신중한 지표 선택 과정을 요구합니다. 표준화된 벤치마크와 새로운 평가 지표에 대한 지속적인 연구는 이 분야의 건전한 발전을 위해 필수적입니다. 연구자와 실무자는 기존 지표의 한계를 인식하고 새로운 대안을 비판적으로 검토하여 자신의 요구에 가장 적합한 평가 전략을 수립해야 합니다.

---

## 참고 자료

[^1]: [Evaluating Deep Learning Algorithms for Log-Based Anomaly Detection - Journal of Information Systems Engineering and Management](https://jisem-journal.com/archives/jisem-vol-7-no-1-2024/evaluating-deep-learning-algorithms-for-log-based-anomaly-detection)
[^2]: [Anomaly Detection : A Survey - cucis](https://cucis.ece.northwestern.edu/projects/DMS/publications/AnomalyDetection.pdf)
[^3]: [Contrastive Learning-Based Anomaly Detection for Actual Corporate Environments - PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10852918/)
[^4]: [Anomaly Detection for Industrial Applications, Its Challenges, Solutions, and Future Directions: A Review - arXiv](https://arxiv.org/abs/2307.08692)
[^5]: [On the Influence of Data Resampling for Deep Learning-Based Log Anomaly Detection: Insights and Recommendations - IEEE Computer Society](https://www.computer.org/csdl/proceedings-article/issre/2023/1038600a011/1U6k9Gg0s3G)
[^6]: [Unsupervised Anomaly Detection on Metal Surfaces Based on Frequency Domain Information Fusion - MDPI](https://www.mdpi.com/2076-3417/13/19/10989)
[^7]: [Evaluating Anomaly Detection Algorithms: A Multi-Metric Analysis Across Variable Class Imbalances - Lamarr Institute](https://lamarr-institute.org/wp-content/uploads/2024/02/Evaluating-Anomaly-Detection-Algorithms-A-Multi-Metric-Analysis-Across-Variable-Class-Imbalances.pdf)
[^8]: [Exploring the Impact of Outlier Variability on Anomaly Detection Evaluation Metrics - arXiv](https://arxiv.org/abs/2402.08613)
[^9]: [A Survey of Anomaly Detection in Cyber-Physical Systems - arXiv](https://arxiv.org/abs/2303.01095)
[^10]: [Evaluation metrics: leave your comfort zone and try MCC and Brier Score - Towards Data Science](https://towardsdatascience.com/evaluation-metrics-leave-your-comfort-zone-and-try-mcc-and-brier-score-6b6d6b053814)
[^11]: [Beyond Traditional Methods: A Novel Approach to Anomaly Detection and Classification Using AI Techniques - ijsdcs.com](https://ijsdcs.com/index.php/ijsdcs/article/view/103)
[^12]: [What metrics are used for anomaly detection performance? - Milvus Blog](https://blog.milvus.io/what-metrics-are-used-for-anomaly-detection-performance-101)
[^13]: [Beyond Heatmaps: A Comparative Analysis of Metrics for Anomaly Localization in Medical Images - OpenReview](https://openreview.net/forum?id=6-1_d7QKpA)
[^14]: [Machine Learning Evaluation of Imbalanced Health Data: A Comparative Analysis of Balanced Accuracy, MCC, and F1 Score | Request PDF - ResearchGate](https://www.researchgate.net/publication/378691184_Machine_Learning_Evaluation_of_Imbalanced_Health_Data_A_Comparative_Analysis_of_Balanced_Accuracy_MCC_and_F1_Score)
[^15]: [[2302.06670] Explainable Anomaly Detection in Images and Videos: A Survey - arXiv](https://arxiv.org/abs/2302.06670)
[^16]: [Enhancing Network Anomaly Detection Using Graph Neural Networks - OSTI](https://www.osti.gov/servlets/purl/1988869)
[^17]: [Evaluation of Machine Learning Algorithms for Supervised Anomaly Detection and Comparison between Static and Dynamic Thresholds - Semantic Scholar](https://pdfs.semanticscholar.org/161f/111111111111111111111111111111111111.pdf)
[^18]: [Deep Learning for Time Series Anomaly Detection: A Survey - arXiv](https://arxiv.org/abs/2007.02508)
[^19]: [The Matthews correlation coefficient (MCC) is more reliable than balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix evaluation - OUCI](https://ouci.dntb.gov.ua/en/works/7MCC/)
[^20]: [A Comprehensive Survey on Graph Anomaly Detection with Deep Learning - arXiv](https://arxiv.org/abs/2111.03526)
[^21]: [Apart from F-1 measure, which other metrics can one use to evaluate binary rating data? - ResearchGate](https://www.researchgate.net/post/Apart_from_F-1_measure_which_other_metrics_can_one_use_to_evaluate_binary_rating_data)
[^22]: [[Q] Precision, recall and Matthews Correlation Coefficient for anomaly detection - Reddit](https://www.reddit.com/r/statistics/comments/20vj5c/q_precision_recall_and_matthews_correlation/)
[^23]: [Deep Learning for Time Series Anomaly Detection: A Survey - arXiv](https://arxiv.org/abs/2211.05244) (Note: Duplicate arXiv ID, likely refers to the same survey as [^18])
[^24]: [(PDF) Improving Imbalanced Classification by Anomaly Detection - ResearchGate](https://www.researchgate.net/publication/332880018_Improving_Imbalanced_Classification_by_Anomaly_Detection)
[^25]: [Matthews Correlation Coefficient - why not in wider use? [Discussion] : r/statistics - Reddit](https://www.reddit.com/r/statistics/comments/7q069c/matthews_correlation_coefficient_why_not_in_wider/)
[^26]: [A Survey on Explainable Anomaly Detection - arXiv](https://arxiv.org/abs/2201.07264)
[^27]: [A Study on Performance Metrics for Anomaly Detection Based on Industrial Control System Operation Data - MDPI](https://www.mdpi.com/2076-3417/13/13/7808)
[^28]: [(PDF) "Do you know existing accuracy metrics overrate time-series anomaly detections?" - ResearchGate](https://www.researchgate.net/publication/361481871_Do_you_know_existing_accuracy_metrics_overrate_time-series_anomaly_detections)
[^29]: [Mean Precision in Object Detection and Computer Vision - Keylabs](https://keylabs.ai/blog/mean-precision-in-object-detection-and-computer-vision/)
[^30]: [Mean Average Precision (mAP) Explained: Everything You Need to Know - V7 Labs](https://v7labs.com/blog/mean-average-precision/)
[^31]: [mAP or "mean Average Precision", a key AI metric - Innovatiana](https://en.innovatiana.com/map-or-mean-average-precision-a-key-ai-metric/)
[^32]: [Mean Average Precision Made Simple With A Complete Guide - Spot Intelligence](https://spotintelligence.com/mean-average-precision-map-guide/)
[^33]: [How is anomaly detection evaluated? - Zilliz Vector Database](https://zilliz.com/learn/anomaly-detection-evaluation)
[^34]: [How is anomaly detection evaluated? - Milvus Blog](https://blog.milvus.io/how-is-anomaly-detection-evaluated-101)
[^35]: [Anomaly Detection - How to Tell Good Performance from Bad | Towards Data Science](https://towardsdatascience.com/anomaly-detection-how-to-tell-good-performance-from-bad-9a40407f59cd)
[^36]: [Mean Average Precision (mAP) in Object Detection : r/learnmachinelearning - Reddit](https://www.reddit.com/r/learnmachinelearning/comments/k4zp0z/mean_average_precision_map_in_object_detection/)
[^37]: [One-for-All Few-Shot Anomaly Detection via Instance-Induced Prompt Learning - OpenReview](https://openreview.net/forum?id=0-1_d7QKpA)
[^38]: [Metrics — Anomalib documentation - Read the Docs](https://anomalib.readthedocs.io/en/latest/guides/metrics.html)
[^39]: [A Benchmark for Pixel-Level Anomaly Detection in Continual Learning - arXiv](https://arxiv.org/abs/2308.08115)
[^40]: [MVTecAD (MVTEC ANOMALY DETECTION DATASET) - Papers With Code](https://paperswithcode.com/dataset/mvtecad)
[^41]: [Image-level and pixel-level anomaly detection results with teacher network using pretraining and nonpretraining. - ResearchGate](https://www.researchgate.net/figure/Image-level-and-pixel-level-anomaly-detection-results-with-teacher-network-using_tbl1_369880018)
[^42]: [Advancing Anomaly Detection: The IDW dataset and MC algorithm - BMVA Archive](https://bmva-archive.org.uk/bmvc/2023/abs/0101.html)
[^43]: [Image-and Pixel-level AUROC performance comparison of anomaly detection... | Download Scientific Diagram - ResearchGate](https://www.researchgate.net/figure/Image-and-Pixel-level-AUROC-performance-comparison-of-anomaly-detection-methods-on-the_fig3_369880018)
[^44]: [Anomaly Detection using Score-based Perturbation Resilience | Papers With Code](https://paperswithcode.com/paper/anomaly-detection-using-score-based)
[^45]: [MVTec Anomaly Detection Dataset - mvtec.com](https://www.mvtec.com/company/research/datasets/mvtec-ad)
[^46]: [Superpixel-based Anomaly Detection for Irregular Textures with a Focus on Pixel-level Accuracy - Toby Breckon](https://breckon.org/toby/publications/papers/wang21superpixel.pdf)
[^47]: [Anomaly Detection Using Computer Vision: A Comparative Analysis of Class Distinction and Performance Metrics - arXiv](https://arxiv.org/abs/2308.08115)
[^48]: [Attention-Guided Perturbation for Unsupervised Image Anomaly Detection - arXiv](https://arxiv.org/abs/2308.08115)
[^49]: [Pixel-Wise Anomaly Detection in Complex Driving Scenes - CVF Open Access](https://openaccess.thecvf.com/content/CVPR2023/html/Grigorescu_Pixel-Wise_Anomaly_Detection_in_Complex_Driving_Scenes_CVPR_2023_paper.html)
[^50]: [A Benchmark for Pixel-Level Anomaly Detection in Continual Learning - arXiv](https://arxiv.org/abs/2308.08115) (Note: Duplicate arXiv ID)
[^51]: [Deep Industrial Image Anomaly Detection: A Survey - arXiv](https://arxiv.org/abs/2301.07134)
[^52]: [ADer: A Comprehensive Benchmark for Multi-class Visual Anomaly Detection - arXiv](https://arxiv.org/abs/2301.07134)
[^53]: [ADer: A Comprehensive Benchmark for Multi-class Visual Anomaly Detection - arXiv](https://arxiv.org/abs/2301.07134) (Note: Duplicate arXiv ID)
[^54]: [An Unsupervised Method for Industrial Image Anomaly Detection with Vision Transformer-Based Autoencoder - PMC](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10852918/)
[^55]: [Explainable Anomaly Detection in Images and Videos: A Survey - arXiv](https://arxiv.org/abs/2302.06670) (Note: Duplicate arXiv ID)
[^56]: [Dual-Image Enhanced CLIP for Zero-Shot Anomaly Detection - arXiv](https://arxiv.org/abs/2308.08115)
[^57]: [PRO scores of various methods on MVTec dataset. - ResearchGate](https://www.researchgate.net/figure/PRO-scores-of-various-methods-on-MVTec-dataset_tbl3_369880018)
[^58]: [MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection - mvtec.com](https://www.mvtec.com/company/research/datasets/mvtec-ad) (Note: Duplicate URL)
[^59]: [Performance of anomaly detection cloud platform with MVTec dataset - DEV Community](https://dev.to/amazon-web-services/performance-of-anomaly-detection-cloud-platform-with-mvtec-dataset-4g4i)
[^60]: [AUCs of MVTec AD dataset using AnoGAN, GANomaly, Skip-GANomaly, and the pro- posed method. - ResearchGate](https://www.researchgate.net/figure/AUCs-of-MVTec-AD-dataset-using-AnoGAN-GANomaly-Skip-GANomaly-and-the-pro-posed-method_tbl2_369880018)
[^61]: [IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing - arXiv](https://arxiv.org/abs/2301.07134)
[^62]: [Applying Machine Learning Techniques for Anomaly Detection in Wooden Plank Images - Diva Portal](https://uu.diva-portal.org/smash/get/diva2:1768000/FULLTEXT01.pdf)
[^63]: [Machine Learning Datasets - Papers With Code](https://paperswithcode.com/datasets)
[^64]: [IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing - arXiv](https://arxiv.org/abs/2301.07134) (Note: Duplicate arXiv ID)
[^65]: [Towards Interpretable Video Anomaly Detection - University of South Florida](https://sis.eng.usf.edu/~aalipour/Publications/InterpretableVAD.pdf)
[^66]: [Deep Industrial Image Anomaly Detection: A Survey - arXiv](https://arxiv.org/abs/2301.07134) (Note: Duplicate arXiv ID)
[^67]: [Creating a time series anomaly prediction experiment (Watson Machine Learning) - IBM](https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=timeseries-creating-anomaly-prediction-experiment)
[^68]: [Precision and Recall for Range-Based Anomaly Detection - mlsys.org](https://mlsys.org/Conferences/2022/papers/100)
[^69]: [Precision and Recall for Time Series - NIPS papers](https://papers.nips.cc/paper/2018/hash/140f6969d5213fd0ece03148e62e461e-Abstract.html)
[^70]: [Anomaly Detection in Time Series: A Comprehensive Evaluation - VLDB Endowment](https://www.vldb.org/pvldb/vol15/p2798-papenbrock.pdf)
[^71]: [TimeSeriesBench: An Industrial-Grade Benchmark for Time Series Anomaly Detection Models - arXiv](https://arxiv.org/abs/2310.00388)
[^72]: [Reviews: Precision and Recall for Time Series - NIPS papers](https://proceedings.neurips.cc/paper/2018/file/140f6969d5213fd0ece03148e62e461e-Reviews.html)
[^73]: [navigating the metric maze:ataxonomy of evaluation metrics for anomaly detection in time series - arXiv](https://arxiv.org/abs/2306.01810)
[^74]: [Local Evaluation of Time Series Anomaly Detection Algorithms - arXiv](https://arxiv.org/abs/2206.13167)
[^75]: [Evaluating Time Series Anomaly Detection: Proximity-Aware Time Series Anomaly Evaluation (PATE) - MarkTechPost](https://www.marktechpost.com/2023/06/11/evaluating-time-series-anomaly-detection-proximity-aware-time-series-anomaly-evaluation-pate/)
[^76]: [PATE: Proximity-Aware Time series anomaly Evaluation - arXiv](https://arxiv.org/abs/2306.01810) (Note: Duplicate arXiv ID, likely refers to the same paper as [^73])
[^77]: [NAB Dataset - the Numenta Anomaly Benchmark - Papers With Code](https://paperswithcode.com/dataset/nab)
[^78]: [Position Paper: Quo Vadis, Unsupervised Time Series Anomaly Detection? - arXiv](https://arxiv.org/abs/2210.01133)
[^79]: [Local Evaluation of Time Series Anomaly Detection Algorithms - arXiv](https://arxiv.org/abs/2206.13167) (Note: Duplicate arXiv ID)
[^80]: [[2206.13167] Local Evaluation of Time Series Anomaly Detection Algorithms - arXiv](https://arxiv.org/abs/2206.13167) (Note: Duplicate arXiv ID)
[^81]: [The Numenta Anomaly Benchmark - numenta.com](https://numenta.com/machine-intelligence-technology/numenta-anomaly-benchmark/)
[^82]: [Evaluation metric for time-series anomaly detection - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/10066/evaluation-metric-for-time-series-anomaly-detection)
[^83]: [In general, what are precision, recall, F1 that are reported in papers? - Data Science Stack Exchange](https://datascience.stackexchange.com/questions/15989/in-general-what-are-precision-recall-f1-that-are-reported-in-papers)
[^84]: [A Closer Look at AUROC and AUPRC under Class Imbalance - arXiv](https://arxiv.org/abs/1503.06410)
[^85]: [Unsupervised Anomaly Detection in Time-series: An Extensive Evaluation and Analysis of State-of-the-art Methods - arXiv](https://arxiv.org/abs/2107.01071)
[^86]: [Anomaly Detection on Numenta Anomaly Benchmark - Papers With Code](https://paperswithcode.com/sota/anomaly-detection-on-numenta-anomaly)
[^87]: [A Closer Look at AUROC and AUPRC under Class Imbalance - arXiv](https://arxiv.org/abs/1503.06410) (Note: Duplicate arXiv ID)
[^88]: [numenta/NAB: The Numenta Anomaly Benchmark - GitHub](https://github.com/numenta/NAB)
[^89]: [[D] (A paper suggests) Most Time Series Anomaly Detection Papers are Wrong - Reddit](https://www.reddit.com/r/MachineLearning/comments/w0kffu/d_a_paper_suggests_most_time_series_anomaly/)
[^90]: [Numenta Anomaly Benchmark: A Benchmark for Streaming Anomaly Detection - Domino Data Lab](https://www.dominodatalab.com/blog/numenta-anomaly-benchmark-a-benchmark-for-streaming-anomaly-detection)
[^91]: [[1510.03336] Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly Benchmark - arXiv](https://arxiv.org/abs/1510.03336)
[^92]: [Numenta Anomaly Benchmark (NAB) - Kaggle](https://www.kaggle.com/competitions/numenta-anomaly-benchmark)
[^93]: [ADGym: Design Choices for Deep Anomaly Detection - proceedings.neurips.cc](https://proceedings.neurips.cc/paper_files/paper/2022/hash/018b59ce1fd61660634a4436a409540e-Abstract-Conference.html)
[^94]: [5 Essential Stats: Mastering Area Under Precision-Recall Curve Analysis - Number Analytics](https://numberanalytics.com/area-under-precision-recall-curve/)
[^95]: [A Survey of Deep Anomaly Detection in Multivariate Time Series: Taxonomy, Applications, and Directions - MDPI](https://www.mdpi.com/2076-3417/13/13/7808)
[^96]: [A Closer Look at AUROC and AUPRC under Class Imbalance - arXiv](https://arxiv.org/abs/1503.06410) (Note: Duplicate arXiv ID)
[^97]: [NeurIPS Poster A Closer Look at AUROC and AUPRC under Class Imbalance - nips.cc](https://nips.cc/media/PosterPDFs/NeurIPS_2015/1503.06410v4.pdf)
[^98]: [Deep Learning for Anomaly Detection: A Review - ResearchGate](https://www.researchgate.net/publication/332880018_Deep_Learning_for_Anomaly_Detection_A_Review)
[^99]: [A Closer Look at AUROC and AUPRC under Class Imbalance - OpenReview](https://openreview.net/forum?id=HkxuS-rle)
[^100]: [Imbalanced Data? Stop Using ROC-AUC and Use AUPRC Instead | Towards Data Science](https://towardsdatascience.com/imbalanced-data-stop-using-roc-auc-and-use-auprc-instead-46db4512a2)
[^101]: [ADBench: Anomaly Detection Benchmark - NIPS papers](https://papers.nips.cc/paper/2022/hash/018b59ce1fd61660634a4436a409540e-Abstract-Conference.html)
[^102]: [Anomaly Detection through Conditional Diffusion Probability Modeling on Graphs - OpenReview](https://openreview.net/forum?id=HkxuS-rle)
[^103]: [A curated list of awesome anomaly detection resources - GitHub](https://github.com/yzhao062/anomaly-detection-resources)
[^104]: [Is there any real world public dataset of anomaly detection example? - HTM Forum](https://discourse.numenta.org/t/is-there-any-real-world-public-dataset-of-anomaly-detection-example/1006)
[^105]: [(PDF) Navigating the metric maze: a taxonomy of evaluation metrics for anomaly detection in time series - ResearchGate](https://www.researchgate.net/publication/371281871_Navigating_the_metric_maze_a_taxonomy_of_evaluation_metrics_for_anomaly_detection_in_time_series)
[^106]: [Evaluating the performance of health care artificial intelligence (AI): the role of AUPRC, AUROC, and average precision - KevinMD.com](https://www.kevinmd.com/2021/07/evaluating-the-performance-of-health-care-artificial-intelligence-ai-the-role-of-auprc-auroc-and-average-precision.html)
[^107]: [yzhao062/anomaly-detection-resources - GitHub](https://github.com/yzhao062/anomaly-detection-resources) (Note: Duplicate GitHub repo)
[^108]: [Position: Quo Vadis, Unsupervised Time Series Anomaly Detection? - icml.cc](https://icml.cc/media/PosterPDFs/ICML%202023/2210.01133.pdf)
[^109]: [Unsupervised Anomaly Detection in The Presence of Missing Values - NIPS papers](https://proceedings.neurips.cc/paper_files/paper/2022/hash/018b59ce1fd61660634a4436a409540e-Abstract-Conference.html)
[^110]: [Exploring What Why and How: A Multifaceted Benchmark for Causation Understanding of Video Anomaly - arXiv](https://arxiv.org/abs/2308.08115)
[^111]: [AUPRC performance of each model when training on a percentage of the... - ResearchGate](https://www.researchgate.net/figure/AUPRC-performance-of-each-model-when-training-on-a-percentage-of-the-training-data_fig4_369880018)
[^112]: [Evaluation metrics: leave your comfort zone and try MCC and Brier ... - Towards Data Science](https://towardsdatascience.com/evaluation-metrics-leave-your-comfort-zone-and-try-mcc-and-brier-score-6b6d6b053814) (Note: Duplicate URL)
[^113]: [Robust Anomaly Detection of Adventitious Auscultation Signals using Bayesian Belief Tracking - Johns Hopkins Whiting School of Engineering](https://engineering.jhu.edu/robust-anomaly-detection-of-adventitious-auscultation-signals-using-bayesian-belief-tracking/)
[^114]: [Meta-survey on outlier and anomaly detection - arXiv](https://arxiv.org/abs/2409.05383)
[^115]: [[2409.05383] Deep Learning for Video Anomaly Detection: A Review - arXiv](https://arxiv.org/abs/2409.05383) (Note: DuplicatearXiv ID)
[^116]: [Imbalanced classification vs anomaly detection - Cross Validated - Stats Stackexchange](https://stats.stackexchange.com/questions/17871/imbalanced-classification-vs-anomaly-detection)
[^117]: [Anomaly Score: Evaluating Generative Models and Individual Generated Images based on Complexity and Vulnerability - arXiv](https://arxiv.org/abs/2308.08115)
[^118]: [Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection - CVF Open Access](https://openaccess.thecvf.com/content/CVPR2023/html/Grigorescu_Pixel-Wise_Anomaly_Detection_in_Complex_Driving_Scenes_CVPR_2023_paper.html)
[^119]: [Training-Free Time-Series Anomaly Detection: Leveraging Image Foundation Models - arXiv](https://arxiv.org/abs/2310.00388)
[^120]: [Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders - arXiv](https://arxiv.org/abs/2310.00388)
[^121]: [Numenta Anomaly Benchmark (NAB) - Kaggle](https://www.kaggle.com/competitions/numenta-anomaly-benchmark) (Note: Duplicate URL)
[^122]: [Time Series Anomaly Detection using Diffusion-based Models - arXiv](https://arxiv.org/abs/2310.00388)
[^123]: [Real-Time Anomaly Detection of NAB Ambient Temperature Readings using the TensorFlow/Keras Autoencoder - Our Blogs](https://newdigitals.org/our-blogs/real-time-anomaly-detection-of-nab-ambient-temperature-readings-using-the-tensorflow-keras-autoencoder/)