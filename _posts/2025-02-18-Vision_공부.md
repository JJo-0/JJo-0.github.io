---
title: "Vision 공부"
date: 2025-02-18 14:18:00
categories:
  - 정리
  - 개발
  - Project
  - Dev
tags:
  - linux
katex: true
toc: true
toc_sticky: true
toc_label: "페이지 주요 목차"
---

## 주제

- 모르는 Vision, AI 관련 용어들 개념부터 정리 없이 올릴 생각이다.  

---

## 고른 것

### 목차

|목차|내용|비고|링크|
|:--:|:--|:--|:--|
|**Transformer**|||<a href="#transformer">바로가기</a>|
|**RNN**|||<a href="#rnn">바로가기</a>|
|**LSTM**|||<a href="#lstm">바로가기</a>|
|**ViT**|||<a href="#vit">바로가기</a>|
|**HR Net**|||<a href="#hr-net">바로가기</a>|
|**GCN**|||<a href="#gcn">바로가기</a>|

---

교수님께 들은 조언, Foundation 모델, ViT는 기본적으로 리소스가 많이 들어간다.  



### Transformer
Attention is All You Need  

SoftMax는 수식의 아래와 같다. 

$$ softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} $$

### RNN 
Recurrent Neural Network

#### Gradient Vanishing Problem (기울기 소실 문제)
[참고_링크](https://www.youtube.com/watch?v=aWInMRtySpQ)
Deep Nerual Network를 제작할 때, hidden layer(은닉 층)을 많이 쌓는다.  
은닉층 구조와 수는 모델의 성능과 복잡도에 큰 영향력을 미쳐서(좋은 모델일 수록 파라미터 가중치(weight, bias)가 많다)  
  
**가중치(weight)**  
- 입력 데이터에 대한 중요도를 결정하는 값  
- 신경망의 각 연결에 곱해지는 값  

**편향(bias)**  
- 특정 노드가 더 쉽게 활성화 되도록 하는 값  
- 가중치와 함께 신경망의 출력을 조정하는 역할  

**역전파**  
- 출력과 실제 값(정답) 사이의 오차(Error) 계산  
- 오차를 줄이기 위해 가중치와 편향을 조정  

**경사 하강법**  
- 오차를 줄이기 위해 가중치와 편향을 조정하는 방법  
- 학습률(learning rate)에 따라 업데이트 크기, 속도 결정  

$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$

여기서, $begin:math:text$w_i$end:math:text$는 가중치(weight), $begin:math:text$x_i$end:math:text$는 입력 데이터, $begin:math:text$b$end:math:text$는 편향(bias)을 의미한다.  


은닉층이 많아지면, 기울기 소실 문제가 발생한다.  
기울기 소실의 문제점은 활성화 함수의 기울기와 관련이 있다.  
역전파를 이용해 

**Sigmoid 함수**  
Sigmoid 함수를 보면 미분할 경우 sigmoid는 0~0.25 1/4 4분의 1이 난다는 것  
![Sigmoid_Derivative](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FyJnMy%2FbtrCKh4ojvw%2FLtlaMwrtEZHC3KtGNWvuLK%2Fimg.png)  

**tanh 함수**  
tanh 함수를 보면 미분할 경우 tanh는 0~1 1/2 2분의 1이 난다는 것
![Tanh_Derivative](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcq4ZGq%2FbtrCJJNA2bI%2FZhOpBc6Rj3061ZMw11AfPK%2Fimg.png)  

##### 활성화 함수
- Sigmoid
- tanh
기울기 소실의 

### MLP
Multilayer Perceptron  

#### **다층 퍼셉트론의 이해**  
- 머신러닝 분야에서 기본적인 모델 중 하나,  
- 인공 신경망 중에서도 다층 퍼셉트론(MLP)은 여러 층으로 구성된 feed forward 신경망의 한 유형  

#### **간략한 설명**
데이터 내의 복잡한 관계, 그중에서도 비선형적인 관계를 학습할 수 있는 강력한 능력을 지니고 있어 다양한 실제 문제 해결에 효과적으로 활용된다. 이미지 인식, 자연어 처리, 음성 인식 등 광범위한 응용 분야에서 핵심적인 역할을 수행 중이다.  
   
#### **MLP의 역사**
MLP의 역사는 선형적으로 분리 가능한 데이터에만 적용 가능했던 초기 단층 퍼셉트론 모델에서 시작되었습니실제 세계의 데이터는 종종 비선형적인 특성을 지니고 있어, 이러한 한계를 극복하기 위해 다층 구조의 필요성이 대두되었다.  
1970년대 초에 여러 연구자들에 의해 독립적으로 개발된 역전파(backpropagation) 알고리즘은 다층 신경망을 효과적으로 학습시키는 데 필수적인 방법론으로 자리 잡았으며 Deep learning 분야가 발전하는 데 결정적인 역할을 했다.  

#### **2. MLP의 구조: 층과 뉴런**  
MLP는 일반적으로 입력층(input layer), 하나 이상의 은닉층(hidden layer), 그리고 출력층(output layer)의 세 가지 주요 층으로 구성된다.  
입력층은 외부에서 제공되는 원시 데이터를 받아들이는 역할을 하며, 입력층의 뉴런 수는 입력 데이터의 특징(feature) 수와 일치합니다 1. 은닉층은 입력층과 출력층 사이에 위치하며, 입력 데이터에 대한 복잡한 계산과 특징 변환을 수행합니다 1. MLP는 하나 이상의 은닉층을 가질 수 있으며, 각 은닉층의 수와 뉴런 수는 모델 설계 단계에서 결정해야 하는 하이퍼파라미터입니다 1. 출력층은 네트워크의 최종 예측 결과를 생성하며, 출력층의 뉴런 수는 해결해야 하는 작업의 성격에 따라 달라집니다 (예: 분류 문제의 경우 클래스 수) 1.각 뉴런(또는 노드)은 이전 층의 뉴런으로부터 입력을 받아들여, 각 입력에 해당하는 가중치(weight)를 곱하고 편향(bias)을 더한 후, 활성화 함수(activation function)를 적용하여 출력을 생성합니다 1. 가중치는 뉴런 간의 연결 강도를 조절하는 역할을 하며 1, 편향은 활성화 함수의 출력을 조절하는 데 사용되는 추가적인 파라미터입니다 1. MLP의 중요한 특징 중 하나는 일반적으로 완전 연결(fully connected) 구조를 갖는다는 점입니다. 이는 한 층의 모든 뉴런이 다음 층의 모든 뉴런과 연결되어 있음을 의미합니다 1. 이러한 완전 연결 구조는 MLP가 데이터 내의 복잡한 패턴을 학습하는 데 강력한 기반을 제공하지만, 동시에 많은 수의 파라미터(가중치와 편향)를 필요로 하게 되어 과적합(overfitting)의 위험을 증가시키고 계산 비용을 높일 수 있다는 점을 시사합니다 3.3. MLP의 학습 방식: 역전파 알고리즘MLP는 일반적으로 지도 학습(supervised learning) 방식을 사용하여 학습됩니다. 이는 레이블이 지정된 데이터를 통해 네트워크가 입력과 출력 간의 관계를 학습하는 과정을 의미합니다 1. 학습 과정은 주로 순전파(forward propagation)와 역전파(backpropagation)라는 두 단계를 반복적으로 수행함으로써 이루어집니다. 순전파 단계에서는 입력 데이터가 네트워크를 통해 입력층에서 출력층으로 전달되어 예측 결과가 생성됩니다 1.생성된 예측 결과와 실제 정답 간의 차이를 측정하기 위해 손실 함수(loss function)가 사용됩니다 8. 회귀 문제의 경우 평균 제곱 오차(mean squared error)가, 분류 문제의 경우 교차 엔트로피(cross-entropy) 등이 대표적인 손실 함수로 사용됩니다 13. 역전파 알고리즘은 이 손실 함수의 값을 최소화하는 방향으로 네트워크의 가중치와 편향을 조정하는 핵심적인 학습 메커니즘입니다 1. 역전파 과정은 다음과 같습니다. 먼저, 미적분학의 연쇄 법칙(chain rule)을 이용하여 각 가중치와 편향에 대한 손실 함수의 기울기(gradient)를 계산합니다 2. 그런 다음, 경사 하강법(gradient descent) 또는 그 변형된 알고리즘을 사용하여 손실을 최소화하는 방향으로 가중치와 편향을 업데이트합니다 1. 이때 학습률(learning rate)은 가중치 업데이트의 크기를 조절하는 중요한 하이퍼파라미터입니다 1. 표준 경사 하강법 외에도 확률적 경사 하강법(SGD)이나 Adam과 같은 다양한 최적화 알고리즘이 사용될 수 있습니다 2. 역전파는 네트워크의 모든 가중치에 대한 기울기를 자동으로 계산할 수 있는 핵심적인 혁신 기술로서, 깊은 MLP의 효율적인 학습을 가능하게 하며, 최적의 내부 표현을 학습함으로써 "특징 엔지니어링" 과정을 효과적으로 자동화합니다 2.4. 활성화 함수: 비선형성 부여활성화 함수는 MLP의 각 뉴런에서 계산된 가중 합에 적용되는 비선형 함수로서, 네트워크에 비선형성을 도입하여 복잡한 관계를 모델링할 수 있도록 합니다 1. 만약 활성화 함수가 없다면, 다층 네트워크는 단층 선형 모델과 동일한 표현력밖에 갖지 못합니다 2.MLP에서 흔히 사용되는 활성화 함수는 다음과 같습니다.
시그모이드(Sigmoid): 입력값을 [0, 1] 범위로 압축하는 함수로, 초기 신경망에서 널리 사용되었으나 최근에는 은닉층에서 ReLU로 대체되는 경우가 많습니다 2. 이진 분류 문제의 출력층에서 자주 사용됩니다 13.
tanh (하이퍼볼릭 탄젠트): 입력값을 [-1, 1] 범위로 압축하는 함수입니다 1.
ReLU (Rectified Linear Unit): 입력값이 양수이면 그대로 출력하고, 음수이면 0을 출력하는 함수입니다 1. 계산 효율성이 높고 기울기 소실 문제 완화에 도움이 됩니다 24. 하지만 "죽어가는 ReLU(dying ReLU)" 문제에 취약할 수 있습니다 24.
소프트맥스(Softmax): 주로 다중 클래스 분류 문제의 출력층에서 사용되며, 각 클래스에 대한 확률 분포를 출력합니다 1.
이 외에도 Leaky ReLU, Softplus 등의 다양한 활성화 함수가 존재합니다 2.
활성화 함수의 선택은 해결해야 하는 작업의 종류와 네트워크의 층에 따라 달라집니다 1. 초기 퍼셉트론에서 사용되었던 Heaviside 계단 함수에서 시그모이드나 ReLU와 같은 연속적인 활성화 함수로의 전환은 역전파 알고리즘의 사용을 가능하게 하는 데 매우 중요했습니다. 이는 역전파가 미분 가능한 함수를 필요로 하기 때문입니다 2.5. 다양한 분야에서의 다층 퍼셉트론 응용MLP는 다양한 지도 학습 작업에 적합한 다재다능한 네트워크입니다 14. 주요 응용 분야는 다음과 같습니다.
이미지 인식 및 분류: 이미지 내의 객체, 숫자, 얼굴 등을 인식하고 분류하는 데 사용됩니다 1.
음성 인식: 음성 데이터를 텍스트로 변환하는 데 활용됩니다 1.
자연어 처리: 텍스트의 의미를 이해하고 감성 분석, 기계 번역 등의 작업을 수행하는 데 적용됩니다 1.
시계열 예측: 과거 데이터를 기반으로 미래 값을 예측하는 데 사용됩니다 (예: 주가 예측, 날씨 예측) 12.
회귀 분석: 연속적인 값을 예측하는 작업에 활용됩니다 1.
데이터 압축 및 암호화: 데이터의 구조를 재구성하거나 내용을 숨기는 데 사용됩니다 18.
자율 주행: 객체 감지 및 경로 조정을 위한 핵심 기술로 사용됩니다 18.
고객 관계 관리 (CRM): 사용자 프로파일링 및 순위 결정에 활용됩니다 18.
의료 진단: 의료 영상 분석 및 질병 진단에 도움을 줍니다 19.
추천 시스템: 사용자 선호도를 분석하여 제품 또는 콘텐츠를 추천하는 데 사용됩니다 19.
금융 예측: 금융 시장의 추세를 예측하는 데 활용됩니다 24.
헬스케어 및 생물의학: 다양한 생물학적 및 의학적 문제 해결에 적용됩니다 24.
로봇 공학 및 자율 시스템: 로봇 제어 및 환경 인식에 사용됩니다 24.
게임 및 강화 학습: 복잡한 환경에서의 의사 결정에 활용됩니다 24.
생물정보학: 생물학적 데이터 분석에 사용됩니다 28.
수면 무호흡증 감지: 웨어러블 센서 데이터를 이용하여 수면 무호흡증을 감지합니다 28.
수질 예측: 수질 매개변수를 예측하는 데 사용됩니다 28.
풍속 프로파일 추정: 지상 관측 데이터를 이용하여 풍속 프로파일을 추정합니다 28.
사회적 기능 장애 예측: 위험에 처한 개인을 식별하는 데 사용됩니다 30.
운전자 졸음 감지: EEG 신호를 이용하여 운전자 졸음을 감지합니다 10.
신약 개발: 신약 설계, 약물 표적 검증 등 다양한 단계에 활용됩니다 10.
도시 경관 영향 분석: 도시 경관이 정신 상태에 미치는 영향을 연구합니다 10.
해충 관리: 해충 개체수를 예측하여 최적의 방제 전략을 개발하는 데 도움을 줍니다 28.
진로 지도: 청소년의 컴퓨터 게임 수행 능력을 기반으로 실시간 진로 추천을 제공합니다 28.
정형외과: 개방형 경골 근위 절골술 후 무릎의 관상면 정렬을 예측하여 수술 계획 및 분석을 지원합니다   
해양 에너지 관리: 동적 해양 환경에서 선박 연료 소비를 정확하게 예측하기 위한 증분 학습 기반 예측 프레임워크의 벤치마크로 사용됩니다 28.
전력 소비 예측: 수요 변화가 있는 지역에서 전력 사용 예측의 정확도를 향상시키기 위해 퍼지 클러스터링과 통합됩니다 28.
의료 영상 분석: 뇌 MRI를 기반으로 성인 간경변 환자의 만성 간성 뇌증 진단 및 중증도를 예측하기 위해 기계 학습 기반 방사선 모델을 구현합니다 28.
주식 시장 추세 예측: 발견적 가능도 추정으로 향상된 베이지안 분류기를 사용하여 주가 추세를 예측하고 다층 퍼셉트론과 성능을 비교합니다 28.
수력 발전소 유출량 예측: 수력 발전소 유역의 일일 유출량 예측 정확도를 높이기 위해 개별 모델 및 투표 앙상블 모델 내에서 다층 퍼셉트론의 성능을 평가합니다 28.
종양학: 악성 대장암 환자의 6개월 생존 예측에서 다른 기계 학습 모델과 함께 다층 퍼셉트론의 성능을 평가하여 임상 결정을 최적화합니다 28.
고객 수요 예측: 고객 행동 정보를 통합하여 보다 정확한 수요 예측을 위해 LSTM 기반 모델을 개발하고 다층 퍼셉트론과 성능을 비교합니다 28.
풍력 하중 예측: 다양한 컨테이너 구성으로 컨테이너선에 작용하는 풍력 하중을 예측하기 위해 다층 퍼셉트론과 같은 기계 학습 모델을 사용하여 선박의 안정성, 조종성 및 연료 효율성을 향상시킵니다 28.
광전력 예측: 중장기 광전력 예측의 정확도를 높이기 위해 대화형 특징 추세 변환기(IFTformer)와 같은 다른 심층 학습 모델과 다층 퍼셉트론을 통합합니다 28.
잔여 유효 수명 예측: 터보팬 엔진의 잔여 유효 수명을 예측하기 위해 CNN-GRU와 같은 다른 모델과 다층 퍼셉트론을 비교하여 예측 유지 관리 전략에 기여합니다 28.
사면 안정성 예측: 문헌 데이터를 학습하여 사면 안정성의 효율적이고 정확한 예측을 위해 다층 퍼셉트론(MLP)을 통합한 증분 학습 베이지안 모델을 개발합니다 28.
임신성 당뇨병(GDM) 조기 진단: 초기 임신에서 잠재적 위험 요인을 식별하기 위해 NearMiss와 같은 기술을 사용하여 신경망을 통해 임신성 당뇨병(GDM)에 대한 조기 위험 예측 모델을 만듭니다 28.
자폐 스펙트럼 장애 분석: BrainAGE 연구 및 개별 가중 다층 퍼셉트론 네트워크(ILWMLP) 회귀를 사용하여 자폐 스펙트럼 장애가 있는 개인의 감각 운동 관련 뇌 네트워크에서 비정형 발달 패턴을 식별합니다 28.
정보 확산 예측: 온라인 소셜 네트워크에서 보다 정확한 예측을 위해 서브그래프 샘플링 및 다중 헤드 어텐션 메커니즘을 사용하여 시간적 및 구조적 정보를 융합합니다 28.
| 응용 분야 | 구체적인 예시 ||---|---|| 이미지 인식 | 객체 감지, 얼굴 인식, 숫자 분류 || 자연어 처리 | 감성 분석, 기계 번역 || 시계열 예측 | 주가 예측, 날씨 예측 || 의료 진단 | 의료 영상 분석, 질병 진단 || 자율 주행 | 객체 감지, 경로 조정 |6. MLP 사용의 장점과 한계MLP는 여러 가지 중요한 장점을 제공합니다. 비선형 함수를 근사하고 데이터 내의 복잡한 관계를 모델링할 수 있는 능력은 MLP의 핵심적인 장점 중 하나입니다 1. 은닉층을 통해 원시 입력 데이터로부터 계층적인 특징 표현을 자동으로 학습할 수 있다는 점 또한 MLP의 중요한 강점입니다 12. 또한 MLP는 다양한 분야와 데이터 유형(수치형, 범주형, 결측치 처리 가능)에 효과적으로 적용될 수 있는 다재다능한 모델입니다 12. 하드웨어 발전과 최적화 기술 덕분에 대규모 데이터셋과 복잡한 구조를 처리할 수 있는 확장성도 갖추고 있습니다 12. 학습 후에는 빠른 예측이 가능하며 6, 비교적 작은 데이터셋으로도 준수한 정확도를 달성할 수 있습니다 7. 온라인 학습 능력은 데이터를 점진적으로 학습할 수 있도록 합니다 7.하지만 MLP는 몇 가지 중요한 한계점도 가지고 있습니다. 작은 데이터셋이나 과도한 모델 복잡성의 경우 과적합이 발생하기 쉽습니다 6. 깊은 MLP의 경우 학습 과정에서 기울기 소실 또는 폭주 문제가 발생하여 학습이 어려워질 수 있습니다 8. 학습률, 네트워크 구조 등 하이퍼파라미터에 민감하게 반응하므로 세심한 튜닝이 필요합니다 12. 특히 깊은 MLP를 학습시키는 데는 상당한 계산 자원과 시간이 소요될 수 있습니다 6. 또한 MLP는 종종 예측의 근거를 명확하게 설명하기 어려운 "블랙 박스" 모델로 간주됩니다 25. 완전 연결 구조는 파라미터 수 증가와 노드 중복의 가능성을 야기할 수 있으며 6, 모델 성능은 완벽한 학습에 크게 의존합니다 6. 학습 시간이 길어질 수 있으며 25, 독립 변수가 종속 변수에 미치는 영향의 크기에 대한 내재적인 지식이 부족할 수 있습니다 31. MLP의 표현력(비선형성 모델링 및 특징 학습 능력)과 과적합 가능성 사이의 균형은 적절한 규제 기술 및 하이퍼파라미터 튜닝의 중요성을 강조합니다 12.7. 현대 신경망 환경에서의 MLP: CNN 및 RNN과의 비교MLP는 완전 연결된 층으로 구성된 반면, 합성곱 신경망(CNN)은 지역 연결성 및 가중치 공유를 사용하는 합성곱 층을 활용하여 이미지와 같은 공간 데이터에 더 효율적입니다 1. CNN은 이미지 내의 공간적 관계를 해석하는 데 더 뛰어납니다 3. MLP는 벡터 입력을 받는 반면 CNN은 텐서 입력을 처리할 수 있습니다 3. MLP는 보다 일반적인 용도로 사용되는 반면 CNN은 공간 데이터를 포함하는 작업에 특화되어 있습니다 3. 이미지 작업의 경우 MLP는 CNN에 비해 더 많은 수의 파라미터를 가집니다 3.순환 신경망(RNN)은 순환 구조를 통해 시퀀스 데이터를 처리하는 데 특화된 반면, MLP는 순환이 없는 피드포워드 네트워크입니다 1. RNN은 시계열 데이터나 자연어와 같이 데이터의 순서가 중요한 작업에 적합합니다 1. MLP는 데이터를 평탄화하여 시퀀스 데이터에 적용할 수 있지만, 일반적으로 RNN이 시간적 의존성을 포착하는 데 더 효과적입니다 20. MLP는 다양한 작업에 사용될 수 있지만, CNN 및 RNN과 같은 특화된 아키텍처의 개발은 데이터의 특정 특성(공간적 대 순차적)에 맞게 네트워크 설계를 조정하는 것이 성능 및 효율성 향상에 중요하다는 것을 보여줍니다 3.8. 다층 퍼셉트론 연구의 최근 동향 및 발전MLP는 의학, 공학, 사회 과학 등 다양한 분야에서 지속적으로 연구되고 응용되고 있습니다 10. MLP는 종종 CNN, RNN과 같은 다른 신경망 유형과 결합되어 더욱 강력한 모델을 만드는 데 사용됩니다 2. 학습 알고리즘을 최적화하고 기울기 소실/폭주와 같은 문제를 해결하는 데 대한 연구가 계속되고 있습니다 8. MLP 프레임워크 내에서 새로운 층 유형 및 연결 전략(예: MLP-Mixer) 개발에 대한 탐구가 이루어지고 있습니다 2. 정보 확산 예측, 신경 발달 장애 분석, 사면 안정성 예측 및 의료 진단과 같은 특정 문제 영역에 MLP를 적용하는 데 중점을 둔 연구가 진행 중입니다 10. 성능 향상을 위해 MLP를 다른 기계 학습 또는 통계적 방법과 결합하는 연구도 활발합니다 10. GPU와 같은 특정 하드웨어에 대한 MLP 구현을 최적화하는 연구도 진행되고 있습니다 33. 보다 전문화된 딥러닝 아키텍처의 등장에도 불구하고 MLP는 여전히 관련성이 높고 활발한 연구 분야이며, 성능, 효율성 및 광범위한 복잡한 문제에 대한 적용 가능성을 향상시키는 데 중점을 둔 발전이 이루어지고 있으며, 종종 다른 고급 기술과 함께 사용됩니다 10.9. 결론: MLP의 지속적인 중요성본 보고서는 다층 퍼셉트론(MLP)의 주요 측면을 상세히 살펴보았습니다. MLP는 인공 신경망 및 딥러닝 분야의 기초적인 개념으로서, 비선형 데이터 모델링 능력과 다양한 응용 가능성을 통해 그 중요성을 입증해 왔습니다. 이미지 인식, 자연어 처리, 시계열 예측 등 다양한 분야에서 핵심적인 역할을 수행하며, 현재까지도 활발한 연구가 진행되고 있습니다. MLP는 단독 모델로서뿐만 아니라, 합성곱 신경망(CNN) 및 순환 신경망(RNN)과 같은 다른 고급 신경망 아키텍처와 결합되어 더욱 강력한 인공 지능 시스템을 구축하는 데 기여하고 있습니다. 이러한 지속적인 연구와 발전은 MLP가 인공 지능 분야에서 변함없는 영향력을 발휘할 것임을 시사합니다.

### LSTM  
Long Short Term Memory, [참고_링크](https://ctkim.tistory.com/entry/LSTMLong-short-time-memory-%EA%B8%B0%EC%B4%88-%EC%9D%B4%ED%95%B4)  

RNN의 한계(Input의 데이터 길이의 한계, [Gradient_Vanishing_Problem](#gradient-vanishing-problem)) 줄이기 위해서 제작된 모델

<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fik.imagekit.io%2Fbotpenguin1%2Fassets%2Fwebsite%2FSigmoid_Function_90ec70976d.png&f=1&nofb=1&ipt=7af621efafe2b4c6327f48959bacad3638cabe79de8dd63511708887aeb1851b&ipo=images"
       alt="Sigmoid_function" width="220" />
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fsaugatbhattarai.com.np%2Fwp-content%2Fuploads%2F2018%2F06%2Ftanh.jpg&f=1&nofb=1&ipt=f78b7c9ba52c61c53ab97f72f83a38c26a7c73ee5e466511bd49bec27491f39f&ipo=images"
       alt="tanh_function" width="220" />
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.nomidl.com%2Fwp-content%2Fuploads%2F2022%2F04%2Fimage-17.png&f=1&nofb=1&ipt=a08af181b4dc86b5fd058670a595ff38bfc5dfe67f0d02c25df56bbae640a01c&ipo=images"
       alt="sig_tan" width="220" />
</div>

### HR Net

### ViT
ViT는 Vision Transformer의 약자로, 이미지 인식 분야에서 Transformer 구조를 적용한 모델  
Attention is All You Need 선행 필수 (트랜스포머 모델, 수학적, 개념적 이해 요구 됨)
Transformer는 원래 자연어 처리(NLP) 분야에서 주로 사용되었지만, ViT는 이를 이미지 인식에 효과적으로 적용하여 뛰어난 성능을 보여줍니다.

ViT의 핵심 아이디어는 이미지를 작은 조각인 '패치(patch)'로 나누고, 각 패치를 Transformer 모델의 입력으로 사용하는 것입니다.  
각 패치는 마치 자연어 처리에서의 단어처럼 취급되며, Transformer의 self-attention 메커니즘을 통해 이미지 내의 패치들 간의 관계를 학습합니다.  
이를 통해 ViT는 이미지 전체의 맥락을 효과적으로 파악하고, 이미지 분류, 객체 감지 등 다양한 비전 task에서 높은 성능을 달성할 수 있습니다.


### GCN
Graph Convolutional Network  
논문 : Semi-Supervised Classification with Graph Convolutional Networks [링크](https://arxiv.org/abs/1609.02907)  

GCN을 공부하게 된 이유 :  
- Pedestrian Trajectory Prediction에서는 "Social-STGCNN"이라는 모델을, 그리고 
- Human Motion Prediction에서 "GCNext 사용하는데,  
이 모델들의 베이스는 GCN을 사용하고 있고, GCN을 언급하기 때문에 GCN을 공부하게 되었다.  
  
GCN에 대한 논문들이 많지만 대표적인 논문이었다.  

#### **논문 제목 Semi-Supervised Classification with Graph Convolutional Networks**

**Semi-supervised Learning 이란**  
- label이 있는 데이터와 label이 없는 데이터가 섞여 있는 경우, 함께 사용하는 학습 방법  
이유 : 데이터 레이블링 비용, 라벨 부족 문제 해결  
label 데이터로 초기 패턴 학습 후, 데이터 유사성, 구조적 관계 바탕으로 라벨 없는 데이터 예측 및 라벨 전파(label propagation)  

**GCN이란**  
그래프 구조 데이터 처리 위해 설계된 신경망, {Grid 형태의 데이터(이미지 등), 시계열 데이터(센서 등)}아닌, 

### PINN
Physics-Informed Neural Networks  
물리학적 지식을 활용한 신경망 모델  
