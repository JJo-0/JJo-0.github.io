---
title: "Vision 공부"
date: 2025-02-18 14:18:00
categories:
  - 정리
  - 개발
  - Project
tags:
  - linux
katex: true
toc: true
toc_sticky: true
toc_label: "페이지 주요 목차"
---

## 주제

- 모르는 Vision, AI 관련 용어들 개념부터 정리 없이 올릴 생각이다.  

---

## 고른 것

### 목차

|목차|내용|비고|링크|
|:--:|:--|:--|:--|
|**Transformer**|||<a href="#transformer">바로가기</a>|
|**RNN**|||<a href="#rnn">바로가기</a>|
|**LSTM**|||<a href="#lstm">바로가기</a>|
|**ViT**|||<a href="#vit">바로가기</a>|
|**HR Net**|||<a href="#hr-net">바로가기</a>|
|**GCN**|||<a href="#gcn">바로가기</a>|

---

교수님께 들은 조언, Foundation 모델, ViT는 기본적으로 리소스가 많이 들어간다.  



### Transformer
Attention is All You Need  

SoftMax는 수식의 아래와 같다. 

$$ softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} $$

### RNN 
Recurrent Neural Network

#### Gradient Vanishing Problem (기울기 소실 문제)
[참고_링크](https://www.youtube.com/watch?v=aWInMRtySpQ)
Deep Nerual Network를 제작할 때, hidden layer(은닉 층)을 많이 쌓는다.  
은닉층 구조와 수는 모델의 성능과 복잡도에 큰 영향력을 미쳐서(좋은 모델일 수록 파라미터 가중치(weight, bias)가 많다)  
  
**가중치(weight)**  
- 입력 데이터에 대한 중요도를 결정하는 값  
- 신경망의 각 연결에 곱해지는 값  

**편향(bias)**  
- 특정 노드가 더 쉽게 활성화 되도록 하는 값  
- 가중치와 함께 신경망의 출력을 조정하는 역할  

**역전파**  
- 출력과 실제 값(정답) 사이의 오차(Error) 계산  
- 오차를 줄이기 위해 가중치와 편향을 조정  

**경사 하강법**  
- 오차를 줄이기 위해 가중치와 편향을 조정하는 방법  
- 학습률(learning rate)에 따라 업데이트 크기, 속도 결정  

$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$

여기서, $begin:math:text$w_i$end:math:text$는 가중치(weight), $begin:math:text$x_i$end:math:text$는 입력 데이터, $begin:math:text$b$end:math:text$는 편향(bias)을 의미한다.  


은닉층이 많아지면, 기울기 소실 문제가 발생한다.  
기울기 소실의 문제점은 활성화 함수의 기울기와 관련이 있다.  
역전파를 이용해 

**Sigmoid 함수**  
Sigmoid 함수를 보면 미분할 경우 sigmoid는 0~0.25 1/4 4분의 1이 난다는 것  
![Sigmoid_Derivative](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FyJnMy%2FbtrCKh4ojvw%2FLtlaMwrtEZHC3KtGNWvuLK%2Fimg.png)  

**tanh 함수**  
tanh 함수를 보면 미분할 경우 tanh는 0~1 1/2 2분의 1이 난다는 것
![Tanh_Derivative](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fcq4ZGq%2FbtrCJJNA2bI%2FZhOpBc6Rj3061ZMw11AfPK%2Fimg.png)  

##### 활성화 함수
- Sigmoid
- tanh
기울기 소실의 

### LSTM  
Long Short Term Memory, [참고_링크](https://ctkim.tistory.com/entry/LSTMLong-short-time-memory-%EA%B8%B0%EC%B4%88-%EC%9D%B4%ED%95%B4)  

RNN의 한계(Input의 데이터 길이의 한계, [Gradient_Vanishing_Problem](#gradient-vanishing-problem)) 줄이기 위해서 제작된 모델

<div style="display: flex; justify-content: center; gap: 10px;">
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fik.imagekit.io%2Fbotpenguin1%2Fassets%2Fwebsite%2FSigmoid_Function_90ec70976d.png&f=1&nofb=1&ipt=7af621efafe2b4c6327f48959bacad3638cabe79de8dd63511708887aeb1851b&ipo=images"
       alt="Sigmoid_function" width="220" />
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fsaugatbhattarai.com.np%2Fwp-content%2Fuploads%2F2018%2F06%2Ftanh.jpg&f=1&nofb=1&ipt=f78b7c9ba52c61c53ab97f72f83a38c26a7c73ee5e466511bd49bec27491f39f&ipo=images"
       alt="tanh_function" width="220" />
  <img src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.nomidl.com%2Fwp-content%2Fuploads%2F2022%2F04%2Fimage-17.png&f=1&nofb=1&ipt=a08af181b4dc86b5fd058670a595ff38bfc5dfe67f0d02c25df56bbae640a01c&ipo=images"
       alt="sig_tan" width="220" />
</div>

### HR Net

### ViT
ViT는 Vision Transformer의 약자로, 이미지 인식 분야에서 Transformer 구조를 적용한 모델  
Attention is All You Need 선행 필수 (트랜스포머 모델, 수학적, 개념적 이해 요구 됨)
Transformer는 원래 자연어 처리(NLP) 분야에서 주로 사용되었지만, ViT는 이를 이미지 인식에 효과적으로 적용하여 뛰어난 성능을 보여줍니다.

ViT의 핵심 아이디어는 이미지를 작은 조각인 '패치(patch)'로 나누고, 각 패치를 Transformer 모델의 입력으로 사용하는 것입니다.  
각 패치는 마치 자연어 처리에서의 단어처럼 취급되며, Transformer의 self-attention 메커니즘을 통해 이미지 내의 패치들 간의 관계를 학습합니다.  
이를 통해 ViT는 이미지 전체의 맥락을 효과적으로 파악하고, 이미지 분류, 객체 감지 등 다양한 비전 task에서 높은 성능을 달성할 수 있습니다.


### GCN
Graph Convolutional Network  
논문 : Semi-Supervised Classification with Graph Convolutional Networks [링크](https://arxiv.org/abs/1609.02907)  

GCN을 공부하게 된 이유 :  
- Pedestrian Trajectory Prediction에서는 "Social-STGCNN"이라는 모델을, 그리고 
- Human Motion Prediction에서 "GCNext 사용하는데,  
이 모델들의 베이스는 GCN을 사용하고 있고, GCN을 언급하기 때문에 GCN을 공부하게 되었다.  
  
GCN에 대한 논문들이 많지만 대표적인 논문이었다.  

#### **논문 제목 Semi-Supervised Classification with Graph Convolutional Networks**

**Semi-supervised Learning 이란**  
- label이 있는 데이터와 label이 없는 데이터가 섞여 있는 경우, 함께 사용하는 학습 방법  
이유 : 데이터 레이블링 비용, 라벨 부족 문제 해결  
label 데이터로 초기 패턴 학습 후, 데이터 유사성, 구조적 관계 바탕으로 라벨 없는 데이터 예측 및 라벨 전파(label propagation)  

**GCN이란**  
그래프 구조 데이터 처리 위해 설계된 신경망, {Grid 형태의 데이터(이미지 등), 시계열 데이터(센서 등)}아닌, 

### PINN
Physics-Informed Neural Networks  
물리학적 지식을 활용한 신경망 모델  
