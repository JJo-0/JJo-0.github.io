---
title: "SLAM (1)"
date: 2022-07-06 11:07:00
categories:
  - 정리
  - 메카트로닉스
tags:
  - 졸업작품
  - ROS2
  - SLAM
---

복습 겸 SLAM의 흐름에 대해서 담은 게시글이다.

# SLAM이란? 
---
SLAM이란 **Simultaneous Localization and Mapping** 의 약자로, 동시에 로봇의 위치와 지도를 만드는 것을 이야기한다. 
1. 움직이는 로봇이 자신의 위치를 알고 있으면 지도를 만들 수 있고, 
2. 지도를 만들면 자신의 위치를 알 수 있기 때문에 
이 둘을 동시에 해결하는 것이 SLAM이다.

# SLAM의 흐름
---
SLAM의 흐름은 크게 3가지로 나눌 수 있다.
1. 센서 데이터를 받아서
2. 로봇의 위치를 추정하고
3. 지도를 만든다.

## 센서데이터
기존적으로 SLAM의 데이터는 Lidar, Camera, IMU, 모터의 encoder 등이 있다. 

주변 환경을 인식하는 센서 : Lidar, Camera 등..
로봇의 움직임을 인식하는 센서 : IMU, 모터의 encoder 등..


### 🌱환경 인식 센서
---
**Lidar의 종류는 크게 2가지로 나눌 수 있다.**
1. 2D Lidar
2. 3D Lidar

**Camera의 종류**
1. Monocular Camera (단안)
2. Stereo Camera (쌍안)
3. Depth 측정이 가능한 카메라  
ToF[^1] , Structured Light[^2] 등...

### 🤖로봇의 움직임 인식 센서
---
**IMU란** 
Inertial Measurement Unit의 약자로, 가속도 센서와 자이로스코프 센서를 합친 것이다. 도움링크 : [IMU란?](https://velog.io/@717lumos/Sensor-IMU의-개념-및-활용법)
1. 3축 가속도 센서
2. 3축 자이로스코프(각속도 센서)
3. 3축 지자기 센서(일부 IMU에만 존재)

## 로봇의 위치 추정
**받은 데이터를 기반으로 로봇의 위치를 추정한다.**
원리 도움링크 : [Matlab_Slam_설명](https://www.youtube.com/watch?v=Fw8JQ5Q-ZwU)  
원리에 대해서 쉽게 잘 설명 되어 있다.

**센서데이터 처리**
받은 센서데이터들은 오류들이 많아서 데이터 처리과정을 꼭 해야한다. 주로 (1)노이즈 제거, (2)데이터 정합, (3)데이터 보간 등의 과정을 거친다.  
센서 데이터 처리는 센서마다 다르기 때문에, 센서 데이터 처리에 대해서는 따로 정리하도록 한다.(vision, lidar, imu 등..)

**로봇의 위치 추정**
로봇의 위치를 추정하는 방법은 여러 위치 추정 알고리즘에 의해 실행된다.  

1. EKF (Extended Kalman Filter)
2. UKF (Unscented Kalman Filter)
3. PF (Particle Filter)
4. 등등.. 

다음 게시물에서 설명 예정  

---
[^1] : Time of Flight, 광학적인 방법으로 거리를 측정하는 방법이다. 광원에서 광선을 쏘고, 물체에 반사되어 돌아오는 시간을 측정하여 거리를 측정한다.  
[^2] : 구조광 방식은 빛을 투사하여 물체의 형태를 파악하는 방식이다. 빛을 투사하여 물체에 반사되어 돌아오는 빛의 패턴을 통해 물체의 형태, 거리를 파악, 측정한다. ![image](https://bitfab.io/wp-content/uploads/2020/03/luz-estructurada.png) 
