---
author_profile: true
categories:
- Resources
- Study-Notes
excerpt: <div id="study-section">...
last_modified_at: 2024-03-17
layout: single
tag:
- study-notes
- vocabulary
- technical-terms
- computer-vision
- artificial-intelligence
title: Essential Vocabulary
toc: true
toc_label: ëª©ì°¨
---

<div id="study-section">

</div>

<div id="quiz-section" style="display: none;">
  <h2>Computer Vision ìš©ì–´ í€´ì¦ˆ</h2>
  <div id="quiz-container">
    <div id="quiz-progress">ë¬¸ì œ <span id="current-question">1</span> / <span id="total-questions">10</span></div>
    <div id="question-container"></div>
    <div id="options-container"></div>
    <div id="result-message" style="display: none;"></div>
    <button id="next-btn" style="display: none;">ë‹¤ìŒ ë¬¸ì œ</button>
    <div id="final-score" style="display: none;"></div>
  </div>
</div>

<div id="control-buttons" style="margin-top: 20px;">
  <button id="start-quiz" onclick="startQuiz()">ì‹œí—˜ ì‹œì‘í•˜ê¸°</button>
  <button id="return-study" onclick="returnToStudy()" style="display: none;">í•™ìŠµ ëª¨ë“œë¡œ ëŒì•„ê°€ê¸°</button>
</div>

<style>
#quiz-section {
  background: #f8f9fa;
  padding: 20px;
  border-radius: 8px;
  margin-top: 20px;
}

#quiz-container {
  max-width: 800px;
  margin: 0 auto;
}

#question-container {
  font-size: 1.2em;
  margin: 20px 0;
  padding: 15px;
  background: white;
  border-radius: 5px;
  box-shadow: 0 2px 5px rgba(0,0,0,0.1);
}

.option {
  padding: 10px 15px;
  margin: 10px 0;
  background: white;
  border: 1px solid #ddd;
  border-radius: 5px;
  cursor: pointer;
  transition: all 0.3s ease;
}

.option:hover {
  background: #e9ecef;
}

.option.correct {
  background: #d4edda;
  border-color: #c3e6cb;
}

.option.incorrect {
  background: #f8d7da;
  border-color: #f5c6cb;
}

button {
  padding: 10px 20px;
  margin: 10px 5px;
  border: none;
  border-radius: 5px;
  background: #007bff;
  color: white;
  cursor: pointer;
  transition: all 0.3s ease;
}

button:hover {
  background: #0056b3;
}

#quiz-progress {
  text-align: right;
  margin-bottom: 20px;
  color: #6c757d;
}

#result-message {
  padding: 15px;
  margin: 10px 0;
  border-radius: 5px;
  text-align: center;
}

#final-score {
  text-align: center;
  font-size: 1.2em;
  margin: 20px 0;
  padding: 20px;
  background: #e9ecef;
  border-radius: 5px;
}
</style>

<script>
const vocabularyData = [
  { word: 'Perceive', type: 'ë™ì‚¬', meaning: 'ì¸ì§€í•˜ë‹¤, ê°ì§€í•˜ë‹¤', example: 'Computers can perceive subtle differences in images that humans might miss.' },
  { word: 'Perceptual', type: 'í˜•ìš©ì‚¬', meaning: 'ì§€ê°ì˜, ì¸ì§€ì˜', example: 'Perceptual computing aims to mimic human sensory processing.' },
  { word: 'Misperception', type: 'ëª…ì‚¬', meaning: 'ì˜¤ì¸, ì˜ëª»ëœ ì¸ì‹', example: 'Image artifacts can lead to misperception in computer vision systems.' },
  { word: 'Translucency', type: 'ëª…ì‚¬', meaning: 'ë°˜íˆ¬ëª…ì„±', example: 'Processing translucency in images requires advanced rendering techniques.' },
  { word: 'Subtle', type: 'í˜•ìš©ì‚¬', meaning: 'ë¯¸ë¬˜í•œ, ì„¬ì„¸í•œ', example: 'The algorithm can detect subtle changes in lighting conditions.' },
  { word: 'Shading', type: 'ëª…ì‚¬', meaning: 'ìŒì˜', example: 'Proper shading analysis helps in understanding 3D structures.' },
  { word: 'Effortlessly', type: 'ë¶€ì‚¬', meaning: 'ìˆ˜ì›”í•˜ê²Œ, ì–´ë ¤ì›€ ì—†ì´', example: 'Modern AI systems effortlessly process thousands of images per second.' },
  { word: 'Portrait', type: 'ëª…ì‚¬', meaning: 'ì´ˆìƒ, ì¸ë¬¼ ì‚¬ì§„', example: 'Face detection algorithms are crucial for portrait photography.' },
  { word: 'Rapid', type: 'í˜•ìš©ì‚¬', meaning: 'ë¹ ë¥¸, ì‹ ì†í•œ', example: 'Rapid image processing is essential for real-time applications.' },
  { word: 'Delineate', type: 'ë™ì‚¬', meaning: 'ìœ¤ê³½ì„ ê·¸ë¦¬ë‹¤, ë¬˜ì‚¬í•˜ë‹¤', example: 'The algorithm can delineate object boundaries precisely.' },
  { word: 'Causality', type: 'ëª…ì‚¬', meaning: 'ì¸ê³¼ê´€ê³„', example: 'Understanding causality in scene analysis improves prediction accuracy.' },
  { word: 'Radiometry', type: 'ëª…ì‚¬', meaning: 'ë°©ì‚¬ì¸¡ì •', example: 'Radiometry is fundamental to understanding light behavior in images.' },
  { word: 'Inspection', type: 'ëª…ì‚¬', meaning: 'ê²€ì‚¬, ì ê²€', example: 'Automated visual inspection systems in manufacturing.' },
  { word: 'Retail', type: 'ëª…ì‚¬', meaning: 'ì†Œë§¤, ìœ í†µ', example: 'Computer vision enhances retail analytics and customer behavior tracking.' },
  { word: 'Warehouse', type: 'ëª…ì‚¬', meaning: 'ì°½ê³ ', example: 'Automated warehouse systems use computer vision for inventory management.' },
  { word: 'Logistics', type: 'ëª…ì‚¬', meaning: 'ë¬¼ë¥˜', example: 'Computer vision optimizes logistics operations through package tracking.' },
  { word: 'Intra-operative', type: 'í˜•ìš©ì‚¬', meaning: 'ìˆ˜ìˆ  ì¤‘ì˜', example: 'Intra-operative imaging assists surgeons during procedures.' },
  { word: 'Morphology', type: 'ëª…ì‚¬', meaning: 'í˜•íƒœí•™', example: 'Mathematical morphology is used in image processing.' },
  { word: 'Surveillance', type: 'ëª…ì‚¬', meaning: 'ê°ì‹œ, ë³´ì•ˆ', example: 'Video surveillance systems employ advanced computer vision techniques.' },
  { word: 'Stitching', type: 'ëª…ì‚¬', meaning: 'ì´ë¯¸ì§€ ì ‘í•©', example: 'Panorama creation through image stitching.' },
  { word: 'Overlapping', type: 'í˜•ìš©ì‚¬', meaning: 'ì¤‘ì²©ë˜ëŠ”', example: 'Overlapping images are used in 3D reconstruction.' },
  { word: 'Bracketing', type: 'ëª…ì‚¬', meaning: 'ë¸Œë¼ì¼€íŒ…(ë…¸ì¶œ ë‹¨ê³„ ì¡°ì ˆ)', example: 'HDR images are created using exposure bracketing.' },
  { word: 'Morphing', type: 'ëª…ì‚¬', meaning: 'ë³€í˜•', example: 'Face morphing creates smooth transitions between images.' },
  { word: 'Meticulously', type: 'ë¶€ì‚¬', meaning: 'ê¼¼ê¼¼í•˜ê²Œ, ì •êµí•˜ê²Œ', example: 'AI models must be meticulously trained for optimal performance.' },
  { word: 'Layer-wise, sample-wise', type: 'í˜•ìš©ì‚¬', meaning: 'ì¸µë³„, ìƒ˜í”Œë³„', example: 'Neural networks are analyzed layer-wise for better understanding.' },
  { word: 'Preexisting', type: 'í˜•ìš©ì‚¬', meaning: 'ê¸°ì¡´ì˜, ì´ë¯¸ ì¡´ì¬í•˜ëŠ”', example: 'Preexisting models can be fine-tuned for specific tasks.' },
  { word: 'Plausible', type: 'í˜•ìš©ì‚¬', meaning: 'ê·¸ëŸ´ë“¯í•œ, íƒ€ë‹¹í•œ', example: 'The system generates plausible reconstructions of 3D scenes.' },
  { word: 'Pose Forecasting', type: 'ëª…ì‚¬êµ¬', meaning: 'ìì„¸ ì˜ˆì¸¡', example: 'Pose forecasting is crucial for safe human-robot interaction.' },
  { word: 'Ablation', type: 'ëª…ì‚¬', meaning: '(ì‹ ê²½ë§ ë“±ì—ì„œ) ì¼ë¶€ ì œê±°, ì ˆì œ', example: 'Ablation studies help understand the contribution of different model components.' },
  { word: 'Endow', type: 'ë™ì‚¬', meaning: 'ë¶€ì—¬í•˜ë‹¤, ì£¼ë‹¤', example: 'The new sensor endows the robot with better perceptive capabilities.' },
  { word: 'Perceptive', type: 'í˜•ìš©ì‚¬', meaning: 'ì§€ê°í•˜ëŠ”, í†µì°°ë ¥ ìˆëŠ”', example: 'Perceptive robots can adapt to dynamic environments.' },
  { word: 'Harm', type: 'ëª…ì‚¬/ë™ì‚¬', meaning: 'í•´, ì†ìƒ / í•´ë¥¼ ë¼ì¹˜ë‹¤', example: 'The primary goal is to prevent any harm to the human worker.' },
  { word: 'Transmission of contact wrenches', type: 'ëª…ì‚¬êµ¬', meaning: 'ì ‘ì´‰ë ¥ ì „ë‹¬', example: 'Accurate transmission of contact wrenches is vital for haptic feedback.' },
  { word: 'On-the-fly', type: 'ë¶€ì‚¬êµ¬', meaning: 'ì¦‰ì„ì—ì„œ, ì‹¤ì‹œê°„ìœ¼ë¡œ', example: 'The robot can adjust its path on-the-fly to avoid obstacles.' },
  { word: 'Adjacency', type: 'ëª…ì‚¬', meaning: 'ì¸ì ‘, ê·¼ì ‘', example: 'Adjacency matrices are used to represent connections in a graph.' },
  { word: 'Sparse', type: 'í˜•ìš©ì‚¬', meaning: 'í¬ì†Œí•œ, ë“œë¬¸ë“œë¬¸ ìˆëŠ”', example: 'Sparse data can be challenging for some machine learning models.' },
  { word: 'Sparsity', type: 'ëª…ì‚¬', meaning: 'í¬ì†Œì„±', example: 'Sparsity is a desirable property in many high-dimensional datasets.' },
  { word: 'Abide', type: 'ë™ì‚¬', meaning: '(ê·œì¹™ ë“±ì„) ì¤€ìˆ˜í•˜ë‹¤, ì§€í‚¤ë‹¤', example: 'Robots must abide by safety protocols.' },
  { word: 'Genuine', type: 'í˜•ìš©ì‚¬', meaning: 'ì§„ì§œì˜, ì§„ì •í•œ', example: 'The system aims to achieve genuine collaboration between humans and robots.' },
  { word: 'Foresee', type: 'ë™ì‚¬', meaning: 'ì˜ˆê²¬í•˜ë‹¤, ì˜ˆì¸¡í•˜ë‹¤', example: 'It is difficult to foresee all possible scenarios in a complex environment.' },
  { word: 'Exploited', type: 'ë™ì‚¬, ìˆ˜ë™íƒœ ë˜ëŠ” ê³¼ê±°í˜•', meaning: 'í™œìš©ëœ, ì´ìš©ëœ', example: "The robot's capabilities were fully exploited in the task." },
  { word: 'Aspect', type: 'ëª…ì‚¬', meaning: 'ì¸¡ë©´, ì–‘ìƒ', example: 'Safety is a critical aspect of human-robot collaboration.' },
  { word: 'Intersection', type: 'ëª…ì‚¬', meaning: 'êµì°¨ì , êµì§‘í•©', example: 'The intersection of AI and robotics has led to many innovations.' },
  { word: 'Prune', type: 'ë™ì‚¬', meaning: '(ê°€ì§€ ë“±ì„) ì¹˜ë‹¤, ì œê±°í•˜ë‹¤, ê°„ê²°í•˜ê²Œ í•˜ë‹¤', example: 'Pruning the neural network can reduce its complexity.' },
  { word: 'Factorize', type: 'ë™ì‚¬', meaning: 'ì¸ìˆ˜ë¶„í•´í•˜ë‹¤, ìš”ì¸ìœ¼ë¡œ ë¶„ì„í•˜ë‹¤', example: 'We can factorize the matrix to understand its underlying structure.' },
  { word: 'Spectral', type: 'í˜•ìš©ì‚¬', meaning: 'ìŠ¤í™íŠ¸ëŸ¼ì˜, ë¶„ê´‘ì˜', example: 'Spectral analysis is used in various sensor technologies.' },
  { word: 'Coarse', type: 'í˜•ìš©ì‚¬', meaning: 'ê±°ì¹œ, ëŒ€ëµì ì¸', example: 'A coarse estimation was made initially, followed by a finer adjustment.' },
  { word: 'Sparsifying', type: 'ë™ì‚¬/í˜•ìš©ì‚¬', meaning: 'í¬ì†Œí™”í•˜ëŠ”', example: 'Sparsifying techniques are used to reduce data dimensionality.' },
  { word: 'Formalize', type: 'ë™ì‚¬', meaning: 'ê³µì‹í™”í•˜ë‹¤, í˜•ì‹í™”í•˜ë‹¤', example: 'We need to formalize the problem statement before designing a solution.' },
  { word: 'Density', type: 'ëª…ì‚¬', meaning: 'ë°€ë„', example: 'Probability density functions describe the likelihood of a continuous variable.' },
  { word: 'PDF (Probability Density Function)', type: 'ëª…ì‚¬', meaning: 'í™•ë¥  ë°€ë„ í•¨ìˆ˜', example: 'The PDF of a normal distribution is bell-shaped.' },
  { word: 'PMF (Probability Mass Function)', type: 'ëª…ì‚¬', meaning: 'í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜', example: 'The PMF is used for discrete random variables.' },
  { word: 'Accomplish', type: 'ë™ì‚¬', meaning: 'ì„±ì·¨í•˜ë‹¤, ë‹¬ì„±í•˜ë‹¤', example: 'The AI model was able to accomplish the task with high accuracy.' },
  { word: 'Observed', type: 'í˜•ìš©ì‚¬/ë™ì‚¬ ê³¼ê±°í˜•', meaning: 'ê´€ì°°ëœ', example: 'The observed data was used to train the model.' },
  { word: 'Intractable', type: 'í˜•ìš©ì‚¬', meaning: 'ë‹¤ë£¨ê¸° í˜ë“ , ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´', example: 'Some problems are computationally intractable.' },
  { word: 'Coefficient', type: 'ëª…ì‚¬', meaning: 'ê³„ìˆ˜', example: 'The coefficients of the linear regression model were estimated.' },
  { word: 'Era', type: 'ëª…ì‚¬', meaning: 'ì‹œëŒ€', example: 'We are in the era of big data and artificial intelligence.' },
  { word: 'Deluge', type: 'ëª…ì‚¬', meaning: 'í­ì£¼, ì‡„ë„, í™ìˆ˜', example: 'Researchers face a deluge of data from various sources.' },
  { word: 'Apparently', type: 'ë¶€ì‚¬', meaning: 'ëª…ë°±íˆ, ë³´ì•„í•˜ë‹ˆ', example: 'Apparently, the new algorithm outperforms the old one.' },
  { word: 'Sufficient', type: 'í˜•ìš©ì‚¬', meaning: 'ì¶©ë¶„í•œ', example: 'Sufficient data is required to train a robust model.' },
  { word: 'Altering', type: 'ë™ì‚¬/í˜•ìš©ì‚¬', meaning: 'ë³€ê²½í•˜ëŠ”', example: 'Altering the hyperparameters can significantly affect model performance.' },
  { word: 'Ability to fix', type: 'êµ¬ë¬¸', meaning: '~ì„ ê³ ì¹˜ëŠ” ëŠ¥ë ¥', example: 'The system has the ability to fix minor errors automatically.' },
  { word: 'Devising', type: 'ë™ì‚¬', meaning: 'ê³ ì•ˆí•˜ë‹¤, ì°½ì•ˆí•˜ë‹¤', example: 'Devising new algorithms is a key part of AI research.' },
  { word: 'Empirical', type: 'í˜•ìš©ì‚¬', meaning: 'ê²½í—˜ì ì¸, ì‹¤ì¦ì ì¸', example: 'Empirical results show the effectiveness of the proposed method.' },
  { word: 'Theorem', type: 'ëª…ì‚¬', meaning: 'ì •ë¦¬ (ìˆ˜í•™, ë…¼ë¦¬í•™)', example: "The Bayes' theorem is fundamental in probability theory." },
  { word: 'Criterion', type: 'ëª…ì‚¬', meaning: 'ê¸°ì¤€, í‘œì¤€ (ë³µìˆ˜í˜•: criteria)', example: 'Accuracy is a common criterion for evaluating classification models.' },
  { word: 'Impose', type: 'ë™ì‚¬', meaning: 'ë¶€ê³¼í•˜ë‹¤, ê°•ìš”í•˜ë‹¤, ë„ì…í•˜ë‹¤', example: 'We can impose constraints on the optimization problem.' },
  { word: 'Regularization', type: 'ëª…ì‚¬', meaning: 'ì •ê·œí™” (ê¸°ê³„ í•™ìŠµ)', example: 'Regularization helps prevent overfitting in machine learning models.' },
  { word: 'Implicitly', type: 'ë¶€ì‚¬', meaning: 'ì•”ë¬µì ìœ¼ë¡œ, í•¨ì¶•ì ìœ¼ë¡œ', example: 'The model implicitly learns features from the data.' },
  { word: 'Explicitly', type: 'ë¶€ì‚¬', meaning: 'ëª…ì‹œì ìœ¼ë¡œ, ëª…ë°±íˆ', example: 'Some parameters need to be explicitly defined.' },
  { word: 'Preferences', type: 'ëª…ì‚¬', meaning: 'ì„ í˜¸ë„', example: 'The recommendation system learns user preferences over time.' },
  { word: 'Validation', type: 'ëª…ì‚¬', meaning: 'ê²€ì¦, í™•ì¸', example: 'Cross-validation is used to assess model performance.' },
  { word: 'Problematic', type: 'í˜•ìš©ì‚¬', meaning: 'ë¬¸ì œê°€ ìˆëŠ”, ë¬¸ì œê°€ ë§ì€', example: 'Handling missing data can be problematic.' },
  { word: 'Associate', type: 'ë™ì‚¬', meaning: 'ì—°ê´€ì‹œí‚¤ë‹¤, ê´€ë ¨ì§“ë‹¤', example: 'The goal is to associate input features with output labels.' },
  { word: 'Evaluations', type: 'ëª…ì‚¬', meaning: 'í‰ê°€', example: 'Multiple evaluations were conducted to ensure robustness.' },
  { word: 'Converge', type: 'ë™ì‚¬', meaning: 'ìˆ˜ë ´í•˜ë‹¤, í•œ ì ì— ëª¨ì´ë‹¤', example: 'The optimization algorithm should converge to a good solution.' },
  { word: 'Convex', type: 'í˜•ìš©ì‚¬', meaning: 'ë³¼ë¡í•œ (ìˆ˜í•™)', example: 'Convex optimization problems are generally easier to solve.' },
  { word: 'Inventing', type: 'ë™ì‚¬', meaning: 'ë°œëª…í•˜ë‹¤, ê³ ì•ˆí•˜ë‹¤', example: 'Inventing new algorithms for efficient image processing.' },
  { word: 'Revising', type: 'ë™ì‚¬', meaning: 'ìˆ˜ì •í•˜ë‹¤, êµì •í•˜ë‹¤', example: 'Revising the methodology section of the research paper.' },
  { word: 'Drafting', type: 'ë™ì‚¬', meaning: 'ì´ˆì•ˆì„ ì‘ì„±í•˜ë‹¤', example: 'Drafting the experimental results section.' },
  { word: 'Vector', type: 'ëª…ì‚¬', meaning: 'í¬ê¸°ì™€ ë°©í–¥ì„ ê°€ì§€ëŠ” ìˆ˜í•™ì  ê°ì²´. ì¼ë°˜ì ìœ¼ë¡œ ìˆ«ì ëª©ë¡ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.', example: 'A vector can represent a point in space or a direction.' },
  { word: 'Matrix', type: 'ëª…ì‚¬', meaning: 'í–‰ê³¼ ì—´ë¡œ ë°°ì—´ëœ ìˆ«ìì˜ ì§ì‚¬ê°í˜• ë°°ì—´.', example: 'A matrix can be used to transform vectors in space.' },
  { word: 'Inner Product (Dot Product)', type: 'ëª…ì‚¬', meaning: 'ë‘ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ ìŠ¤ì¹¼ë¼ ê°’ì„ ì–»ëŠ” ì—°ì‚°. ë²¡í„°ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.', example: 'The dot product of two orthogonal vectors is zero.' },
  { word: 'Outer Product', type: 'ëª…ì‚¬', meaning: 'ë‘ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ í–‰ë ¬ì„ ì–»ëŠ” ì—°ì‚°.', example: 'The outer product of two vectors results in a matrix.' },
  { word: 'Matrix-Vector Product', type: 'ëª…ì‚¬', meaning: 'í–‰ë ¬ê³¼ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ ë²¡í„°ë¥¼ ì–»ëŠ” ì—°ì‚°. ë²¡í„°ë¥¼ í–‰ë ¬ì˜ ì—´ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.', example: 'Matrix-vector multiplication is common in linear algebra.' },
  { word: 'Matrix-Matrix Product', type: 'ëª…ì‚¬', meaning: 'ë‘ í–‰ë ¬ì„ ê³±í•˜ì—¬ í–‰ë ¬ì„ ì–»ëŠ” ì—°ì‚°.', example: 'Matrix-matrix multiplication is used in many algorithms.' },
  { word: 'Identity Matrix', type: 'ëª…ì‚¬', meaning: 'ì£¼ëŒ€ê°ì„  ìš”ì†Œê°€ ëª¨ë‘ 1ì´ê³  ë‚˜ë¨¸ì§€ ìš”ì†ŒëŠ” ëª¨ë‘ 0ì¸ ì •ë°© í–‰ë ¬. í–‰ë ¬ ê³±ì…ˆì—ì„œ í•­ë“±ì› ì—­í• ì„ í•©ë‹ˆë‹¤.', example: 'Multiplying by the identity matrix leaves a matrix unchanged.' },
  { word: 'Diagonal Matrix', type: 'ëª…ì‚¬', meaning: 'ì£¼ëŒ€ê°ì„  ìš”ì†Œ ì™¸ì˜ ëª¨ë“  ìš”ì†Œê°€ 0ì¸ ì •ë°© í–‰ë ¬.', example: 'A diagonal matrix has nonzero entries only on its main diagonal.' },
  { word: 'Transpose', type: 'ëª…ì‚¬', meaning: 'í–‰ë ¬ì˜ í–‰ê³¼ ì—´ì„ ë°”ê¾¼ ì—°ì‚° (A>).', example: 'The transpose of a matrix swaps its rows and columns.' },
  { word: 'Symmetric Matrix', type: 'ëª…ì‚¬', meaning: 'ì „ì¹˜ì™€ ê°™ì€ ì •ë°© í–‰ë ¬ (A = A>).', example: 'A symmetric matrix is equal to its transpose.' },
  { word: 'Trace', type: 'ëª…ì‚¬', meaning: 'ì •ë°© í–‰ë ¬ì˜ ì£¼ëŒ€ê°ì„  ìš”ì†Œë“¤ì˜ í•© (tr(A)).', example: 'The trace of a matrix is the sum of its diagonal elements.' },
  { word: 'Norm', type: 'ëª…ì‚¬', meaning: 'ë²¡í„° ë˜ëŠ” í–‰ë ¬ì˜ "í¬ê¸°"ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜. L1, L2, Lâˆ, Frobenius ë…¸ë¦„ ë“±ì´ ìˆìŠµë‹ˆë‹¤.', example: 'The L2 norm measures the Euclidean length of a vector.' },
  { word: 'Linear Independence', type: 'ëª…ì‚¬', meaning: 'ë²¡í„° ì§‘í•© ë‚´ì˜ ì–´ë–¤ ë²¡í„°ë„ ë‚˜ë¨¸ì§€ ë²¡í„°ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ì—†ëŠ” ì†ì„±.', example: 'Linearly independent vectors cannot be written as combinations of each other.' },
  { word: 'Rank', type: 'ëª…ì‚¬', meaning: 'í–‰ë ¬ì˜ ì„ í˜• ë…ë¦½ì¸ í–‰ ë˜ëŠ” ì—´ì˜ ìµœëŒ€ ê°œìˆ˜. í–‰ë ¬ì˜ ì°¨ì› ê³µê°„ì„ ì–¼ë§ˆë‚˜ ì˜ ì±„ìš°ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.', example: 'The rank of a matrix is the dimension of its column space.' },
  { word: 'Invertible (Non-singular) Matrix', type: 'ëª…ì‚¬', meaning: 'ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ëŠ” ì •ë°© í–‰ë ¬.', example: 'An invertible matrix has a nonzero determinant.' },
  { word: 'Inverse Matrix', type: 'ëª…ì‚¬', meaning: 'í–‰ë ¬ Aì— ëŒ€í•´ ê³±í–ˆì„ ë•Œ í•­ë“± í–‰ë ¬ì„ ë§Œë“œëŠ” ìœ ì¼í•œ í–‰ë ¬ (A^-1).', example: 'The inverse of a matrix undoes its transformation.' },
  { word: 'Orthogonal Vectors', type: 'ëª…ì‚¬', meaning: 'ë‚´ì ì´ 0ì¸ ë‘ ë²¡í„°. ì„œë¡œ ìˆ˜ì§ì…ë‹ˆë‹¤.', example: 'Orthogonal vectors are perpendicular to each other.' },
  { word: 'Normalized Vector', type: 'ëª…ì‚¬', meaning: 'L2 ë…¸ë¦„ì´ 1ì¸ ë²¡í„°.', example: 'A normalized vector has unit length.' },
  { word: 'Orthogonal Matrix', type: 'ëª…ì‚¬', meaning: 'ì—´ ë²¡í„°ë“¤ì´ ì„œë¡œ ì§êµí•˜ë©° ì •ê·œí™”ëœ(ì§ê·œ ì§êµ) ì •ë°© í–‰ë ¬ (U>U = I = UU>). ì—­í–‰ë ¬ì´ ì „ì¹˜ì™€ ê°™ìŠµë‹ˆë‹¤.', example: 'The inverse of an orthogonal matrix is its transpose.' },
  { word: 'Determinant', type: 'ëª…ì‚¬', meaning: 'ì •ë°© í–‰ë ¬ì— ëŒ€í•´ ì •ì˜ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’. í–‰ë ¬ì´ ë‚˜íƒ€ë‚´ëŠ” ì„ í˜• ë³€í™˜ì— ì˜í•´ ê³µê°„ì˜ ë¶€í”¼ê°€ ì–¼ë§ˆë‚˜ ìŠ¤ì¼€ì¼ë§ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í–‰ë ¬ì´ íŠ¹ì´ í–‰ë ¬ì¼ ë•Œ í–‰ë ¬ì‹ì€ 0ì…ë‹ˆë‹¤.', example: 'A zero determinant means the matrix is singular.' },
  { word: 'Quadratic Form', type: 'ëª…ì‚¬', meaning: 'ì •ë°© í–‰ë ¬ Aì™€ ë²¡í„° xì— ëŒ€í•´ x>Ax í˜•íƒœë¡œ í‘œí˜„ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’.', example: 'Quadratic forms are used in optimization.' },
  { word: 'Positive Definite (PD) Matrix', type: 'ëª…ì‚¬', meaning: 'ëŒ€ì¹­ í–‰ë ¬ì´ë©°, ëª¨ë“  0ì´ ì•„ë‹Œ ë²¡í„° xì— ëŒ€í•´ x>Ax > 0ì¸ í–‰ë ¬.', example: 'A positive definite matrix has all positive eigenvalues.' },
  { word: 'Positive Semidefinite (PSD) Matrix', type: 'ëª…ì‚¬', meaning: 'ëŒ€ì¹­ í–‰ë ¬ì´ë©°, ëª¨ë“  ë²¡í„° xì— ëŒ€í•´ x>Ax â‰¥ 0ì¸ í–‰ë ¬.', example: 'A positive semidefinite matrix has nonnegative eigenvalues.' },
  { word: 'Eigenvalue', type: 'ëª…ì‚¬', meaning: 'í–‰ë ¬ Aë¥¼ ê³±í–ˆì„ ë•Œ ë²¡í„°ì˜ ë°©í–¥ì€ ìœ ì§€í•˜ê³  í¬ê¸°ë§Œ Î»ë§Œí¼ ìŠ¤ì¼€ì¼ë§ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ (Ax = Î»x).', example: 'Eigenvalues describe the scaling factor of eigenvectors.' },
  { word: 'Eigenvector', type: 'ëª…ì‚¬', meaning: 'ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” 0ì´ ì•„ë‹Œ ë²¡í„° (Ax = Î»x).', example: 'Eigenvectors remain in the same direction after transformation.' },
  { word: 'Gradient', type: 'ëª…ì‚¬', meaning: 'ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê° ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ ê°’ë“¤ì„ ë²¡í„°ë¡œ ëª¨ì•„ ë†“ì€ ê²ƒ (âˆ‡f(x)). í•¨ìˆ˜ì˜ ìµœëŒ€ ì¦ê°€ ë°©í–¥ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.', example: 'The gradient points in the direction of steepest ascent.' },
  { word: 'Hessian', type: 'ëª…ì‚¬', meaning: 'ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì´ì°¨ í¸ë¯¸ë¶„ ê°’ë“¤ì„ í–‰ë ¬ë¡œ ëª¨ì•„ ë†“ì€ ê²ƒ (âˆ‡Â²f(x)). í•¨ìˆ˜ì˜ ë³¼ë¡ì„± ë˜ëŠ” ì˜¤ëª©ì„±ì„ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.', example: 'The Hessian matrix is used in second-order optimization.' },
  { word: 'Random Variable', type: 'ëª…ì‚¬', meaning: 'ì‹¤í—˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜.', example: 'A random variable assigns a number to each outcome.' },
  { word: 'Cumulative Distribution Function (CDF)', type: 'ëª…ì‚¬', meaning: 'í™•ë¥  ë³€ìˆ˜ê°€ íŠ¹ì • ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì„ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ (F_X(x) = P{X â‰¤ x}).', example: 'The CDF gives the probability that a variable is less than or equal to a value.' },
  { word: 'Probability Density Function (PDF)', type: 'ëª…ì‚¬', meaning: 'ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜. CDFì˜ ë¯¸ë¶„ìœ¼ë¡œ ì–»ì–´ì§‘ë‹ˆë‹¤ (p_X(x) = dF_X(x)/dx).', example: 'The PDF describes the likelihood of a continuous variable.' },
  { word: 'Independent Random Variables', type: 'ëª…ì‚¬', meaning: 'ë‘ í™•ë¥  ë³€ìˆ˜ì˜ ê²°í•© ë¶„í¬ê°€ ê° í™•ë¥  ë³€ìˆ˜ì˜ ì£¼ë³€ ë¶„í¬ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ” ê²½ìš°.', example: 'Independent random variables do not influence each other.' },
  { word: 'Expected Value (Mean)', type: 'ëª…ì‚¬', meaning: 'í™•ë¥  ë³€ìˆ˜ì˜ "í‰ê· " ê°’. ì´ë¡ ì ì¸ ë¶„í¬ì— ê¸°ë°˜í•©ë‹ˆë‹¤ (E[X] = âˆ«x p_X(x) dx).', example: 'The expected value is the long-run average outcome.' },
  { word: 'Variance', type: 'ëª…ì‚¬', meaning: 'í™•ë¥  ë³€ìˆ˜ê°€ í‰ê·  ì£¼ë³€ì— ì–¼ë§ˆë‚˜ í¼ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (Var(X) = E[|X - E[X]|^2]).', example: 'Variance measures the spread of a distribution.' },
  { word: 'Correlation', type: 'ëª…ì‚¬', meaning: 'ë‘ í™•ë¥  ë³€ìˆ˜ ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ ê°•ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (E[XY*]).', example: 'Correlation quantifies the linear relationship between variables.' },
  { word: 'Covariance', type: 'ëª…ì‚¬', meaning: 'ë‘ í™•ë¥  ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (Cov(X,Y) = E[(X - E[X])(Y - E[Y])*]).', example: 'Covariance measures how two variables change together.' },
  { word: 'Random Vector', type: 'ëª…ì‚¬', meaning: 'ì—¬ëŸ¬ ê°œì˜ í™•ë¥  ë³€ìˆ˜ë¥¼ ë¬¶ì–´ì„œ ë²¡í„°ë¡œ í‘œí˜„í•œ ê²ƒ.', example: 'A random vector consists of multiple random variables.' },
  { word: 'Multivariate Gaussian (Normal) Distribution', type: 'ëª…ì‚¬', meaning: 'ì—¬ëŸ¬ í™•ë¥  ë³€ìˆ˜ì˜ ê²°í•© ë¶„í¬ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°. í‰ê·  ë²¡í„°ì™€ ê³µë¶„ì‚° í–‰ë ¬ë¡œ ì •ì˜ë©ë‹ˆë‹¤.', example: 'The multivariate normal distribution is defined by a mean vector and covariance matrix.' },
  { word: 'Jointly Gaussian Random Vector', type: 'ëª…ì‚¬', meaning: 'ì„ì˜ì˜ ì„ í˜• ë³€í™˜ì„ í–ˆì„ ë•Œ ê²°ê³¼ê°€ ê°€ìš°ì‹œì•ˆ í™•ë¥  ë³€ìˆ˜ê°€ ë˜ëŠ” ëœë¤ ë²¡í„°. ê° ìš”ì†Œê°€ ê°€ìš°ì‹œì•ˆì´ë”ë¼ë„ ê²°í•© ê°€ìš°ì‹œì•ˆì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.', example: 'Jointly Gaussian vectors remain Gaussian under linear transformation.' },
  { word: 'Random Process', type: 'ëª…ì‚¬', meaning: 'ì‹œê°„ì— ë”°ë¼ ë˜ëŠ” ê³µê°„ì— ë”°ë¼ ê°’ì´ ë³€í•˜ëŠ” í™•ë¥  ë³€ìˆ˜ë“¤ì˜ ëª¨ì„. ì´ ì—°êµ¬ ê°€ì´ë“œì—ì„œëŠ” 2ì°¨ì› ì´ì‚° ê³µê°„ì—ì„œì˜ ëœë¤ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.', example: 'A random process describes how a system evolves over time.' },
  { word: 'Power Spectrum (Power Spectral Density)', type: 'ëª…ì‚¬', meaning: 'ê´‘ì˜ ì •ìƒì„±(Wide Sense Stationary, WSS) ëœë¤ í”„ë¡œì„¸ìŠ¤ì˜ ìê¸° ìƒê´€ í•¨ìˆ˜(autocorrelation function)ì˜ ì´ì‚° ê³µê°„ í‘¸ë¦¬ì— ë³€í™˜(DSFT). ëœë¤ í”„ë¡œì„¸ìŠ¤ì˜ ì£¼íŒŒìˆ˜ êµ¬ì„± ìš”ì†Œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.', example: 'The power spectrum shows the frequency content of a process.' },
  { word: 'Loss Function', type: 'ëª…ì‚¬', meaning: 'ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (L(D(x), y)).', example: 'The loss function measures prediction error.' },
  { word: 'Supervised Learning', type: 'ëª…ì‚¬', meaning: 'ëª¨ë¸ í•™ìŠµì— ì…ë ¥ ë°ì´í„°(x)ì™€ í•´ë‹¹ ì •ë‹µ ë°ì´í„°(y)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹.', example: 'Supervised learning uses labeled data for training.' },
  { word: 'Empirical Loss Function', type: 'ëª…ì‚¬', meaning: 'ì‹¤ì œ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ í‰ê·  ë˜ëŠ” í•©. ë¶„ì„ì  ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¼ì‚¬ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.', example: 'Empirical loss is computed over the training dataset.' },
  { word: 'Gradient Descent', type: 'ëª…ì‚¬', meaning: 'í•¨ìˆ˜ì˜ ê·¹ì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ í•¨ìˆ˜ì˜ ìŒìˆ˜ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œí¼ ì´ë™í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.', example: 'Gradient descent is used to minimize loss functions.' },
  { word: 'Learning Rate (Step Size)', type: 'ëª…ì‚¬', meaning: 'ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸ì—ì„œ ê° ë‹¨ê³„ì—ì„œ ì´ë™í•˜ëŠ” ê±°ë¦¬.', example: 'The learning rate controls the step size in optimization.' },
  { word: 'Stochastic Gradient Descent (SGD)', type: 'ëª…ì‚¬', meaning: 'ì „ì²´ í•™ìŠµ ë°ì´í„° ëŒ€ì‹  ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì‘ì€ ë°°ì¹˜(batch)ì˜ ë°ì´í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.', example: 'SGD updates model parameters using random mini-batches.' },
  { word: 'Batch Size', type: 'ëª…ì‚¬', meaning: 'SGDì—ì„œ ê° ì—…ë°ì´íŠ¸ ë‹¨ê³„ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ ê°œìˆ˜.', example: 'Batch size determines how many samples are used per update.' },
  { word: 'Local Minimum', type: 'ëª…ì‚¬', meaning: 'í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ê³  í—¤ì„¸ í–‰ë ¬ì´ ì–‘ì˜ ì •ë¶€í˜¸ í–‰ë ¬ì¸ ì§€ì .', example: 'A local minimum is a point where the function value is lower than nearby points.' },
  { word: 'Overfitting', type: 'ëª…ì‚¬', meaning: 'ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ëŠ” ë§¤ìš° ì˜ ë§ì§€ë§Œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ.', example: 'Overfitting occurs when a model learns noise instead of the signal.' },
  { word: 'Underfitting', type: 'ëª…ì‚¬', meaning: 'ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ ë‚®ì€ í˜„ìƒ.', example: 'Underfitting means the model is too simple to capture the data patterns.' },
  { word: 'Regularization', type: 'ëª…ì‚¬', meaning: 'ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì¶”ê°€í•˜ëŠ” ê¸°ë²•.', example: 'Regularization helps prevent overfitting.' },
  { word: 'No Free Lunch Theorem', type: 'ëª…ì‚¬', meaning: 'ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì œì— ëŒ€í•´ ìµœì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ë‹¨ì¼ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì—†ë‹¤ëŠ” ì •ë¦¬.', example: 'The no free lunch theorem states that no algorithm is best for all problems.' },
  { word: 'Cross-Validation', type: 'ëª…ì‚¬', meaning: 'ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ í´ë“œ(fold)ë¡œ ë‚˜ëˆ„ì–´ ì¼ë¶€ëŠ” í•™ìŠµì— ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í‰ê°€ì— ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¶”ì •í•˜ëŠ” ê¸°ë²•.', example: 'Cross-validation is used to estimate model performance.' },
  { word: 'Classification', type: 'ëª…ì‚¬', meaning: 'ì…ë ¥ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì •ì˜ëœ ì—¬ëŸ¬ ë²”ì£¼(í´ë˜ìŠ¤) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ.', example: 'Classification assigns inputs to discrete categories.' },
  { word: 'Regression', type: 'ëª…ì‚¬', meaning: 'ì…ë ¥ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì—°ì†ì ì¸ ì¶œë ¥ ê°’(ì‹¤ìˆ˜ ê°’)ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ.', example: 'Regression predicts continuous values.' },
  { word: 'Density Estimation', type: 'ëª…ì‚¬', meaning: 'ì£¼ì–´ì§„ ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê±°ë‚˜ ì¶”ì •í•˜ëŠ” ë¬¸ì œ.', example: 'Density estimation models the probability distribution of data.' },
  { word: 'Object Detection', type: 'ëª…ì‚¬', meaning: 'ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì°¾ê³  í•´ë‹¹ ê°ì²´ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ì‹ë³„í•˜ëŠ” ë¬¸ì œ (ë¶„ë¥˜ì™€ íšŒê·€ì˜ ì¡°í•©).', example: 'Object detection locates and classifies objects in images.' },
  { word: 'Self-Supervised Learning', type: 'ëª…ì‚¬', meaning: 'ì •ë‹µ ë¼ë²¨ ì—†ì´ ì…ë ¥ ë°ì´í„° ìì²´ì—ì„œ í•™ìŠµ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹.', example: 'Self-supervised learning uses the data itself to generate labels.' },
];

let currentQuestions = [];
let currentQuestionIndex = 0;
let score = 0;

function shuffleArray(array) {
  for (let i = array.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [array[i], array[j]] = [array[j], array[i]];
  }
  return array;
}

function generateQuestionTypes() {
  const types = [
    { type: 'meaning', question: 'ë‹¤ìŒ ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?' },
    { type: 'word', question: 'ë‹¤ìŒ ì˜ë¯¸ì— í•´ë‹¹í•˜ëŠ” ì˜ë‹¨ì–´ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?' },
    { type: 'type', question: 'ë‹¤ìŒ ë‹¨ì–´ì˜ í’ˆì‚¬ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?' },
    { type: 'example', question: 'ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ë¹ˆì¹¸ì— ë“¤ì–´ê°ˆ ì ì ˆí•œ ë‹¨ì–´ëŠ”?' }
  ];
  return shuffleArray(types);
}

function generateQuestion(vocab, questionType) {
  const question = {
    type: questionType.type,
    questionText: questionType.question,
    correctAnswer: '',
    options: []
  };

  switch (questionType.type) {
    case 'meaning':
      question.word = vocab.word;
      question.correctAnswer = vocab.meaning;
      question.options = [vocab.meaning];
      // ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì˜ ì˜ë¯¸ì—ì„œ ë¬´ì‘ìœ„ë¡œ 3ê°œ ì„ íƒ
      const otherMeanings = vocabularyData
        .filter(v => v.word !== vocab.word)
        .map(v => v.meaning);
      question.options.push(...shuffleArray(otherMeanings).slice(0, 3));
      break;

    case 'word':
      question.meaning = vocab.meaning;
      question.correctAnswer = vocab.word;
      question.options = [vocab.word];
      // ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì—ì„œ ë¬´ì‘ìœ„ë¡œ 3ê°œ ì„ íƒ
      const otherWords = vocabularyData
        .filter(v => v.word !== vocab.word)
        .map(v => v.word);
      question.options.push(...shuffleArray(otherWords).slice(0, 3));
      break;

    case 'type':
      question.word = vocab.word;
      question.correctAnswer = vocab.type;
      question.options = ['ëª…ì‚¬', 'ë™ì‚¬', 'í˜•ìš©ì‚¬', 'ë¶€ì‚¬'];
      break;

    case 'example':
      question.example = vocab.example.replace(vocab.word, '_____');
      question.correctAnswer = vocab.word;
      question.options = [vocab.word];
      // ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì—ì„œ ë¬´ì‘ìœ„ë¡œ 3ê°œ ì„ íƒ
      const otherExamples = vocabularyData
        .filter(v => v.word !== vocab.word)
        .map(v => v.word);
      question.options.push(...shuffleArray(otherExamples).slice(0, 3));
      break;
  }

  question.options = shuffleArray(question.options);
  return question;
}

function startQuiz() {
  document.getElementById('study-section').style.display = 'none';
  document.getElementById('quiz-section').style.display = 'block';
  document.getElementById('start-quiz').style.display = 'none';
  document.getElementById('return-study').style.display = 'inline-block';

  // í€´ì¦ˆ ì´ˆê¸°í™”
  currentQuestionIndex = 0;
  score = 0;
  currentQuestions = [];

  // 10ê°œì˜ ëœë¤ ë‹¨ì–´ ì„ íƒ
  const selectedVocab = shuffleArray([...vocabularyData]).slice(0, 10);
  const questionTypes = generateQuestionTypes();

  // ê° ë‹¨ì–´ì— ëŒ€í•´ ëœë¤í•œ ìœ í˜•ì˜ ë¬¸ì œ ìƒì„±
  selectedVocab.forEach((vocab, index) => {
    const questionType = questionTypes[index % questionTypes.length];
    currentQuestions.push(generateQuestion(vocab, questionType));
  });

  showQuestion();
}

function showQuestion() {
  const question = currentQuestions[currentQuestionIndex];
  const questionContainer = document.getElementById('question-container');
  const optionsContainer = document.getElementById('options-container');
  
  // ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
  document.getElementById('current-question').textContent = currentQuestionIndex + 1;
  document.getElementById('total-questions').textContent = currentQuestions.length;

  // ë¬¸ì œ í‘œì‹œ
  let questionText = question.questionText + '<br>';
  switch (question.type) {
    case 'meaning':
      questionText += `<strong>${question.word}</strong>`;
      break;
    case 'word':
      questionText += `<strong>${question.meaning}</strong>`;
      break;
    case 'type':
      questionText += `<strong>${question.word}</strong>`;
      break;
    case 'example':
      questionText += `<strong>${question.example}</strong>`;
      break;
  }
  questionContainer.innerHTML = questionText;

  // ë³´ê¸° í‘œì‹œ
  optionsContainer.innerHTML = '';
  question.options.forEach((option, index) => {
    const optionDiv = document.createElement('div');
    optionDiv.className = 'option';
    optionDiv.textContent = option;
    optionDiv.onclick = () => checkAnswer(option);
    optionsContainer.appendChild(optionDiv);
  });

  // ê²°ê³¼ ë©”ì‹œì§€ì™€ ë‹¤ìŒ ë²„íŠ¼ ìˆ¨ê¸°ê¸°
  document.getElementById('result-message').style.display = 'none';
  document.getElementById('next-btn').style.display = 'none';
}

function checkAnswer(selectedAnswer) {
  const question = currentQuestions[currentQuestionIndex];
  const resultMessage = document.getElementById('result-message');
  const nextButton = document.getElementById('next-btn');
  const options = document.querySelectorAll('.option');

  // ëª¨ë“  ì˜µì…˜ ë¹„í™œì„±í™”
  options.forEach(option => {
    option.style.pointerEvents = 'none';
    if (option.textContent === question.correctAnswer) {
      option.classList.add('correct');
    }
    if (option.textContent === selectedAnswer && selectedAnswer !== question.correctAnswer) {
      option.classList.add('incorrect');
    }
  });

  // ì •ë‹µ ì²´í¬
  if (selectedAnswer === question.correctAnswer) {
    score++;
    resultMessage.style.backgroundColor = '#d4edda';
    resultMessage.style.color = '#155724';
    resultMessage.textContent = 'ì •ë‹µì…ë‹ˆë‹¤! ğŸ‰';
  } else {
    resultMessage.style.backgroundColor = '#f8d7da';
    resultMessage.style.color = '#721c24';
    resultMessage.textContent = `í‹€ë ¸ìŠµë‹ˆë‹¤. ì •ë‹µì€ "${question.correctAnswer}" ì…ë‹ˆë‹¤.`;
  }

  resultMessage.style.display = 'block';
  nextButton.style.display = 'block';
  nextButton.onclick = () => {
    if (currentQuestionIndex < currentQuestions.length - 1) {
      currentQuestionIndex++;
      showQuestion();
    } else {
      showFinalScore();
    }
  };
}

function showFinalScore() {
  const quizContainer = document.getElementById('quiz-container');
  const finalScore = document.getElementById('final-score');
  const percentage = (score / currentQuestions.length) * 100;

  quizContainer.innerHTML = `
    <div id="final-score">
      <h3>í€´ì¦ˆ ì™„ë£Œ!</h3>
      <p>ë‹¹ì‹ ì˜ ì ìˆ˜: ${score} / ${currentQuestions.length} (${percentage.toFixed(1)}%)</p>
      <button onclick="startQuiz()">ë‹¤ì‹œ ì‹œì‘í•˜ê¸°</button>
    </div>
  `;
}

function returnToStudy() {
  document.getElementById('study-section').style.display = 'block';
  document.getElementById('quiz-section').style.display = 'none';
  document.getElementById('start-quiz').style.display = 'inline-block';
  document.getElementById('return-study').style.display = 'none';
}
</script>


## Core Concepts and Terminology

### 1. Perception and Processing (ì§€ê°ê³¼ ì²˜ë¦¬)

- **Perceive** (ë™ì‚¬): ì¸ì§€í•˜ë‹¤, ê°ì§€í•˜ë‹¤
  - Example: Computers can perceive subtle differences in images that humans might miss.
  
- **Perceptual** (í˜•ìš©ì‚¬): ì§€ê°ì˜, ì¸ì§€ì˜
  - Example: Perceptual computing aims to mimic human sensory processing.
  
- **Misperception** (ëª…ì‚¬): ì˜¤ì¸, ì˜ëª»ëœ ì¸ì‹
  - Example: Image artifacts can lead to misperception in computer vision systems.

### 2. Visual Properties (ì‹œê°ì  íŠ¹ì„±)

- **Translucency** (ëª…ì‚¬): ë°˜íˆ¬ëª…ì„±
  - Example: Processing translucency in images requires advanced rendering techniques.
  
- **Subtle** (í˜•ìš©ì‚¬): ë¯¸ë¬˜í•œ, ì„¬ì„¸í•œ
  - Example: The algorithm can detect subtle changes in lighting conditions.
  
- **Shading** (ëª…ì‚¬): ìŒì˜
  - Example: Proper shading analysis helps in understanding 3D structures.

### 3. Image Analysis (ì´ë¯¸ì§€ ë¶„ì„)

- **Effortlessly** (ë¶€ì‚¬): ìˆ˜ì›”í•˜ê²Œ, ì–´ë ¤ì›€ ì—†ì´
  - Example: Modern AI systems effortlessly process thousands of images per second.
  
- **Portrait** (ëª…ì‚¬): ì´ˆìƒ, ì¸ë¬¼ ì‚¬ì§„
  - Example: Face detection algorithms are crucial for portrait photography.
  
- **Rapid** (í˜•ìš©ì‚¬): ë¹ ë¥¸, ì‹ ì†í•œ
  - Example: Rapid image processing is essential for real-time applications.

### 4. Technical Processes (ê¸°ìˆ ì  ê³¼ì •)

- **Delineate** (ë™ì‚¬): ìœ¤ê³½ì„ ê·¸ë¦¬ë‹¤, ë¬˜ì‚¬í•˜ë‹¤
  - Example: The algorithm can delineate object boundaries precisely.
  
- **Causality** (ëª…ì‚¬): ì¸ê³¼ê´€ê³„
  - Example: Understanding causality in scene analysis improves prediction accuracy.
  
- **Radiometry** (ëª…ì‚¬): ë°©ì‚¬ì¸¡ì •
  - Example: Radiometry is fundamental to understanding light behavior in images.

### 5. Industrial Applications (ì‚°ì—…ì  ì‘ìš©)

- **Inspection** (ëª…ì‚¬): ê²€ì‚¬, ì ê²€
  - Example: Automated visual inspection systems in manufacturing.
  
- **Retail** (ëª…ì‚¬): ì†Œë§¤, ìœ í†µ
  - Example: Computer vision enhances retail analytics and customer behavior tracking.
  
- **Warehouse** (ëª…ì‚¬): ì°½ê³ 
  - Example: Automated warehouse systems use computer vision for inventory management.
  
- **Logistics** (ëª…ì‚¬): ë¬¼ë¥˜
  - Example: Computer vision optimizes logistics operations through package tracking.

### 6. Advanced Applications (ê³ ê¸‰ ì‘ìš©)

- **Intra-operative** (í˜•ìš©ì‚¬): ìˆ˜ìˆ  ì¤‘ì˜
  - Example: Intra-operative imaging assists surgeons during procedures.
  
- **Morphology** (ëª…ì‚¬): í˜•íƒœí•™
  - Example: Mathematical morphology is used in image processing.
  
- **Surveillance** (ëª…ì‚¬): ê°ì‹œ, ë³´ì•ˆ
  - Example: Video surveillance systems employ advanced computer vision techniques.

### 7. Image Processing Techniques (ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë²•)

- **Stitching** (ëª…ì‚¬): ì´ë¯¸ì§€ ì ‘í•©
  - Example: Panorama creation through image stitching.
  
- **Overlapping** (í˜•ìš©ì‚¬): ì¤‘ì²©ë˜ëŠ”
  - Example: Overlapping images are used in 3D reconstruction.
  
- **Bracketing** (ëª…ì‚¬): ë¸Œë¼ì¼€íŒ…(ë…¸ì¶œ ë‹¨ê³„ ì¡°ì ˆ)
  - Example: HDR images are created using exposure bracketing.
  
- **Morphing** (ëª…ì‚¬): ë³€í˜•
  - Example: Face morphing creates smooth transitions between images.

### 8. Research and Development (ì—°êµ¬ ê°œë°œ)

- **Meticulously** (ë¶€ì‚¬): ê¼¼ê¼¼í•˜ê²Œ, ì •êµí•˜ê²Œ
  - Example: AI models must be meticulously trained for optimal performance.
  
- **Layer-wise, sample-wise** (í˜•ìš©ì‚¬): ì¸µë³„, ìƒ˜í”Œë³„
  - Example: Neural networks are analyzed layer-wise for better understanding.
  
- **Preexisting** (í˜•ìš©ì‚¬): ê¸°ì¡´ì˜, ì´ë¯¸ ì¡´ì¬í•˜ëŠ”
  - Example: Preexisting models can be fine-tuned for specific tasks.
  
- **Plausible** (í˜•ìš©ì‚¬): ê·¸ëŸ´ë“¯í•œ, íƒ€ë‹¹í•œ
  - Example: The system generates plausible reconstructions of 3D scenes.

### 9. Industrial Human-Robot Collaboration (ì‚°ì—… í˜„ì¥ ì¸ê°„-ë¡œë´‡ í˜‘ì—…)

- **Pose Forecasting** (ëª…ì‚¬êµ¬): ìì„¸ ì˜ˆì¸¡
  - Example: Pose forecasting is crucial for safe human-robot interaction.
- **Ablation** (ëª…ì‚¬): (ì‹ ê²½ë§ ë“±ì—ì„œ) ì¼ë¶€ ì œê±°, ì ˆì œ
  - Example: Ablation studies help understand the contribution of different model components.
- **Endow** (ë™ì‚¬): ë¶€ì—¬í•˜ë‹¤, ì£¼ë‹¤
  - Example: The new sensor endows the robot with better perceptive capabilities.
- **Perceptive** (í˜•ìš©ì‚¬): ì§€ê°í•˜ëŠ”, í†µì°°ë ¥ ìˆëŠ”
  - Example: Perceptive robots can adapt to dynamic environments.
- **Harm** (ëª…ì‚¬/ë™ì‚¬): í•´, ì†ìƒ / í•´ë¥¼ ë¼ì¹˜ë‹¤
  - Example: The primary goal is to prevent any harm to the human worker.
- **Transmission of contact wrenches** (ëª…ì‚¬êµ¬): ì ‘ì´‰ë ¥ ì „ë‹¬
  - Example: Accurate transmission of contact wrenches is vital for haptic feedback.
- **On-the-fly** (ë¶€ì‚¬êµ¬): ì¦‰ì„ì—ì„œ, ì‹¤ì‹œê°„ìœ¼ë¡œ
  - Example: The robot can adjust its path on-the-fly to avoid obstacles.
- **Adjacency** (ëª…ì‚¬): ì¸ì ‘, ê·¼ì ‘
  - Example: Adjacency matrices are used to represent connections in a graph.
- **Sparse** (í˜•ìš©ì‚¬): í¬ì†Œí•œ, ë“œë¬¸ë“œë¬¸ ìˆëŠ”
  - Example: Sparse data can be challenging for some machine learning models.
- **Sparsity** (ëª…ì‚¬): í¬ì†Œì„±
  - Example: Sparsity is a desirable property in many high-dimensional datasets.
- **Abide** (ë™ì‚¬): (ê·œì¹™ ë“±ì„) ì¤€ìˆ˜í•˜ë‹¤, ì§€í‚¤ë‹¤
  - Example: Robots must abide by safety protocols.
- **Genuine** (í˜•ìš©ì‚¬): ì§„ì§œì˜, ì§„ì •í•œ
  - Example: The system aims to achieve genuine collaboration between humans and robots.
- **Foresee** (ë™ì‚¬): ì˜ˆê²¬í•˜ë‹¤, ì˜ˆì¸¡í•˜ë‹¤
  - Example: It is difficult to foresee all possible scenarios in a complex environment.
- **Exploited** (ë™ì‚¬, ìˆ˜ë™íƒœ ë˜ëŠ” ê³¼ê±°í˜•): í™œìš©ëœ, ì´ìš©ëœ
  - Example: The robot's capabilities were fully exploited in the task.
- **Aspect** (ëª…ì‚¬): ì¸¡ë©´, ì–‘ìƒ
  - Example: Safety is a critical aspect of human-robot collaboration.
- **Intersection** (ëª…ì‚¬): êµì°¨ì , êµì§‘í•©
  - Example: The intersection of AI and robotics has led to many innovations.
- **Prune** (ë™ì‚¬): (ê°€ì§€ ë“±ì„) ì¹˜ë‹¤, ì œê±°í•˜ë‹¤, ê°„ê²°í•˜ê²Œ í•˜ë‹¤
  - Example: Pruning the neural network can reduce its complexity.
- **Factorize** (ë™ì‚¬): ì¸ìˆ˜ë¶„í•´í•˜ë‹¤, ìš”ì¸ìœ¼ë¡œ ë¶„ì„í•˜ë‹¤
  - Example: We can factorize the matrix to understand its underlying structure.
- **Adjacency** (ëª…ì‚¬): ì¸ì ‘, ê·¼ì ‘ (ì¤‘ë³µëœ ë‹¨ì–´ì…ë‹ˆë‹¤. í•„ìš”ì‹œ í•˜ë‚˜ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ë‹¤ë¥¸ ì˜ë¯¸ë¡œ ì‚¬ìš©ëœ ê²½ìš° ì„¤ëª…ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.)
  - Example: Adjacency lists are another way to represent graph connections.
- **Spectral** (í˜•ìš©ì‚¬): ìŠ¤í™íŠ¸ëŸ¼ì˜, ë¶„ê´‘ì˜
  - Example: Spectral analysis is used in various sensor technologies.
- **Coarse** (í˜•ìš©ì‚¬): ê±°ì¹œ, ëŒ€ëµì ì¸
  - Example: A coarse estimation was made initially, followed by a finer adjustment.
- **Sparsifying** (ë™ì‚¬/í˜•ìš©ì‚¬): í¬ì†Œí™”í•˜ëŠ”
  - Example: Sparsifying techniques are used to reduce data dimensionality.

### 10. Modern Artificial Intelligence (í˜„ëŒ€ ì¸ê³µì§€ëŠ¥)

- **Formalize** (ë™ì‚¬): ê³µì‹í™”í•˜ë‹¤, í˜•ì‹í™”í•˜ë‹¤
  - Example: We need to formalize the problem statement before designing a solution.
- **Density** (ëª…ì‚¬): ë°€ë„
  - Example: Probability density functions describe the likelihood of a continuous variable.
- **PDF (Probability Density Function)** (ëª…ì‚¬): í™•ë¥  ë°€ë„ í•¨ìˆ˜
  - Example: The PDF of a normal distribution is bell-shaped.
- **PMF (Probability Mass Function)** (ëª…ì‚¬): í™•ë¥  ì§ˆëŸ‰ í•¨ìˆ˜
  - Example: The PMF is used for discrete random variables.
- **Accomplish** (ë™ì‚¬): ì„±ì·¨í•˜ë‹¤, ë‹¬ì„±í•˜ë‹¤
  - Example: The AI model was able to accomplish the task with high accuracy.
- **Observed** (í˜•ìš©ì‚¬/ë™ì‚¬ ê³¼ê±°í˜•): ê´€ì°°ëœ
  - Example: The observed data was used to train the model.
- **Intractable** (í˜•ìš©ì‚¬): ë‹¤ë£¨ê¸° í˜ë“ , ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´
  - Example: Some problems are computationally intractable.
- **Coefficient** (ëª…ì‚¬): ê³„ìˆ˜
  - Example: The coefficients of the linear regression model were estimated.
- **Era** (ëª…ì‚¬): ì‹œëŒ€
  - Example: We are in the era of big data and artificial intelligence.
- **Deluge** (ëª…ì‚¬): í­ì£¼, ì‡„ë„, í™ìˆ˜
  - Example: Researchers face a deluge of data from various sources.
- **Apparently** (ë¶€ì‚¬): ëª…ë°±íˆ, ë³´ì•„í•˜ë‹ˆ
  - Example: Apparently, the new algorithm outperforms the old one.
- **Sufficient** (í˜•ìš©ì‚¬): ì¶©ë¶„í•œ
  - Example: Sufficient data is required to train a robust model.
- **Altering** (ë™ì‚¬/í˜•ìš©ì‚¬): ë³€ê²½í•˜ëŠ”
  - Example: Altering the hyperparameters can significantly affect model performance.
- **Ability to fix** (êµ¬ë¬¸): ~ì„ ê³ ì¹˜ëŠ” ëŠ¥ë ¥
  - Example: The system has the ability to fix minor errors automatically.
- **Devising** (ë™ì‚¬): ê³ ì•ˆí•˜ë‹¤, ì°½ì•ˆí•˜ë‹¤
  - Example: Devising new algorithms is a key part of AI research.
- **Empirical** (í˜•ìš©ì‚¬): ê²½í—˜ì ì¸, ì‹¤ì¦ì ì¸
  - Example: Empirical results show the effectiveness of the proposed method.
- **Theorem** (ëª…ì‚¬): ì •ë¦¬ (ìˆ˜í•™, ë…¼ë¦¬í•™)
  - Example: The Bayes' theorem is fundamental in probability theory.
- **Criterion** (ëª…ì‚¬): ê¸°ì¤€, í‘œì¤€ (ë³µìˆ˜í˜•: criteria)
  - Example: Accuracy is a common criterion for evaluating classification models.
- **Impose** (ë™ì‚¬): ë¶€ê³¼í•˜ë‹¤, ê°•ìš”í•˜ë‹¤, ë„ì…í•˜ë‹¤
  - Example: We can impose constraints on the optimization problem.
- **Regularization** (ëª…ì‚¬): ì •ê·œí™” (ê¸°ê³„ í•™ìŠµ)
  - Example: Regularization helps prevent overfitting in machine learning models.
- **Implicitly** (ë¶€ì‚¬): ì•”ë¬µì ìœ¼ë¡œ, í•¨ì¶•ì ìœ¼ë¡œ
  - Example: The model implicitly learns features from the data.
- **Explicitly** (ë¶€ì‚¬): ëª…ì‹œì ìœ¼ë¡œ, ëª…ë°±íˆ
  - Example: Some parameters need to be explicitly defined.
- **Preferences** (ëª…ì‚¬): ì„ í˜¸ë„
  - Example: The recommendation system learns user preferences over time.
- **Validation** (ëª…ì‚¬): ê²€ì¦, í™•ì¸
  - Example: Cross-validation is used to assess model performance.
- **Problematic** (í˜•ìš©ì‚¬): ë¬¸ì œê°€ ìˆëŠ”, ë¬¸ì œê°€ ë§ì€
  - Example: Handling missing data can be problematic.
- **Associate** (ë™ì‚¬): ì—°ê´€ì‹œí‚¤ë‹¤, ê´€ë ¨ì§“ë‹¤
  - Example: The goal is to associate input features with output labels.
- **Evaluations** (ëª…ì‚¬): í‰ê°€
  - Example: Multiple evaluations were conducted to ensure robustness.
- **Converge** (ë™ì‚¬): ìˆ˜ë ´í•˜ë‹¤, í•œ ì ì— ëª¨ì´ë‹¤
  - Example: The optimization algorithm should converge to a good solution.
- **Convex** (í˜•ìš©ì‚¬): ë³¼ë¡í•œ (ìˆ˜í•™)
  - Example: Convex optimization problems are generally easier to solve.

## Writing Technical Papers (ë…¼ë¬¸ ì‘ì„±)

- **Inventing** (ë™ì‚¬): ë°œëª…í•˜ë‹¤, ê³ ì•ˆí•˜ë‹¤
  - Example: Inventing new algorithms for efficient image processing.
  
- **Revising** (ë™ì‚¬): ìˆ˜ì •í•˜ë‹¤, êµì •í•˜ë‹¤
  - Example: Revising the methodology section of the research paper.
  
- **Drafting** (ë™ì‚¬): ì´ˆì•ˆì„ ì‘ì„±í•˜ë‹¤
  - Example: Drafting the experimental results section.

### 11. ìˆ˜í•™/ì„ í˜•ëŒ€ìˆ˜/í™•ë¥ /ë¨¸ì‹ ëŸ¬ë‹ ìš©ì–´ ì •ë¦¬

- **Vector**: í¬ê¸°ì™€ ë°©í–¥ì„ ê°€ì§€ëŠ” ìˆ˜í•™ì  ê°ì²´. ì¼ë°˜ì ìœ¼ë¡œ ìˆ«ì ëª©ë¡ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
- **Matrix**: í–‰ê³¼ ì—´ë¡œ ë°°ì—´ëœ ìˆ«ìì˜ ì§ì‚¬ê°í˜• ë°°ì—´.
- **Inner Product (Dot Product)**: ë‘ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ ìŠ¤ì¹¼ë¼ ê°’ì„ ì–»ëŠ” ì—°ì‚°. ë²¡í„°ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
- **Outer Product**: ë‘ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ í–‰ë ¬ì„ ì–»ëŠ” ì—°ì‚°.
- **Matrix-Vector Product**: í–‰ë ¬ê³¼ ë²¡í„°ë¥¼ ê³±í•˜ì—¬ ë²¡í„°ë¥¼ ì–»ëŠ” ì—°ì‚°. ë²¡í„°ë¥¼ í–‰ë ¬ì˜ ì—´ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Matrix-Matrix Product**: ë‘ í–‰ë ¬ì„ ê³±í•˜ì—¬ í–‰ë ¬ì„ ì–»ëŠ” ì—°ì‚°.
- **Identity Matrix**: ì£¼ëŒ€ê°ì„  ìš”ì†Œê°€ ëª¨ë‘ 1ì´ê³  ë‚˜ë¨¸ì§€ ìš”ì†ŒëŠ” ëª¨ë‘ 0ì¸ ì •ë°© í–‰ë ¬. í–‰ë ¬ ê³±ì…ˆì—ì„œ í•­ë“±ì› ì—­í• ì„ í•©ë‹ˆë‹¤.
- **Diagonal Matrix**: ì£¼ëŒ€ê°ì„  ìš”ì†Œ ì™¸ì˜ ëª¨ë“  ìš”ì†Œê°€ 0ì¸ ì •ë°© í–‰ë ¬.
- **Transpose**: í–‰ë ¬ì˜ í–‰ê³¼ ì—´ì„ ë°”ê¾¼ ì—°ì‚° (A>).
- **Symmetric Matrix**: ì „ì¹˜ì™€ ê°™ì€ ì •ë°© í–‰ë ¬ (A = A>).
- **Trace**: ì •ë°© í–‰ë ¬ì˜ ì£¼ëŒ€ê°ì„  ìš”ì†Œë“¤ì˜ í•© (tr(A)).
- **Norm**: ë²¡í„° ë˜ëŠ” í–‰ë ¬ì˜ "í¬ê¸°"ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜. L1, L2, Lâˆ, Frobenius ë…¸ë¦„ ë“±ì´ ìˆìŠµë‹ˆë‹¤.
- **Linear Independence**: ë²¡í„° ì§‘í•© ë‚´ì˜ ì–´ë–¤ ë²¡í„°ë„ ë‚˜ë¨¸ì§€ ë²¡í„°ë“¤ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ì—†ëŠ” ì†ì„±.
- **Rank**: í–‰ë ¬ì˜ ì„ í˜• ë…ë¦½ì¸ í–‰ ë˜ëŠ” ì—´ì˜ ìµœëŒ€ ê°œìˆ˜. í–‰ë ¬ì˜ ì°¨ì› ê³µê°„ì„ ì–¼ë§ˆë‚˜ ì˜ ì±„ìš°ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- **Invertible (Non-singular) Matrix**: ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ëŠ” ì •ë°© í–‰ë ¬.
- **Inverse Matrix**: í–‰ë ¬ Aì— ëŒ€í•´ ê³±í–ˆì„ ë•Œ í•­ë“± í–‰ë ¬ì„ ë§Œë“œëŠ” ìœ ì¼í•œ í–‰ë ¬ (A^-1).
- **Orthogonal Vectors**: ë‚´ì ì´ 0ì¸ ë‘ ë²¡í„°. ì„œë¡œ ìˆ˜ì§ì…ë‹ˆë‹¤.
- **Normalized Vector**: L2 ë…¸ë¦„ì´ 1ì¸ ë²¡í„°.
- **Orthogonal Matrix**: ì—´ ë²¡í„°ë“¤ì´ ì„œë¡œ ì§êµí•˜ë©° ì •ê·œí™”ëœ(ì§ê·œ ì§êµ) ì •ë°© í–‰ë ¬ (U>U = I = UU>). ì—­í–‰ë ¬ì´ ì „ì¹˜ì™€ ê°™ìŠµë‹ˆë‹¤.
- **Determinant**: ì •ë°© í–‰ë ¬ì— ëŒ€í•´ ì •ì˜ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’. í–‰ë ¬ì´ ë‚˜íƒ€ë‚´ëŠ” ì„ í˜• ë³€í™˜ì— ì˜í•´ ê³µê°„ì˜ ë¶€í”¼ê°€ ì–¼ë§ˆë‚˜ ìŠ¤ì¼€ì¼ë§ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í–‰ë ¬ì´ íŠ¹ì´ í–‰ë ¬ì¼ ë•Œ í–‰ë ¬ì‹ì€ 0ì…ë‹ˆë‹¤.
- **Quadratic Form**: ì •ë°© í–‰ë ¬ Aì™€ ë²¡í„° xì— ëŒ€í•´ x>Ax í˜•íƒœë¡œ í‘œí˜„ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’.
- **Positive Definite (PD) Matrix**: ëŒ€ì¹­ í–‰ë ¬ì´ë©°, ëª¨ë“  0ì´ ì•„ë‹Œ ë²¡í„° xì— ëŒ€í•´ x>Ax > 0ì¸ í–‰ë ¬.
- **Positive Semidefinite (PSD) Matrix**: ëŒ€ì¹­ í–‰ë ¬ì´ë©°, ëª¨ë“  ë²¡í„° xì— ëŒ€í•´ x>Ax â‰¥ 0ì¸ í–‰ë ¬.
- **Eigenvalue**: í–‰ë ¬ Aë¥¼ ê³±í–ˆì„ ë•Œ ë²¡í„°ì˜ ë°©í–¥ì€ ìœ ì§€í•˜ê³  í¬ê¸°ë§Œ Î»ë§Œí¼ ìŠ¤ì¼€ì¼ë§ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ (Ax = Î»x).
- **Eigenvector**: ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” 0ì´ ì•„ë‹Œ ë²¡í„° (Ax = Î»x).
- **Gradient**: ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê° ë³€ìˆ˜ì— ëŒ€í•œ í¸ë¯¸ë¶„ ê°’ë“¤ì„ ë²¡í„°ë¡œ ëª¨ì•„ ë†“ì€ ê²ƒ (âˆ‡f(x)). í•¨ìˆ˜ì˜ ìµœëŒ€ ì¦ê°€ ë°©í–¥ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
- **Hessian**: ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì´ì°¨ í¸ë¯¸ë¶„ ê°’ë“¤ì„ í–‰ë ¬ë¡œ ëª¨ì•„ ë†“ì€ ê²ƒ (âˆ‡Â²f(x)). í•¨ìˆ˜ì˜ ë³¼ë¡ì„± ë˜ëŠ” ì˜¤ëª©ì„±ì„ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
- **Random Variable**: ì‹¤í—˜ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜.
- **Cumulative Distribution Function (CDF)**: í™•ë¥  ë³€ìˆ˜ê°€ íŠ¹ì • ê°’ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ì„ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ (F_X(x) = P{X â‰¤ x}).
- **Probability Density Function (PDF)**: ì—°ì† í™•ë¥  ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜. CDFì˜ ë¯¸ë¶„ìœ¼ë¡œ ì–»ì–´ì§‘ë‹ˆë‹¤ (p_X(x) = dF_X(x)/dx).
- **Independent Random Variables**: ë‘ í™•ë¥  ë³€ìˆ˜ì˜ ê²°í•© ë¶„í¬ê°€ ê° í™•ë¥  ë³€ìˆ˜ì˜ ì£¼ë³€ ë¶„í¬ì˜ ê³±ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ” ê²½ìš°.
- **Expected Value (Mean)**: í™•ë¥  ë³€ìˆ˜ì˜ "í‰ê· " ê°’. ì´ë¡ ì ì¸ ë¶„í¬ì— ê¸°ë°˜í•©ë‹ˆë‹¤ (E[X] = âˆ«x p_X(x) dx).
- **Variance**: í™•ë¥  ë³€ìˆ˜ê°€ í‰ê·  ì£¼ë³€ì— ì–¼ë§ˆë‚˜ í¼ì ¸ ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (Var(X) = E[|X - E[X]|^2]).
- **Correlation**: ë‘ í™•ë¥  ë³€ìˆ˜ ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ ê°•ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (E[XY*]).
- **Covariance**: ë‘ í™•ë¥  ë³€ìˆ˜ê°€ í•¨ê»˜ ë³€í•˜ëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¸¡ë„ (Cov(X,Y) = E[(X - E[X])(Y - E[Y])*]).
- **Random Vector**: ì—¬ëŸ¬ ê°œì˜ í™•ë¥  ë³€ìˆ˜ë¥¼ ë¬¶ì–´ì„œ ë²¡í„°ë¡œ í‘œí˜„í•œ ê²ƒ.
- **Multivariate Gaussian (Normal) Distribution**: ì—¬ëŸ¬ í™•ë¥  ë³€ìˆ˜ì˜ ê²°í•© ë¶„í¬ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš°. í‰ê·  ë²¡í„°ì™€ ê³µë¶„ì‚° í–‰ë ¬ë¡œ ì •ì˜ë©ë‹ˆë‹¤.
- **Jointly Gaussian Random Vector**: ì„ì˜ì˜ ì„ í˜• ë³€í™˜ì„ í–ˆì„ ë•Œ ê²°ê³¼ê°€ ê°€ìš°ì‹œì•ˆ í™•ë¥  ë³€ìˆ˜ê°€ ë˜ëŠ” ëœë¤ ë²¡í„°. ê° ìš”ì†Œê°€ ê°€ìš°ì‹œì•ˆì´ë”ë¼ë„ ê²°í•© ê°€ìš°ì‹œì•ˆì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- **Random Process**: ì‹œê°„ì— ë”°ë¼ ë˜ëŠ” ê³µê°„ì— ë”°ë¼ ê°’ì´ ë³€í•˜ëŠ” í™•ë¥  ë³€ìˆ˜ë“¤ì˜ ëª¨ì„. ì´ ì—°êµ¬ ê°€ì´ë“œì—ì„œëŠ” 2ì°¨ì› ì´ì‚° ê³µê°„ì—ì„œì˜ ëœë¤ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
- **Power Spectrum (Power Spectral Density)**: ê´‘ì˜ ì •ìƒì„±(Wide Sense Stationary, WSS) ëœë¤ í”„ë¡œì„¸ìŠ¤ì˜ ìê¸° ìƒê´€ í•¨ìˆ˜(autocorrelation function)ì˜ ì´ì‚° ê³µê°„ í‘¸ë¦¬ì— ë³€í™˜(DSFT). ëœë¤ í”„ë¡œì„¸ìŠ¤ì˜ ì£¼íŒŒìˆ˜ êµ¬ì„± ìš”ì†Œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
- **Loss Function**: ê¸°ê³„ í•™ìŠµ ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ë¶ˆì¼ì¹˜ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (L(D(x), y)).
- **Supervised Learning**: ëª¨ë¸ í•™ìŠµì— ì…ë ¥ ë°ì´í„°(x)ì™€ í•´ë‹¹ ì •ë‹µ ë°ì´í„°(y)ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹.
- **Empirical Loss Function**: ì‹¤ì œ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ í‰ê·  ë˜ëŠ” í•©. ë¶„ì„ì  ì†ì‹¤ í•¨ìˆ˜ì˜ ê·¼ì‚¬ê°’ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
- **Gradient Descent**: í•¨ìˆ˜ì˜ ê·¹ì†Œê°’ì„ ì°¾ê¸° ìœ„í•´ í˜„ì¬ ìœ„ì¹˜ì—ì„œ í•¨ìˆ˜ì˜ ìŒìˆ˜ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì¼ì • ê±°ë¦¬ë§Œí¼ ì´ë™í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.
- **Learning Rate (Step Size)**: ê·¸ë˜ë””ì–¸íŠ¸ ë””ì„¼íŠ¸ì—ì„œ ê° ë‹¨ê³„ì—ì„œ ì´ë™í•˜ëŠ” ê±°ë¦¬.
- **Stochastic Gradient Descent (SGD)**: ì „ì²´ í•™ìŠµ ë°ì´í„° ëŒ€ì‹  ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì‘ì€ ë°°ì¹˜(batch)ì˜ ë°ì´í„°ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜.
- **Batch Size**: SGDì—ì„œ ê° ì—…ë°ì´íŠ¸ ë‹¨ê³„ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ ê°œìˆ˜.
- **Local Minimum**: í•¨ìˆ˜ì˜ ê·¸ë˜ë””ì–¸íŠ¸ê°€ 0ì´ê³  í—¤ì„¸ í–‰ë ¬ì´ ì–‘ì˜ ì •ë¶€í˜¸ í–‰ë ¬ì¸ ì§€ì .
- **Overfitting**: ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ëŠ” ë§¤ìš° ì˜ ë§ì§€ë§Œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ.
- **Underfitting**: ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•´ í•™ìŠµ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„° ëª¨ë‘ì—ì„œ ì„±ëŠ¥ì´ ë‚®ì€ í˜„ìƒ.
- **Regularization**: ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì¶”ê°€í•˜ëŠ” ê¸°ë²•.
- **No Free Lunch Theorem**: ëª¨ë“  ì¢…ë¥˜ì˜ ë¬¸ì œì— ëŒ€í•´ ìµœì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ë‹¨ì¼ ê¸°ê³„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì€ ì—†ë‹¤ëŠ” ì •ë¦¬.
- **Cross-Validation**: ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ í´ë“œ(fold)ë¡œ ë‚˜ëˆ„ì–´ ì¼ë¶€ëŠ” í•™ìŠµì— ì‚¬ìš©í•˜ê³  ë‚˜ë¨¸ì§€ëŠ” í‰ê°€ì— ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ì¶”ì •í•˜ëŠ” ê¸°ë²•.
- **Classification**: ì…ë ¥ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì •ì˜ëœ ì—¬ëŸ¬ ë²”ì£¼(í´ë˜ìŠ¤) ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œ.
- **Regression**: ì…ë ¥ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ì—°ì†ì ì¸ ì¶œë ¥ ê°’(ì‹¤ìˆ˜ ê°’)ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œ.
- **Density Estimation**: ì£¼ì–´ì§„ ë°ì´í„°ì˜ í™•ë¥  ë¶„í¬ë¥¼ ëª¨ë¸ë§í•˜ê±°ë‚˜ ì¶”ì •í•˜ëŠ” ë¬¸ì œ.
- **Object Detection**: ì´ë¯¸ì§€ ë˜ëŠ” ë¹„ë””ì˜¤ì—ì„œ ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì°¾ê³  í•´ë‹¹ ê°ì²´ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ì‹ë³„í•˜ëŠ” ë¬¸ì œ (ë¶„ë¥˜ì™€ íšŒê·€ì˜ ì¡°í•©).
- **Self-Supervised Learning**: ì •ë‹µ ë¼ë²¨ ì—†ì´ ì…ë ¥ ë°ì´í„° ìì²´ì—ì„œ í•™ìŠµ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ì‹.